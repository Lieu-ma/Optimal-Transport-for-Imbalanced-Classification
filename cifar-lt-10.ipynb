{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Create directories for saving model checkpoints\n!mkdir -p checkpoint/ours/pretrain/\n!mkdir -p ../cifar-10 # for downloading dataset\n\n# Install dependencies\nprint(\"\\n--- Installing dependencies ---\")\n!pip install torch==2.0.0 torchvision==0.15.1 tqdm -q\nprint(\"--> Dependencies installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:42.618565Z","iopub.execute_input":"2025-06-15T08:06:42.618720Z","iopub.status.idle":"2025-06-15T08:06:45.909855Z","shell.execute_reply.started":"2025-06-15T08:06:42.618705Z","shell.execute_reply":"2025-06-15T08:06:45.908882Z"}},"outputs":[{"name":"stdout","text":"\n--- Installing dependencies ---\n--> Dependencies installed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install \"numpy<2.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:45.910984Z","iopub.execute_input":"2025-06-15T08:06:45.911230Z","iopub.status.idle":"2025-06-15T08:06:48.849394Z","shell.execute_reply.started":"2025-06-15T08:06:45.911204Z","shell.execute_reply":"2025-06-15T08:06:48.848492Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile resnet.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\ndef to_var(x, requires_grad=True):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, requires_grad=requires_grad)\n\nclass resnet_attention(nn.Module):\n    def __init__(self, enc_hid_dim=64, dec_hid_dim=100):\n        super(resnet_attention, self).__init__()\n        self.attn = nn.Linear(enc_hid_dim , dec_hid_dim, bias=True)\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n    def forward(self, s):\n        energy = torch.tanh(self.attn(s))\n        attention = self.v(energy)\n        return  F.softmax(attention, dim=0)\n\nclass MetaModule(nn.Module):\n    def params(self):\n        for name, param in self.named_params(self):\n            yield param\n    def named_leaves(self):\n        return []\n    def named_submodules(self):\n        return []\n    def named_params(self, curr_module=None, memo=None, prefix=''):\n        if memo is None:\n            memo = set()\n        if hasattr(curr_module, 'named_leaves'):\n            for name, p in curr_module.named_leaves():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n        else:\n            for name, p in curr_module._parameters.items():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n        for mname, module in curr_module.named_children():\n            submodule_prefix = prefix + ('.' if prefix else '') + mname\n            for name, p in self.named_params(module, memo, submodule_prefix):\n                yield name, p\n    def update_params(self, lr_inner, first_order=False, source_params=None, detach=False):\n        if source_params is not None:\n            for tgt, src in zip(self.named_params(self), source_params):\n                name_t, param_t = tgt\n                grad = src\n                if first_order:\n                    grad = to_var(grad.detach().data)\n                tmp = param_t - lr_inner * grad\n                self.set_param(self, name_t, tmp)\n        else:\n            for name, param in self.named_params(self):\n                if not detach:\n                    grad = param.grad\n                    if first_order:\n                        grad = to_var(grad.detach().data)\n                    tmp = param - lr_inner * grad\n                    self.set_param(self, name, tmp)\n                else:\n                    param = param.detach_()\n                    self.set_param(self, name, param)\n    def set_param(self, curr_mod, name, param):\n        if '.' in name:\n            n = name.split('.')\n            module_name = n[0]\n            rest = '.'.join(n[1:])\n            for name, mod in curr_mod.named_children():\n                if module_name == name:\n                    self.set_param(mod, rest, param)\n                    break\n        else:\n            setattr(curr_mod, name, param)\n    def detach_params(self):\n        for name, param in self.named_params(self):\n            self.set_param(self, name, param.detach())\n    def copy(self, other, same_var=False):\n        for name, param in other.named_params():\n            if not same_var:\n                param = to_var(param.data.clone(), requires_grad=True)\n            self.set_param(name, param)\n\nclass MetaLinear(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Linear(*args, **kwargs)\n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\nclass MetaConv2d(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Conv2d(*args, **kwargs)\n        self.in_channels = ignore.in_channels\n        self.out_channels = ignore.out_channels\n        self.stride = ignore.stride\n        self.padding = ignore.padding\n        self.dilation = ignore.dilation\n        self.groups = ignore.groups\n        self.kernel_size = ignore.kernel_size\n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        if ignore.bias is not None:\n            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        else:\n            self.register_buffer('bias', None)\n    def forward(self, x):\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\nclass MetaBatchNorm2d(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.BatchNorm2d(*args, **kwargs)\n        self.num_features = ignore.num_features\n        self.eps = ignore.eps\n        self.momentum = ignore.momentum\n        self.affine = ignore.affine\n        self.track_running_stats = ignore.track_running_stats\n        if self.affine:\n            self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(self.num_features))\n            self.register_buffer('running_var', torch.ones(self.num_features))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n    def forward(self, x):\n        return F.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                            self.training or not self.track_running_stats, self.momentum, self.eps)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, MetaLinear) or isinstance(m, MetaConv2d):\n        init.kaiming_normal_(m.weight)\n\nclass LambdaLayer(MetaModule):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n    def forward(self, x):\n        return self.lambd(x)\n\nclass BasicBlock(MetaModule):\n    expansion = 1\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = MetaConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = MetaBatchNorm2d(planes)\n        self.conv2 = MetaConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = MetaBatchNorm2d(planes)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     MetaConv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     MetaBatchNorm2d(self.expansion * planes)\n                )\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet32(MetaModule):\n    def __init__(self, num_classes, block=BasicBlock, num_blocks=[5, 5, 5]):\n        super(ResNet32, self).__init__()\n        self.in_planes = 16\n        self.conv1 = MetaConv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = MetaBatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        self.linear = MetaLinear(64, num_classes)\n        self.apply(_weights_init)\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        y = self.linear(out)\n        return out, y\n\nprint(\"File 'resnet.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.850624Z","iopub.execute_input":"2025-06-15T08:06:48.850886Z","iopub.status.idle":"2025-06-15T08:06:48.859739Z","shell.execute_reply.started":"2025-06-15T08:06:48.850861Z","shell.execute_reply":"2025-06-15T08:06:48.859036Z"}},"outputs":[{"name":"stdout","text":"Overwriting resnet.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile data_utils.py\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport copy\nfrom torch.utils.data import Dataset\n\nnp.random.seed(6)\n\ndef build_dataset(dataset,num_meta):\n    normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n    transform_train = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0), (4, 4, 4, 4), mode='reflect').squeeze()),\n        transforms.ToPILImage(),\n        transforms.RandomCrop(32),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])\n    if dataset == 'cifar10':\n        train_dataset = torchvision.datasets.CIFAR10(root='../cifar-10', train=True, download=True, transform=transform_train)\n        test_dataset = torchvision.datasets.CIFAR10('../cifar-10', train=False, transform=transform_test)\n        img_num_list = [num_meta] * 10\n        num_classes = 10\n    if dataset == 'cifar100':\n        train_dataset = torchvision.datasets.CIFAR100(root='../cifar-100', train=True, download=True, transform=transform_train)\n        test_dataset = torchvision.datasets.CIFAR100('../cifar-100', train=False, transform=transform_test)\n        img_num_list = [num_meta] * 100\n        num_classes = 100\n    data_list_val = {}\n    for j in range(num_classes):\n        data_list_val[j] = [i for i, label in enumerate(train_dataset.targets) if label == j]\n    idx_to_meta = []\n    idx_to_train = []\n    for cls_idx, img_id_list in data_list_val.items():\n        np.random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_meta.extend(img_id_list[:img_num])\n        idx_to_train.extend(img_id_list[img_num:])\n    train_data = copy.deepcopy(train_dataset)\n    train_data_meta = copy.deepcopy(train_dataset)\n    train_data_meta.data = np.delete(train_dataset.data, idx_to_train,axis=0)\n    train_data_meta.targets = np.delete(train_dataset.targets, idx_to_train, axis=0)\n    train_data.data = np.delete(train_dataset.data, idx_to_meta, axis=0)\n    train_data.targets = np.delete(train_dataset.targets, idx_to_meta, axis=0)\n    return train_data_meta, train_data, test_dataset\n\ndef get_img_num_per_cls(dataset, imb_factor=None, num_meta=None):\n    if dataset == 'cifar10':\n        img_max = (50000-num_meta)/10\n        cls_num = 10\n    if dataset == 'cifar100':\n        img_max = (50000-num_meta)/100\n        cls_num = 100\n    if imb_factor is None:\n        return [int(img_max)] * cls_num\n    img_num_per_cls = []\n    for cls_idx in range(cls_num):\n        num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n        img_num_per_cls.append(int(num))\n    return img_num_per_cls\n\nclass new_dataset(Dataset):\n    def __init__(self, dataset, train=None):\n        self.data = dataset.data\n        self.targets = dataset.targets\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n        if train:\n            self.transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),(4, 4, 4, 4), mode='reflect').squeeze()),\n                                transforms.ToPILImage(),\n                                transforms.RandomCrop(32),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ToTensor(),\n                                normalize,\n                            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                normalize\n            ])\n    def __getitem__(self, index):\n        img, label = self.data[index, ::], self.targets[index]\n        img = self.transform(img)\n        label = torch.LongTensor([np.int64(label)])\n        return img, label, index\n    def __len__(self):\n        return len(self.data)\n\nprint(\"File 'data_utils.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.860769Z","iopub.execute_input":"2025-06-15T08:06:48.861029Z","iopub.status.idle":"2025-06-15T08:06:48.876411Z","shell.execute_reply.started":"2025-06-15T08:06:48.861013Z","shell.execute_reply":"2025-06-15T08:06:48.875798Z"}},"outputs":[{"name":"stdout","text":"Overwriting data_utils.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile Sinkhorn_distance.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd.variable import Variable\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nd_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)\n\nclass SinkhornDistance(nn.Module):\n    def __init__(self, eps, max_iter, dis, reduction='none'):\n        super(SinkhornDistance, self).__init__()\n        self.eps = eps\n        self.max_iter = max_iter\n        self.reduction = reduction\n        self.dis = dis\n    def forward(self, x, y, nu):\n        if self.dis == 'cos':\n            C = self._cost_matrix(x, y, 'cos')\n        elif self.dis == 'euc':\n            C = self._cost_matrix(x, y, 'euc')\n        x_points = x.shape[-2]\n        if x.dim() == 2:\n            batch_size = 1\n        else:\n            batch_size = x.shape[0]\n        mu = torch.empty(batch_size, x_points, dtype=torch.float, requires_grad=False).fill_(1.0 / x_points).to(device).squeeze()\n        u = torch.zeros_like(mu).to(device)\n        v = torch.zeros_like(nu).to(device)\n        for i in range(self.max_iter):\n            u1 = u\n            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n            err = (u - u1).abs().sum(-1).mean()\n            if err.item() < 1e-1:\n                break\n        U, V = u, v\n        pi = torch.exp(self.M(C, U, V))\n        cost = torch.sum(pi * C, dim=(-2, -1))\n        if self.reduction == 'mean':\n            cost = cost.mean()\n        elif self.reduction == 'sum':\n            cost = cost.sum()\n        return cost\n    def M(self, C, u, v):\n        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n    @staticmethod\n    def _cost_matrix(x, y, dis, p=2):\n        x_col = x.unsqueeze(-2)\n        y_lin = y.unsqueeze(-3)\n        if dis == 'cos':\n            C = 1 - d_cosine(x_col, y_lin)\n        elif dis == 'euc':\n            C = torch.mean((torch.abs(x_col - y_lin)) ** p, -1)\n        return C\nprint(\"File 'Sinkhorn_distance.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.878508Z","iopub.execute_input":"2025-06-15T08:06:48.878674Z","iopub.status.idle":"2025-06-15T08:06:48.894283Z","shell.execute_reply.started":"2025-06-15T08:06:48.878661Z","shell.execute_reply":"2025-06-15T08:06:48.893648Z"}},"outputs":[{"name":"stdout","text":"Overwriting Sinkhorn_distance.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile Sinkhorn_distance_fl.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd.variable import Variable\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nd_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)\n\nclass SinkhornDistance(nn.Module):\n    def __init__(self, eps, max_iter, reduction='none'):\n        super(SinkhornDistance, self).__init__()\n        self.eps = eps\n        self.max_iter = max_iter\n        self.reduction = reduction\n    def forward(self, x, y, x1, y1, nu):\n        C1 = self._cost_matrix(x, y, dis='cos')\n        C2 = self._cost_matrix(x1, y1, dis='euc')\n        C = 0.5*C1 + 0.5*C2\n        x_points = x.shape[-2]\n        if x.dim() == 2:\n            batch_size = 1\n        else:\n            batch_size = x.shape[0]\n        mu = torch.empty(batch_size, x_points, dtype=torch.float, requires_grad=False).fill_(1.0 / x_points).to(device).squeeze()\n        u = torch.zeros_like(mu).to(device)\n        v = torch.zeros_like(nu).to(device)\n        for i in range(self.max_iter):\n            u1 = u\n            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n            err = (u - u1).abs().sum(-1).mean()\n            if err.item() < 1e-1:\n                break\n        U, V = u, v\n        pi = torch.exp(self.M(C, U, V))\n        cost = torch.sum(pi * C, dim=(-2, -1))\n        if self.reduction == 'mean':\n            cost = cost.mean()\n        elif self.reduction == 'sum':\n            cost = cost.sum()\n        return cost\n    def M(self, C, u, v):\n        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n    @staticmethod\n    def _cost_matrix(x, y, dis, p=2):\n        x_col = x.unsqueeze(-2)\n        y_lin = y.unsqueeze(-3)\n        if dis == 'cos':\n            C = 1 - d_cosine(x_col , y_lin)\n        elif dis == 'euc':\n            C = torch.mean((torch.abs(x_col - y_lin)) ** p, -1)\n        return C\nprint(\"File 'Sinkhorn_distance_fl.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.895052Z","iopub.execute_input":"2025-06-15T08:06:48.895318Z","iopub.status.idle":"2025-06-15T08:06:48.908918Z","shell.execute_reply.started":"2025-06-15T08:06:48.895292Z","shell.execute_reply":"2025-06-15T08:06:48.908211Z"}},"outputs":[{"name":"stdout","text":"Overwriting Sinkhorn_distance_fl.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile pretrain_stage1.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport os\nfrom tqdm import tqdm\nimport random\nimport copy\nimport numpy as np\n\n# Import from the scripts you have\nfrom data_utils import build_dataset, get_img_num_per_cls, new_dataset\nfrom resnet import ResNet32\nfrom torch.utils.data import DataLoader\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Stage 1 Pre-training')\n    parser.add_argument('--dataset', default='cifar10', type=str)\n    parser.add_argument('--imb_factor', default=0.01, type=float)\n    parser.add_argument('--batch_size', type=int, default=128)\n    parser.add_argument('--epochs', type=int, default=200) # Paper uses 200 epochs\n    parser.add_argument('--lr', default=0.1, type=float)\n    parser.add_argument('--gpu', default=0, type=int)\n    return parser.parse_args()\n\ndef main():\n    args = get_args()\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_meta = 10 # From your OT_train.py\n    num_classes = 10 if args.dataset == 'cifar10' else 100\n\n    # 1. Create Imbalanced Dataset\n    print(\"--> Creating Imbalanced Dataset...\")\n    _, train_data, test_dataset = build_dataset(args.dataset, num_meta)\n    img_num_list = get_img_num_per_cls(args.dataset, args.imb_factor, num_meta * num_classes)\n    data_list = {j: [i for i, label in enumerate(train_data.targets) if label == j] for j in range(num_classes)}\n    idx_to_del = []\n    for cls_idx, img_id_list in data_list.items():\n        random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_del.extend(img_id_list[img_num:])\n    imbalanced_train_dataset = copy.deepcopy(train_data)\n    imbalanced_train_dataset.targets = np.delete(train_data.targets, idx_to_del, axis=0)\n    imbalanced_train_dataset.data = np.delete(train_data.data, idx_to_del, axis=0)\n    imbalanced_train_loader = DataLoader(new_dataset(imbalanced_train_dataset, train=True), batch_size=args.batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(new_dataset(test_dataset, train=False), batch_size=100, shuffle=False, num_workers=2)\n\n    # 2. Build and Train Model\n    print(\"--> Building and Training Model for Stage 1...\")\n    model = ResNet32(num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    \n    optimizer = optim.SGD(model.params(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n   \n\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[160, 180], gamma=0.1)\n\n    for epoch in range(args.epochs):\n        model.train()\n        pbar = tqdm(imbalanced_train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')\n        for inputs, labels, _ in pbar:\n            inputs, labels = inputs.to(device), labels.to(device).squeeze()\n            _, outputs = model(inputs) # Your ResNet returns (features, logits)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix({'Loss': f'{loss.item():.3f}'})\n        scheduler.step()\n\n    # 3. Save Checkpoint for OT_train.py\n    save_dir = 'checkpoint/ours/pretrain/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    save_path = f'{save_dir}/cifar10_imb{args.imb_factor}_stage1.pth'\n    print(f\"--> Stage 1 training complete. Saving model to {save_path}\")\n    torch.save({'state_dict': model.state_dict()}, save_path)\n\nif __name__ == '__main__':\n    main()\n\nprint(\"File 'pretrain_stage1.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.909744Z","iopub.execute_input":"2025-06-15T08:06:48.910455Z","iopub.status.idle":"2025-06-15T08:06:48.923502Z","shell.execute_reply.started":"2025-06-15T08:06:48.910432Z","shell.execute_reply":"2025-06-15T08:06:48.922865Z"}},"outputs":[{"name":"stdout","text":"Overwriting pretrain_stage1.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile OT_train.py\nimport os\nimport time\nimport argparse\nimport random\nimport copy\nimport torch\nimport torchvision\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom data_utils import *\nfrom resnet import *\nimport shutil\nfrom Sinkhorn_distance import SinkhornDistance\nfrom Sinkhorn_distance_fl import SinkhornDistance as SinkhornDistance_fl\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.backends.cudnn as cudnn\n\nparser = argparse.ArgumentParser(description='Imbalanced Example')\nparser.add_argument('--dataset', default='cifar10', type=str)\nparser.add_argument('--cost', default='combined', type=str)\nparser.add_argument('--meta_set', default='prototype', type=str)\nparser.add_argument('--batch-size', type=int, default=16, metavar='N')\nparser.add_argument('--num_classes', type=int, default=10)\nparser.add_argument('--num_meta', type=int, default=10)\nparser.add_argument('--imb_factor', type=float, default=0.005)\nparser.add_argument('--epochs', type=int, default=250, metavar='N')\nparser.add_argument('--lr', '--learning-rate', default=2e-5, type=float)\nparser.add_argument('--momentum', default=0.9, type=float)\nparser.add_argument('--nesterov', default=True, type=bool)\nparser.add_argument('--weight-decay', '--wd', default=5e-4, type=float)\nparser.add_argument('--no-cuda', action='store_true', default=False)\nparser.add_argument('--seed', type=int, default=42, metavar='S')\nparser.add_argument('--print-freq', '-p', default=100, type=int)\nparser.add_argument('--gpu', default=0, type=int)\nparser.add_argument('--save_name', default='OT_cifar10_imb0.005', type=str)\nparser.add_argument('--idx', default='ours', type=str)\nparser.add_argument('--ckpt_path', type=str, help='Path to pre-trained model checkpoint')\n\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n    for arg in vars(args):\n        print(f\"{arg}={getattr(args, arg)}\")\n\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n    kwargs = {'num_workers': 0, 'pin_memory': False}\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_data_meta, train_data, test_dataset = build_dataset(args.dataset, args.num_meta)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    np.random.seed(42)\n    random.seed(42)\n    torch.manual_seed(args.seed)\n\n    data_list = {}\n    for j in range(args.num_classes):\n        data_list[j] = [i for i, label in enumerate(train_loader.dataset.targets) if label == j]\n\n    img_num_list = get_img_num_per_cls(args.dataset, args.imb_factor, args.num_meta*args.num_classes)\n\n    idx_to_del = []\n    for cls_idx, img_id_list in data_list.items():\n        random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_del.extend(img_id_list[img_num:])\n\n    imbalanced_train_dataset = copy.deepcopy(train_data)\n    imbalanced_train_dataset.targets = np.delete(train_loader.dataset.targets, idx_to_del, axis=0)\n    imbalanced_train_dataset.data = np.delete(train_loader.dataset.data, idx_to_del, axis=0)\n\n    imbalanced_train_loader = DataLoader(new_dataset(imbalanced_train_dataset, train=True),\n                                         batch_size=args.batch_size, shuffle=True, **kwargs)\n    validation_loader = DataLoader(new_dataset(train_data_meta, train=True),\n                                   batch_size=args.num_classes*args.num_meta, shuffle=False, **kwargs)\n    test_loader = DataLoader(new_dataset(test_dataset, train=False),\n                             batch_size=args.batch_size, shuffle=False, **kwargs)\n\n    best_prec1 = 0\n\n    beta = 0.9999\n    effective_num = 1.0 - np.power(beta, img_num_list)\n    per_cls_weights = (1.0 - beta) / np.array(effective_num)\n    per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(img_num_list)\n    per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n    weightsbuffer = torch.tensor([per_cls_weights[cls_i] for cls_i in imbalanced_train_dataset.targets]).to(device)\n\n    eplisons = 0.1\n    criterion = SinkhornDistance(eps=eplisons, max_iter=200, reduction=None, dis='cos').to(device)\n    criterion_label = SinkhornDistance(eps=eplisons, max_iter=200, reduction=None, dis='euc').to(device)\n    criterion_fl = SinkhornDistance_fl(eps=eplisons, max_iter=200, reduction=None).to(device)\n\n    model = build_model(load_pretrain=True, ckpt_path=args.ckpt_path)\n    if not model:\n        print(\"Exiting: Failed to build model.\")\n        return\n\n    optimizer_a = torch.optim.SGD(model.linear.params(), args.lr,\n                                  momentum=args.momentum, nesterov=args.nesterov,\n                                  weight_decay=args.weight_decay)\n\n    cudnn.benchmark = True\n    criterion_classifier = nn.CrossEntropyLoss(reduction='none').to(device)\n\n    for epoch in range(160, args.epochs):\n\n        train_OT(imbalanced_train_loader, validation_loader, weightsbuffer,\n                 model, optimizer_a, epoch, criterion_classifier, device,\n                 criterion, criterion_label, criterion_fl)\n\n        prec1, _, _ = validate(test_loader, model, device)\n\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n\n        if is_best:\n            save_checkpoint(args, {\n                'epoch': epoch + 1,\n                'state_dict': model.state_dict(),\n                'best_acc1': best_prec1,\n                'optimizer': optimizer_a.state_dict(),\n            }, is_best)\n\n    print('Best accuracy: ', best_prec1)\n\n\ndef train_OT(train_loader, validation_loader, weightsbuffer, model, optimizer, epoch, criterion_classifier, device, criterion, criterion_label, criterion_fl):\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    model.train()\n\n    val_data, val_labels, _ = next(iter(validation_loader))\n    val_data = to_var(val_data.to(device), requires_grad=False)\n    val_labels = to_var(val_labels.to(device), requires_grad=False).squeeze()\n\n    if args.meta_set == 'prototype':\n        val_data_bycls = torch.zeros([args.num_classes, 3, 32, 32]).to(device)\n        for i_cls in range(args.num_classes):\n            class_samples = val_data[val_labels == i_cls]\n            if len(class_samples) > 0:\n                 val_data_bycls[i_cls, ::] = class_samples.mean(dim=0)\n        val_labels_bycls = torch.tensor([i_l for i_l in range(args.num_classes)]).to(device)\n    else: # 'whole'\n        val_data_bycls = val_data\n        val_labels_bycls = val_labels\n\n    val_labels_onehot = to_categorical(val_labels_bycls).to(device)\n    with torch.no_grad():\n        feature_val, _ = model(val_data_bycls)\n\n    for i, batch in enumerate(train_loader):\n        inputs, labels, ids = tuple(t.to(device) for t in batch)\n        labels = labels.squeeze()\n        labels_onehot = to_categorical(labels).to(device)\n        weights = to_var(weightsbuffer[ids])\n        model.eval()\n        Attoptimizer = torch.optim.SGD([weights], lr=0.01, momentum=0.9, weight_decay=5e-4)\n        with torch.no_grad():\n            feature_train, _ = model(inputs)\n        probability_train = softmax_normalize(weights)\n\n        if args.cost == 'feature':\n            OTloss = criterion(feature_val.detach(), feature_train.detach(), probability_train.squeeze())\n        elif args.cost == 'label':\n            OTloss = criterion_label(val_labels_onehot.float(),\n                                     labels_onehot.float(),\n                                     probability_train.squeeze())\n        elif args.cost == 'combined':\n            OTloss = criterion_fl(feature_val.detach(), feature_train.detach(),\n                                  val_labels_onehot.float(),\n                                  labels_onehot.float(),\n                                  probability_train.squeeze())\n\n        Attoptimizer.zero_grad()\n        OTloss.backward()\n        Attoptimizer.step()\n        weightsbuffer[ids] = weights.data\n\n        model.train()\n        optimizer.zero_grad()\n        _, logits = model(inputs)\n        loss_train = criterion_classifier(logits, labels.long())\n\n        loss = torch.sum(loss_train * weights.detach())\n        loss.backward()\n        optimizer.step()\n\n        prec_train = accuracy(logits.data, labels, topk=(1,))[0]\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec_train.item(), inputs.size(0))\n        if i % args.print_freq == 0 or i == len(train_loader) -1:\n            print(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\\\t'\n                  f'Loss {losses.val:.4f} ({losses.avg:.4f})\\\\t'\n                  f'Prec@1 {top1.val:.3f} ({top1.avg:.3f})')\n\n\ndef validate(val_loader, model, device):\n    top1 = AverageMeter()\n    model.eval()\n    with torch.no_grad():\n      for i, batch in enumerate(val_loader):\n          input, target, _ = tuple(t.to(device) for t in batch)\n          target = target.squeeze().to(device)\n\n          _, output = model(input)\n          prec1 = accuracy(output.data, target, topk=(1,))[0]\n          top1.update(prec1.item(), input.size(0))\n    print(f' * Prec@1 {top1.avg:.3f}')\n    return top1.avg, None, None\n\n\ndef build_model(load_pretrain, ckpt_path=None):\n    model = ResNet32(args.num_classes)\n    if load_pretrain:\n        if not ckpt_path or not os.path.exists(ckpt_path):\n            print(f\"ERROR: Checkpoint file not found at {ckpt_path}\")\n            return None\n        checkpoint = torch.load(ckpt_path)\n        model.load_state_dict(checkpoint['state_dict'])\n    if torch.cuda.is_available():\n        model.cuda()\n        torch.backends.cudnn.benchmark = True\n    return model\n\ndef softmax_normalize(weights, temperature=1.):\n    return F.softmax(weights / temperature, dim=0)\n\nclass AverageMeter(object):\n    def __init__(self): self.reset()\n    def reset(self): self.val = 0; self.avg = 0; self.sum = 0; self.count = 0\n    def update(self, val, n=1):\n        self.val = val; self.sum += val * n; self.count += n; self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef save_checkpoint(args, state, is_best):\n    path = 'checkpoint/ours/'\n    save_name = args.save_name\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filename = path + save_name + '_ckpt.pth.tar'\n    if is_best:\n        torch.save(state, filename)\n\ndef to_categorical(labels):\n    return F.one_hot(labels.long(), num_classes=args.num_classes)\n\nif __name__ == '__main__':\n    main()\nprint(\"File 'OT_train.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.924264Z","iopub.execute_input":"2025-06-15T08:06:48.924470Z","iopub.status.idle":"2025-06-15T08:06:48.938617Z","shell.execute_reply.started":"2025-06-15T08:06:48.924445Z","shell.execute_reply":"2025-06-15T08:06:48.937948Z"}},"outputs":[{"name":"stdout","text":"Overwriting OT_train.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Imbalance Factor - 100**","metadata":{}},{"cell_type":"code","source":"# --- Run Full Stage 1 Pre-training (200 epochs) ---\n# This will create the pre-trained model checkpoint needed for Stage 2.\n# Using Imbalance Factor of 100 (0.01) as an example.\n\n!python pretrain_stage1.py \\\n--dataset cifar10 \\\n--imb_factor 0.01 \\\n--epochs 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:06:48.939461Z","iopub.execute_input":"2025-06-15T08:06:48.939729Z","iopub.status.idle":"2025-06-15T08:23:47.114722Z","shell.execute_reply.started":"2025-06-15T08:06:48.939708Z","shell.execute_reply":"2025-06-15T08:23:47.114038Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|██████████████████| 97/97 [00:06<00:00, 15.18it/s, Loss=1.327]\nEpoch 2/200: 100%|██████████████████| 97/97 [00:05<00:00, 18.94it/s, Loss=1.284]\nEpoch 3/200: 100%|██████████████████| 97/97 [00:05<00:00, 18.74it/s, Loss=1.101]\nEpoch 4/200: 100%|██████████████████| 97/97 [00:05<00:00, 18.54it/s, Loss=1.147]\nEpoch 5/200: 100%|██████████████████| 97/97 [00:05<00:00, 18.71it/s, Loss=1.385]\nEpoch 6/200: 100%|██████████████████| 97/97 [00:05<00:00, 18.94it/s, Loss=0.930]\nEpoch 7/200: 100%|██████████████████| 97/97 [00:05<00:00, 19.08it/s, Loss=0.932]\nEpoch 8/200: 100%|██████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=1.026]\nEpoch 9/200: 100%|██████████████████| 97/97 [00:05<00:00, 19.30it/s, Loss=0.776]\nEpoch 10/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.708]\nEpoch 11/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.50it/s, Loss=0.706]\nEpoch 12/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.58it/s, Loss=0.597]\nEpoch 13/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.62it/s, Loss=0.608]\nEpoch 14/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.59it/s, Loss=0.577]\nEpoch 15/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.55it/s, Loss=0.658]\nEpoch 16/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.37it/s, Loss=0.597]\nEpoch 17/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.42it/s, Loss=0.763]\nEpoch 18/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.507]\nEpoch 19/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.565]\nEpoch 20/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.405]\nEpoch 21/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.440]\nEpoch 22/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.09it/s, Loss=0.382]\nEpoch 23/200: 100%|█████████████████| 97/97 [00:05<00:00, 18.99it/s, Loss=0.503]\nEpoch 24/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.12it/s, Loss=0.432]\nEpoch 25/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.526]\nEpoch 26/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.370]\nEpoch 27/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.657]\nEpoch 28/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.27it/s, Loss=0.510]\nEpoch 29/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.463]\nEpoch 30/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.540]\nEpoch 31/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.35it/s, Loss=0.345]\nEpoch 32/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.36it/s, Loss=0.359]\nEpoch 33/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.360]\nEpoch 34/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.28it/s, Loss=0.429]\nEpoch 35/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.315]\nEpoch 36/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.363]\nEpoch 37/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.31it/s, Loss=0.285]\nEpoch 38/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.491]\nEpoch 39/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.31it/s, Loss=0.395]\nEpoch 40/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.448]\nEpoch 41/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.309]\nEpoch 42/200: 100%|█████████████████| 97/97 [00:05<00:00, 18.94it/s, Loss=0.361]\nEpoch 43/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.393]\nEpoch 44/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.414]\nEpoch 45/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.208]\nEpoch 46/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.502]\nEpoch 47/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.27it/s, Loss=0.273]\nEpoch 48/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.338]\nEpoch 49/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.381]\nEpoch 50/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.12it/s, Loss=0.218]\nEpoch 51/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.189]\nEpoch 52/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.471]\nEpoch 53/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.34it/s, Loss=0.344]\nEpoch 54/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.317]\nEpoch 55/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.32it/s, Loss=0.291]\nEpoch 56/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.27it/s, Loss=0.276]\nEpoch 57/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.466]\nEpoch 58/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.32it/s, Loss=0.417]\nEpoch 59/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.30it/s, Loss=0.221]\nEpoch 60/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.35it/s, Loss=0.253]\nEpoch 61/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.02it/s, Loss=0.384]\nEpoch 62/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.173]\nEpoch 63/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.31it/s, Loss=0.274]\nEpoch 64/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.31it/s, Loss=0.322]\nEpoch 65/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.245]\nEpoch 66/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.33it/s, Loss=0.335]\nEpoch 67/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.370]\nEpoch 68/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.28it/s, Loss=0.317]\nEpoch 69/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.374]\nEpoch 70/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.32it/s, Loss=0.340]\nEpoch 71/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.327]\nEpoch 72/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.27it/s, Loss=0.505]\nEpoch 73/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.07it/s, Loss=0.465]\nEpoch 74/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.302]\nEpoch 75/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.278]\nEpoch 76/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.316]\nEpoch 77/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.237]\nEpoch 78/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.31it/s, Loss=0.323]\nEpoch 79/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.294]\nEpoch 80/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.12it/s, Loss=0.192]\nEpoch 81/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.523]\nEpoch 82/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=0.281]\nEpoch 83/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.354]\nEpoch 84/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.28it/s, Loss=0.435]\nEpoch 85/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.247]\nEpoch 86/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.137]\nEpoch 87/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.301]\nEpoch 88/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.162]\nEpoch 89/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.210]\nEpoch 90/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.291]\nEpoch 91/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.278]\nEpoch 92/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.08it/s, Loss=0.414]\nEpoch 93/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.05it/s, Loss=0.332]\nEpoch 94/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.179]\nEpoch 95/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.306]\nEpoch 96/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.246]\nEpoch 97/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.199]\nEpoch 98/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=0.178]\nEpoch 99/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.293]\nEpoch 100/200: 100%|████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.167]\nEpoch 101/200: 100%|████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.196]\nEpoch 102/200: 100%|████████████████| 97/97 [00:05<00:00, 19.27it/s, Loss=0.283]\nEpoch 103/200: 100%|████████████████| 97/97 [00:05<00:00, 19.30it/s, Loss=0.338]\nEpoch 104/200: 100%|████████████████| 97/97 [00:05<00:00, 19.30it/s, Loss=0.305]\nEpoch 105/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.345]\nEpoch 106/200: 100%|████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.204]\nEpoch 107/200: 100%|████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.170]\nEpoch 108/200: 100%|████████████████| 97/97 [00:05<00:00, 19.30it/s, Loss=0.314]\nEpoch 109/200: 100%|████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.198]\nEpoch 110/200: 100%|████████████████| 97/97 [00:05<00:00, 19.28it/s, Loss=0.184]\nEpoch 111/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.305]\nEpoch 112/200: 100%|████████████████| 97/97 [00:05<00:00, 19.00it/s, Loss=0.315]\nEpoch 113/200: 100%|████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.125]\nEpoch 114/200: 100%|████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.190]\nEpoch 115/200: 100%|████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=0.228]\nEpoch 116/200: 100%|████████████████| 97/97 [00:05<00:00, 19.29it/s, Loss=0.125]\nEpoch 117/200: 100%|████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.221]\nEpoch 118/200: 100%|████████████████| 97/97 [00:05<00:00, 19.09it/s, Loss=0.248]\nEpoch 119/200: 100%|████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=0.315]\nEpoch 120/200: 100%|████████████████| 97/97 [00:05<00:00, 19.26it/s, Loss=0.228]\nEpoch 121/200: 100%|████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.192]\nEpoch 122/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.353]\nEpoch 123/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.100]\nEpoch 124/200: 100%|████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.221]\nEpoch 125/200: 100%|████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.318]\nEpoch 126/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.178]\nEpoch 127/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.199]\nEpoch 128/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.177]\nEpoch 129/200: 100%|████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.213]\nEpoch 130/200: 100%|████████████████| 97/97 [00:05<00:00, 19.04it/s, Loss=0.156]\nEpoch 131/200: 100%|████████████████| 97/97 [00:05<00:00, 18.92it/s, Loss=0.233]\nEpoch 132/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.087]\nEpoch 133/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.287]\nEpoch 134/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.257]\nEpoch 135/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.369]\nEpoch 136/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.285]\nEpoch 137/200: 100%|████████████████| 97/97 [00:05<00:00, 19.08it/s, Loss=0.333]\nEpoch 138/200: 100%|████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=0.263]\nEpoch 139/200: 100%|████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.234]\nEpoch 140/200: 100%|████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.171]\nEpoch 141/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.328]\nEpoch 142/200: 100%|████████████████| 97/97 [00:05<00:00, 19.22it/s, Loss=0.183]\nEpoch 143/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.183]\nEpoch 144/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.259]\nEpoch 145/200: 100%|████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.281]\nEpoch 146/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.261]\nEpoch 147/200: 100%|████████████████| 97/97 [00:05<00:00, 19.08it/s, Loss=0.200]\nEpoch 148/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.260]\nEpoch 149/200: 100%|████████████████| 97/97 [00:05<00:00, 19.11it/s, Loss=0.270]\nEpoch 150/200: 100%|████████████████| 97/97 [00:05<00:00, 19.00it/s, Loss=0.295]\nEpoch 151/200: 100%|████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.414]\nEpoch 152/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.317]\nEpoch 153/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.199]\nEpoch 154/200: 100%|████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.237]\nEpoch 155/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.126]\nEpoch 156/200: 100%|████████████████| 97/97 [00:05<00:00, 19.06it/s, Loss=0.136]\nEpoch 157/200: 100%|████████████████| 97/97 [00:05<00:00, 19.25it/s, Loss=0.149]\nEpoch 158/200: 100%|████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.230]\nEpoch 159/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.195]\nEpoch 160/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.181]\nEpoch 161/200: 100%|████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.080]\nEpoch 162/200: 100%|████████████████| 97/97 [00:05<00:00, 18.94it/s, Loss=0.040]\nEpoch 163/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.048]\nEpoch 164/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.056]\nEpoch 165/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.072]\nEpoch 166/200: 100%|████████████████| 97/97 [00:05<00:00, 19.09it/s, Loss=0.033]\nEpoch 167/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.042]\nEpoch 168/200: 100%|████████████████| 97/97 [00:05<00:00, 19.06it/s, Loss=0.068]\nEpoch 169/200: 100%|████████████████| 97/97 [00:05<00:00, 19.00it/s, Loss=0.041]\nEpoch 170/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.040]\nEpoch 171/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.061]\nEpoch 172/200: 100%|████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.042]\nEpoch 173/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.064]\nEpoch 174/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.064]\nEpoch 175/200: 100%|████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.022]\nEpoch 176/200: 100%|████████████████| 97/97 [00:05<00:00, 19.12it/s, Loss=0.036]\nEpoch 177/200: 100%|████████████████| 97/97 [00:05<00:00, 19.17it/s, Loss=0.007]\nEpoch 178/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.026]\nEpoch 179/200: 100%|████████████████| 97/97 [00:05<00:00, 19.15it/s, Loss=0.018]\nEpoch 180/200: 100%|████████████████| 97/97 [00:05<00:00, 19.15it/s, Loss=0.046]\nEpoch 181/200: 100%|████████████████| 97/97 [00:05<00:00, 19.01it/s, Loss=0.038]\nEpoch 182/200: 100%|████████████████| 97/97 [00:05<00:00, 19.15it/s, Loss=0.008]\nEpoch 183/200: 100%|████████████████| 97/97 [00:05<00:00, 19.18it/s, Loss=0.006]\nEpoch 184/200: 100%|████████████████| 97/97 [00:05<00:00, 19.14it/s, Loss=0.014]\nEpoch 185/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.019]\nEpoch 186/200: 100%|████████████████| 97/97 [00:05<00:00, 19.15it/s, Loss=0.006]\nEpoch 187/200: 100%|████████████████| 97/97 [00:05<00:00, 19.07it/s, Loss=0.018]\nEpoch 188/200: 100%|████████████████| 97/97 [00:05<00:00, 18.88it/s, Loss=0.024]\nEpoch 189/200: 100%|████████████████| 97/97 [00:05<00:00, 19.19it/s, Loss=0.010]\nEpoch 190/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.009]\nEpoch 191/200: 100%|████████████████| 97/97 [00:05<00:00, 19.21it/s, Loss=0.039]\nEpoch 192/200: 100%|████████████████| 97/97 [00:05<00:00, 19.23it/s, Loss=0.025]\nEpoch 193/200: 100%|████████████████| 97/97 [00:05<00:00, 19.11it/s, Loss=0.056]\nEpoch 194/200: 100%|████████████████| 97/97 [00:05<00:00, 18.90it/s, Loss=0.008]\nEpoch 195/200: 100%|████████████████| 97/97 [00:05<00:00, 19.10it/s, Loss=0.017]\nEpoch 196/200: 100%|████████████████| 97/97 [00:05<00:00, 19.16it/s, Loss=0.014]\nEpoch 197/200: 100%|████████████████| 97/97 [00:05<00:00, 19.13it/s, Loss=0.064]\nEpoch 198/200: 100%|████████████████| 97/97 [00:05<00:00, 19.11it/s, Loss=0.036]\nEpoch 199/200: 100%|████████████████| 97/97 [00:05<00:00, 19.20it/s, Loss=0.058]\nEpoch 200/200: 100%|████████████████| 97/97 [00:05<00:00, 19.15it/s, Loss=0.005]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar10_imb0.01_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- Run Full Stage 2 Re-weighting (40 epochs) ---\n# This loads the checkpoint from Stage 1 and performs the final training.\n\n!python OT_train.py \\\n--dataset cifar10 \\\n--imb_factor 0.01 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--gpu 0 \\\n--save_name OT_cifar10_IF100_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar10_imb0.01_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:23:47.115771Z","iopub.execute_input":"2025-06-15T08:23:47.116033Z","iopub.status.idle":"2025-06-15T08:38:54.628262Z","shell.execute_reply.started":"2025-06-15T08:23:47.116009Z","shell.execute_reply":"2025-06-15T08:38:54.627336Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar10\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=10\nnum_meta=10\nimb_factor=0.01\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar10_IF100_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar10_imb0.01_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/774]\\tLoss 0.0698 (0.0698)\\tPrec@1 100.000 (100.000)\nEpoch: [160][100/774]\\tLoss 6.0032 (3.2475)\\tPrec@1 87.500 (90.347)\nEpoch: [160][200/774]\\tLoss 12.0183 (3.8177)\\tPrec@1 81.250 (89.956)\nEpoch: [160][300/774]\\tLoss 0.9147 (3.7208)\\tPrec@1 87.500 (90.407)\nEpoch: [160][400/774]\\tLoss 1.5014 (3.6063)\\tPrec@1 87.500 (90.461)\nEpoch: [160][500/774]\\tLoss 0.2453 (3.6400)\\tPrec@1 93.750 (90.581)\nEpoch: [160][600/774]\\tLoss 9.7589 (3.5989)\\tPrec@1 81.250 (90.381)\nEpoch: [160][700/774]\\tLoss 0.6230 (3.6448)\\tPrec@1 93.750 (90.246)\nEpoch: [160][773/774]\\tLoss 0.1435 (3.5219)\\tPrec@1 91.667 (90.436)\n * Prec@1 77.360\nEpoch: [161][0/774]\\tLoss 4.4244 (4.4244)\\tPrec@1 87.500 (87.500)\nEpoch: [161][100/774]\\tLoss 2.1654 (3.6650)\\tPrec@1 87.500 (91.275)\nEpoch: [161][200/774]\\tLoss 0.9171 (2.8874)\\tPrec@1 87.500 (91.325)\nEpoch: [161][300/774]\\tLoss 0.1394 (2.8112)\\tPrec@1 100.000 (90.739)\nEpoch: [161][400/774]\\tLoss 1.5758 (2.6215)\\tPrec@1 93.750 (90.773)\nEpoch: [161][500/774]\\tLoss 4.5697 (2.6569)\\tPrec@1 81.250 (90.818)\nEpoch: [161][600/774]\\tLoss 11.8729 (2.8210)\\tPrec@1 81.250 (90.921)\nEpoch: [161][700/774]\\tLoss 3.1744 (2.9962)\\tPrec@1 81.250 (90.826)\nEpoch: [161][773/774]\\tLoss 0.1180 (2.9559)\\tPrec@1 100.000 (90.961)\n * Prec@1 77.680\nEpoch: [162][0/774]\\tLoss 1.6453 (1.6453)\\tPrec@1 93.750 (93.750)\nEpoch: [162][100/774]\\tLoss 2.1289 (2.4938)\\tPrec@1 100.000 (90.470)\nEpoch: [162][200/774]\\tLoss 0.4119 (2.7672)\\tPrec@1 100.000 (90.361)\nEpoch: [162][300/774]\\tLoss 5.2404 (2.7111)\\tPrec@1 87.500 (90.303)\nEpoch: [162][400/774]\\tLoss 1.7470 (2.6955)\\tPrec@1 87.500 (90.212)\nEpoch: [162][500/774]\\tLoss 1.5418 (2.6845)\\tPrec@1 81.250 (90.157)\nEpoch: [162][600/774]\\tLoss 0.0551 (2.5950)\\tPrec@1 100.000 (90.308)\nEpoch: [162][700/774]\\tLoss 1.2152 (2.6100)\\tPrec@1 81.250 (90.362)\nEpoch: [162][773/774]\\tLoss 0.0378 (2.5545)\\tPrec@1 100.000 (90.485)\n * Prec@1 79.740\nEpoch: [163][0/774]\\tLoss 0.3059 (0.3059)\\tPrec@1 93.750 (93.750)\nEpoch: [163][100/774]\\tLoss 0.9608 (1.7851)\\tPrec@1 87.500 (91.275)\nEpoch: [163][200/774]\\tLoss 3.8787 (1.8873)\\tPrec@1 81.250 (91.387)\nEpoch: [163][300/774]\\tLoss 0.3601 (2.1936)\\tPrec@1 100.000 (90.573)\nEpoch: [163][400/774]\\tLoss 1.5597 (2.1794)\\tPrec@1 93.750 (90.508)\nEpoch: [163][500/774]\\tLoss 1.2414 (2.1389)\\tPrec@1 81.250 (90.631)\nEpoch: [163][600/774]\\tLoss 1.4896 (2.2077)\\tPrec@1 93.750 (90.505)\nEpoch: [163][700/774]\\tLoss 0.3164 (2.2204)\\tPrec@1 93.750 (90.415)\nEpoch: [163][773/774]\\tLoss 0.1333 (2.3239)\\tPrec@1 91.667 (90.355)\n * Prec@1 79.710\nEpoch: [164][0/774]\\tLoss 0.5630 (0.5630)\\tPrec@1 81.250 (81.250)\nEpoch: [164][100/774]\\tLoss 0.0789 (2.4873)\\tPrec@1 100.000 (89.295)\nEpoch: [164][200/774]\\tLoss 0.0339 (2.1788)\\tPrec@1 100.000 (90.361)\nEpoch: [164][300/774]\\tLoss 0.6998 (2.2510)\\tPrec@1 81.250 (90.469)\nEpoch: [164][400/774]\\tLoss 0.5864 (2.3701)\\tPrec@1 100.000 (90.165)\nEpoch: [164][500/774]\\tLoss 0.9204 (2.3053)\\tPrec@1 93.750 (89.933)\nEpoch: [164][600/774]\\tLoss 3.8904 (2.1974)\\tPrec@1 75.000 (89.975)\nEpoch: [164][700/774]\\tLoss 0.0848 (2.2084)\\tPrec@1 100.000 (89.952)\nEpoch: [164][773/774]\\tLoss 0.3006 (2.1927)\\tPrec@1 91.667 (89.911)\n * Prec@1 80.830\nEpoch: [165][0/774]\\tLoss 2.1536 (2.1536)\\tPrec@1 81.250 (81.250)\nEpoch: [165][100/774]\\tLoss 1.1257 (1.6832)\\tPrec@1 87.500 (89.851)\nEpoch: [165][200/774]\\tLoss 0.5107 (1.8791)\\tPrec@1 93.750 (89.303)\nEpoch: [165][300/774]\\tLoss 0.0904 (1.8789)\\tPrec@1 93.750 (89.556)\nEpoch: [165][400/774]\\tLoss 0.7408 (2.0011)\\tPrec@1 87.500 (89.666)\nEpoch: [165][500/774]\\tLoss 0.9826 (1.9312)\\tPrec@1 100.000 (89.820)\nEpoch: [165][600/774]\\tLoss 0.0476 (1.9874)\\tPrec@1 100.000 (89.590)\nEpoch: [165][700/774]\\tLoss 0.8804 (1.9619)\\tPrec@1 93.750 (89.453)\nEpoch: [165][773/774]\\tLoss 0.0356 (2.0352)\\tPrec@1 100.000 (89.362)\n * Prec@1 81.770\nEpoch: [166][0/774]\\tLoss 0.5086 (0.5086)\\tPrec@1 93.750 (93.750)\nEpoch: [166][100/774]\\tLoss 0.2243 (1.9940)\\tPrec@1 100.000 (88.738)\nEpoch: [166][200/774]\\tLoss 1.9890 (1.9528)\\tPrec@1 87.500 (88.744)\nEpoch: [166][300/774]\\tLoss 1.5061 (2.0552)\\tPrec@1 87.500 (88.725)\nEpoch: [166][400/774]\\tLoss 1.7125 (1.9892)\\tPrec@1 81.250 (88.544)\nEpoch: [166][500/774]\\tLoss 0.5836 (2.0005)\\tPrec@1 87.500 (88.510)\nEpoch: [166][600/774]\\tLoss 0.9887 (1.9952)\\tPrec@1 87.500 (88.436)\nEpoch: [166][700/774]\\tLoss 0.7479 (1.9946)\\tPrec@1 75.000 (88.507)\nEpoch: [166][773/774]\\tLoss 0.1984 (1.9688)\\tPrec@1 91.667 (88.522)\n * Prec@1 81.470\nEpoch: [167][0/774]\\tLoss 1.7982 (1.7982)\\tPrec@1 93.750 (93.750)\nEpoch: [167][100/774]\\tLoss 0.1035 (2.0790)\\tPrec@1 100.000 (88.552)\nEpoch: [167][200/774]\\tLoss 1.7407 (1.8490)\\tPrec@1 81.250 (88.526)\nEpoch: [167][300/774]\\tLoss 0.1330 (1.8287)\\tPrec@1 100.000 (88.393)\nEpoch: [167][400/774]\\tLoss 1.0437 (1.9571)\\tPrec@1 62.500 (87.999)\nEpoch: [167][500/774]\\tLoss 0.8061 (1.9225)\\tPrec@1 93.750 (87.937)\nEpoch: [167][600/774]\\tLoss 0.1513 (1.9459)\\tPrec@1 100.000 (87.916)\nEpoch: [167][700/774]\\tLoss 2.9931 (1.9494)\\tPrec@1 93.750 (88.017)\nEpoch: [167][773/774]\\tLoss 0.4841 (1.9474)\\tPrec@1 75.000 (87.964)\n * Prec@1 81.610\nEpoch: [168][0/774]\\tLoss 0.5125 (0.5125)\\tPrec@1 87.500 (87.500)\nEpoch: [168][100/774]\\tLoss 0.4060 (1.6932)\\tPrec@1 100.000 (89.356)\nEpoch: [168][200/774]\\tLoss 0.6906 (1.5975)\\tPrec@1 81.250 (88.588)\nEpoch: [168][300/774]\\tLoss 0.0365 (1.7632)\\tPrec@1 100.000 (88.061)\nEpoch: [168][400/774]\\tLoss 1.1463 (1.8410)\\tPrec@1 100.000 (87.858)\nEpoch: [168][500/774]\\tLoss 0.1890 (1.7955)\\tPrec@1 100.000 (87.987)\nEpoch: [168][600/774]\\tLoss 1.6623 (1.8237)\\tPrec@1 87.500 (87.968)\nEpoch: [168][700/774]\\tLoss 0.9528 (1.7533)\\tPrec@1 87.500 (87.964)\nEpoch: [168][773/774]\\tLoss 0.3063 (1.7429)\\tPrec@1 91.667 (88.005)\n * Prec@1 81.400\nEpoch: [169][0/774]\\tLoss 2.9534 (2.9534)\\tPrec@1 62.500 (62.500)\nEpoch: [169][100/774]\\tLoss 1.8658 (2.1305)\\tPrec@1 81.250 (87.871)\nEpoch: [169][200/774]\\tLoss 2.8544 (2.0557)\\tPrec@1 75.000 (87.500)\nEpoch: [169][300/774]\\tLoss 0.4794 (1.9796)\\tPrec@1 81.250 (87.936)\nEpoch: [169][400/774]\\tLoss 1.1049 (1.9887)\\tPrec@1 87.500 (87.812)\nEpoch: [169][500/774]\\tLoss 0.8654 (2.0132)\\tPrec@1 87.500 (87.338)\nEpoch: [169][600/774]\\tLoss 0.5442 (1.9783)\\tPrec@1 93.750 (87.500)\nEpoch: [169][700/774]\\tLoss 2.0420 (1.9089)\\tPrec@1 87.500 (87.571)\nEpoch: [169][773/774]\\tLoss 1.8275 (1.9480)\\tPrec@1 100.000 (87.520)\n * Prec@1 81.700\nEpoch: [170][0/774]\\tLoss 1.0319 (1.0319)\\tPrec@1 87.500 (87.500)\nEpoch: [170][100/774]\\tLoss 0.8743 (2.2830)\\tPrec@1 87.500 (87.748)\nEpoch: [170][200/774]\\tLoss 9.3713 (2.0272)\\tPrec@1 68.750 (86.629)\nEpoch: [170][300/774]\\tLoss 0.8514 (1.9003)\\tPrec@1 81.250 (86.732)\nEpoch: [170][400/774]\\tLoss 0.2612 (1.9450)\\tPrec@1 93.750 (86.939)\nEpoch: [170][500/774]\\tLoss 1.6779 (1.9230)\\tPrec@1 81.250 (86.727)\nEpoch: [170][600/774]\\tLoss 1.3924 (1.9009)\\tPrec@1 87.500 (86.741)\nEpoch: [170][700/774]\\tLoss 0.7619 (1.9401)\\tPrec@1 87.500 (86.715)\nEpoch: [170][773/774]\\tLoss 3.5010 (1.9280)\\tPrec@1 91.667 (86.640)\n * Prec@1 82.000\nEpoch: [171][0/774]\\tLoss 4.5233 (4.5233)\\tPrec@1 75.000 (75.000)\nEpoch: [171][100/774]\\tLoss 0.3924 (1.5985)\\tPrec@1 93.750 (88.057)\nEpoch: [171][200/774]\\tLoss 1.6115 (1.5690)\\tPrec@1 93.750 (87.780)\nEpoch: [171][300/774]\\tLoss 1.0749 (1.5934)\\tPrec@1 81.250 (87.562)\nEpoch: [171][400/774]\\tLoss 5.6797 (1.6318)\\tPrec@1 87.500 (87.251)\nEpoch: [171][500/774]\\tLoss 0.3763 (1.6124)\\tPrec@1 100.000 (87.138)\nEpoch: [171][600/774]\\tLoss 1.2485 (1.7221)\\tPrec@1 87.500 (86.938)\nEpoch: [171][700/774]\\tLoss 0.8499 (1.8201)\\tPrec@1 81.250 (86.617)\nEpoch: [171][773/774]\\tLoss 0.9426 (1.7850)\\tPrec@1 91.667 (86.591)\n * Prec@1 81.750\nEpoch: [172][0/774]\\tLoss 0.4846 (0.4846)\\tPrec@1 87.500 (87.500)\nEpoch: [172][100/774]\\tLoss 0.3967 (1.8422)\\tPrec@1 87.500 (87.252)\nEpoch: [172][200/774]\\tLoss 0.4161 (1.8384)\\tPrec@1 81.250 (86.598)\nEpoch: [172][300/774]\\tLoss 0.2520 (1.7829)\\tPrec@1 100.000 (86.420)\nEpoch: [172][400/774]\\tLoss 1.0049 (1.7688)\\tPrec@1 81.250 (86.222)\nEpoch: [172][500/774]\\tLoss 0.7265 (1.7955)\\tPrec@1 87.500 (86.352)\nEpoch: [172][600/774]\\tLoss 2.6001 (1.8364)\\tPrec@1 81.250 (86.221)\nEpoch: [172][700/774]\\tLoss 0.2007 (1.8129)\\tPrec@1 93.750 (86.234)\nEpoch: [172][773/774]\\tLoss 1.3305 (1.8278)\\tPrec@1 66.667 (86.317)\n * Prec@1 82.340\nEpoch: [173][0/774]\\tLoss 3.5468 (3.5468)\\tPrec@1 93.750 (93.750)\nEpoch: [173][100/774]\\tLoss 0.6566 (1.6769)\\tPrec@1 87.500 (86.200)\nEpoch: [173][200/774]\\tLoss 0.7764 (1.8633)\\tPrec@1 87.500 (86.101)\nEpoch: [173][300/774]\\tLoss 0.3521 (1.8062)\\tPrec@1 87.500 (85.880)\nEpoch: [173][400/774]\\tLoss 0.6988 (1.7952)\\tPrec@1 87.500 (85.910)\nEpoch: [173][500/774]\\tLoss 0.9023 (1.7786)\\tPrec@1 81.250 (85.978)\nEpoch: [173][600/774]\\tLoss 2.1927 (1.7735)\\tPrec@1 87.500 (86.002)\nEpoch: [173][700/774]\\tLoss 0.1946 (1.7758)\\tPrec@1 93.750 (86.154)\nEpoch: [173][773/774]\\tLoss 1.6213 (1.7733)\\tPrec@1 83.333 (86.179)\n * Prec@1 82.140\nEpoch: [174][0/774]\\tLoss 1.6762 (1.6762)\\tPrec@1 81.250 (81.250)\nEpoch: [174][100/774]\\tLoss 1.4650 (2.3364)\\tPrec@1 93.750 (84.468)\nEpoch: [174][200/774]\\tLoss 7.7219 (1.9563)\\tPrec@1 68.750 (85.479)\nEpoch: [174][300/774]\\tLoss 9.4287 (1.8356)\\tPrec@1 68.750 (85.901)\nEpoch: [174][400/774]\\tLoss 4.9336 (1.8912)\\tPrec@1 87.500 (85.739)\nEpoch: [174][500/774]\\tLoss 2.3933 (1.7946)\\tPrec@1 75.000 (85.579)\nEpoch: [174][600/774]\\tLoss 0.1360 (1.7704)\\tPrec@1 93.750 (85.587)\nEpoch: [174][700/774]\\tLoss 0.9813 (1.8593)\\tPrec@1 93.750 (85.405)\nEpoch: [174][773/774]\\tLoss 0.9074 (1.8804)\\tPrec@1 75.000 (85.485)\n * Prec@1 82.000\nEpoch: [175][0/774]\\tLoss 1.8642 (1.8642)\\tPrec@1 75.000 (75.000)\nEpoch: [175][100/774]\\tLoss 0.4355 (1.9166)\\tPrec@1 81.250 (85.644)\nEpoch: [175][200/774]\\tLoss 0.9777 (1.8871)\\tPrec@1 87.500 (85.417)\nEpoch: [175][300/774]\\tLoss 2.2154 (1.8559)\\tPrec@1 62.500 (85.569)\nEpoch: [175][400/774]\\tLoss 0.2877 (1.8145)\\tPrec@1 100.000 (85.552)\nEpoch: [175][500/774]\\tLoss 0.6489 (1.8095)\\tPrec@1 81.250 (85.529)\nEpoch: [175][600/774]\\tLoss 0.3536 (1.8068)\\tPrec@1 93.750 (85.441)\nEpoch: [175][700/774]\\tLoss 0.9022 (1.8148)\\tPrec@1 81.250 (85.307)\nEpoch: [175][773/774]\\tLoss 0.9016 (1.8378)\\tPrec@1 75.000 (85.347)\n * Prec@1 82.340\nEpoch: [176][0/774]\\tLoss 1.9364 (1.9364)\\tPrec@1 87.500 (87.500)\nEpoch: [176][100/774]\\tLoss 1.1248 (1.7056)\\tPrec@1 93.750 (86.015)\nEpoch: [176][200/774]\\tLoss 0.3223 (1.5766)\\tPrec@1 87.500 (86.163)\nEpoch: [176][300/774]\\tLoss 0.9737 (1.5846)\\tPrec@1 87.500 (86.337)\nEpoch: [176][400/774]\\tLoss 0.4536 (1.6265)\\tPrec@1 87.500 (86.362)\nEpoch: [176][500/774]\\tLoss 0.7684 (1.6459)\\tPrec@1 81.250 (86.128)\nEpoch: [176][600/774]\\tLoss 4.3584 (1.7293)\\tPrec@1 81.250 (85.857)\nEpoch: [176][700/774]\\tLoss 3.2429 (1.7001)\\tPrec@1 87.500 (85.752)\nEpoch: [176][773/774]\\tLoss 0.9451 (1.6998)\\tPrec@1 91.667 (85.493)\n * Prec@1 81.930\nEpoch: [177][0/774]\\tLoss 1.6978 (1.6978)\\tPrec@1 87.500 (87.500)\nEpoch: [177][100/774]\\tLoss 0.1685 (1.8137)\\tPrec@1 100.000 (84.406)\nEpoch: [177][200/774]\\tLoss 5.8255 (1.8266)\\tPrec@1 81.250 (85.572)\nEpoch: [177][300/774]\\tLoss 2.3389 (1.8028)\\tPrec@1 81.250 (85.610)\nEpoch: [177][400/774]\\tLoss 1.3462 (1.8568)\\tPrec@1 87.500 (85.162)\nEpoch: [177][500/774]\\tLoss 3.3829 (1.7926)\\tPrec@1 75.000 (85.354)\nEpoch: [177][600/774]\\tLoss 0.1334 (1.7915)\\tPrec@1 100.000 (85.389)\nEpoch: [177][700/774]\\tLoss 1.0143 (1.8567)\\tPrec@1 87.500 (85.414)\nEpoch: [177][773/774]\\tLoss 2.1069 (1.8400)\\tPrec@1 83.333 (85.380)\n * Prec@1 82.130\nEpoch: [178][0/774]\\tLoss 0.6643 (0.6643)\\tPrec@1 81.250 (81.250)\nEpoch: [178][100/774]\\tLoss 3.6249 (1.5318)\\tPrec@1 87.500 (86.448)\nEpoch: [178][200/774]\\tLoss 0.9157 (1.6646)\\tPrec@1 81.250 (85.697)\nEpoch: [178][300/774]\\tLoss 1.9115 (1.6516)\\tPrec@1 68.750 (85.507)\nEpoch: [178][400/774]\\tLoss 1.9251 (1.6087)\\tPrec@1 81.250 (85.271)\nEpoch: [178][500/774]\\tLoss 0.5207 (1.6895)\\tPrec@1 87.500 (84.955)\nEpoch: [178][600/774]\\tLoss 0.0935 (1.7201)\\tPrec@1 100.000 (84.942)\nEpoch: [178][700/774]\\tLoss 1.9116 (1.7037)\\tPrec@1 93.750 (84.923)\nEpoch: [178][773/774]\\tLoss 0.6279 (1.7290)\\tPrec@1 83.333 (84.968)\n * Prec@1 81.910\nEpoch: [179][0/774]\\tLoss 0.7961 (0.7961)\\tPrec@1 87.500 (87.500)\nEpoch: [179][100/774]\\tLoss 0.5601 (1.7657)\\tPrec@1 87.500 (84.839)\nEpoch: [179][200/774]\\tLoss 4.7636 (1.7428)\\tPrec@1 81.250 (85.976)\nEpoch: [179][300/774]\\tLoss 0.2251 (1.7631)\\tPrec@1 100.000 (85.631)\nEpoch: [179][400/774]\\tLoss 0.5185 (1.8804)\\tPrec@1 93.750 (85.287)\nEpoch: [179][500/774]\\tLoss 0.9567 (1.8476)\\tPrec@1 87.500 (85.205)\nEpoch: [179][600/774]\\tLoss 2.6001 (1.8538)\\tPrec@1 87.500 (85.295)\nEpoch: [179][700/774]\\tLoss 0.7434 (1.8087)\\tPrec@1 81.250 (85.325)\nEpoch: [179][773/774]\\tLoss 0.1755 (1.8235)\\tPrec@1 100.000 (85.178)\n * Prec@1 82.000\nEpoch: [180][0/774]\\tLoss 0.9100 (0.9100)\\tPrec@1 81.250 (81.250)\nEpoch: [180][100/774]\\tLoss 1.0483 (1.5302)\\tPrec@1 87.500 (86.262)\nEpoch: [180][200/774]\\tLoss 1.1794 (1.6008)\\tPrec@1 93.750 (85.976)\nEpoch: [180][300/774]\\tLoss 0.2489 (1.6295)\\tPrec@1 93.750 (85.818)\nEpoch: [180][400/774]\\tLoss 0.8206 (1.6011)\\tPrec@1 81.250 (85.801)\nEpoch: [180][500/774]\\tLoss 0.1703 (1.6346)\\tPrec@1 100.000 (85.803)\nEpoch: [180][600/774]\\tLoss 1.9247 (1.6985)\\tPrec@1 87.500 (85.576)\nEpoch: [180][700/774]\\tLoss 0.3632 (1.7284)\\tPrec@1 93.750 (85.530)\nEpoch: [180][773/774]\\tLoss 0.0452 (1.7350)\\tPrec@1 100.000 (85.477)\n * Prec@1 82.120\nEpoch: [181][0/774]\\tLoss 0.9603 (0.9603)\\tPrec@1 100.000 (100.000)\nEpoch: [181][100/774]\\tLoss 8.6161 (2.1854)\\tPrec@1 87.500 (86.139)\nEpoch: [181][200/774]\\tLoss 2.5911 (1.8087)\\tPrec@1 81.250 (85.790)\nEpoch: [181][300/774]\\tLoss 0.7652 (1.7902)\\tPrec@1 81.250 (85.714)\nEpoch: [181][400/774]\\tLoss 4.1702 (1.7890)\\tPrec@1 68.750 (85.708)\nEpoch: [181][500/774]\\tLoss 0.5904 (1.8049)\\tPrec@1 87.500 (85.379)\nEpoch: [181][600/774]\\tLoss 1.3047 (1.8333)\\tPrec@1 87.500 (85.337)\nEpoch: [181][700/774]\\tLoss 1.1847 (1.7913)\\tPrec@1 87.500 (85.307)\nEpoch: [181][773/774]\\tLoss 1.9307 (1.7476)\\tPrec@1 83.333 (85.501)\n * Prec@1 82.120\nEpoch: [182][0/774]\\tLoss 0.5138 (0.5138)\\tPrec@1 81.250 (81.250)\nEpoch: [182][100/774]\\tLoss 5.7575 (1.8109)\\tPrec@1 81.250 (84.777)\nEpoch: [182][200/774]\\tLoss 0.8434 (1.8748)\\tPrec@1 75.000 (84.515)\nEpoch: [182][300/774]\\tLoss 0.6410 (1.7777)\\tPrec@1 87.500 (84.884)\nEpoch: [182][400/774]\\tLoss 1.0103 (1.8521)\\tPrec@1 81.250 (84.757)\nEpoch: [182][500/774]\\tLoss 0.6426 (1.8208)\\tPrec@1 100.000 (84.955)\nEpoch: [182][600/774]\\tLoss 0.6714 (1.8363)\\tPrec@1 87.500 (84.723)\nEpoch: [182][700/774]\\tLoss 0.3682 (1.8203)\\tPrec@1 93.750 (84.816)\nEpoch: [182][773/774]\\tLoss 0.1657 (1.8429)\\tPrec@1 91.667 (84.863)\n * Prec@1 82.100\nEpoch: [183][0/774]\\tLoss 0.1534 (0.1534)\\tPrec@1 93.750 (93.750)\nEpoch: [183][100/774]\\tLoss 2.1482 (1.6811)\\tPrec@1 87.500 (86.200)\nEpoch: [183][200/774]\\tLoss 0.7406 (1.7154)\\tPrec@1 87.500 (85.665)\nEpoch: [183][300/774]\\tLoss 1.3230 (1.7080)\\tPrec@1 87.500 (85.050)\nEpoch: [183][400/774]\\tLoss 6.4422 (1.6819)\\tPrec@1 68.750 (84.850)\nEpoch: [183][500/774]\\tLoss 1.4766 (1.7701)\\tPrec@1 87.500 (84.918)\nEpoch: [183][600/774]\\tLoss 0.7506 (1.7657)\\tPrec@1 75.000 (84.796)\nEpoch: [183][700/774]\\tLoss 9.2246 (1.7575)\\tPrec@1 75.000 (84.799)\nEpoch: [183][773/774]\\tLoss 0.2150 (1.7269)\\tPrec@1 100.000 (84.919)\n * Prec@1 82.030\nEpoch: [184][0/774]\\tLoss 1.1613 (1.1613)\\tPrec@1 75.000 (75.000)\nEpoch: [184][100/774]\\tLoss 1.5260 (1.8623)\\tPrec@1 81.250 (84.530)\nEpoch: [184][200/774]\\tLoss 2.4290 (1.7716)\\tPrec@1 81.250 (85.230)\nEpoch: [184][300/774]\\tLoss 0.3923 (1.8737)\\tPrec@1 93.750 (84.759)\nEpoch: [184][400/774]\\tLoss 1.7493 (1.8052)\\tPrec@1 81.250 (85.006)\nEpoch: [184][500/774]\\tLoss 0.6588 (1.7984)\\tPrec@1 81.250 (84.818)\nEpoch: [184][600/774]\\tLoss 0.6862 (1.7821)\\tPrec@1 87.500 (84.609)\nEpoch: [184][700/774]\\tLoss 1.5185 (1.7458)\\tPrec@1 81.250 (84.700)\nEpoch: [184][773/774]\\tLoss 0.2645 (1.7338)\\tPrec@1 91.667 (84.798)\n * Prec@1 82.090\nEpoch: [185][0/774]\\tLoss 0.6870 (0.6870)\\tPrec@1 81.250 (81.250)\nEpoch: [185][100/774]\\tLoss 0.2960 (1.6627)\\tPrec@1 100.000 (85.396)\nEpoch: [185][200/774]\\tLoss 0.9079 (1.5905)\\tPrec@1 87.500 (85.323)\nEpoch: [185][300/774]\\tLoss 11.7255 (1.6716)\\tPrec@1 75.000 (85.154)\nEpoch: [185][400/774]\\tLoss 5.7203 (1.7354)\\tPrec@1 68.750 (84.772)\nEpoch: [185][500/774]\\tLoss 1.4135 (1.7746)\\tPrec@1 81.250 (84.706)\nEpoch: [185][600/774]\\tLoss 1.7554 (1.7933)\\tPrec@1 75.000 (84.713)\nEpoch: [185][700/774]\\tLoss 0.5852 (1.7766)\\tPrec@1 93.750 (84.665)\nEpoch: [185][773/774]\\tLoss 0.4318 (1.7460)\\tPrec@1 91.667 (84.838)\n * Prec@1 82.190\nEpoch: [186][0/774]\\tLoss 0.5112 (0.5112)\\tPrec@1 87.500 (87.500)\nEpoch: [186][100/774]\\tLoss 1.6317 (1.8415)\\tPrec@1 81.250 (84.963)\nEpoch: [186][200/774]\\tLoss 1.0469 (1.9203)\\tPrec@1 75.000 (85.137)\nEpoch: [186][300/774]\\tLoss 0.0907 (1.8406)\\tPrec@1 100.000 (84.884)\nEpoch: [186][400/774]\\tLoss 1.7193 (1.8266)\\tPrec@1 93.750 (84.991)\nEpoch: [186][500/774]\\tLoss 0.7480 (1.8191)\\tPrec@1 81.250 (85.205)\nEpoch: [186][600/774]\\tLoss 0.4126 (1.7743)\\tPrec@1 87.500 (85.233)\nEpoch: [186][700/774]\\tLoss 0.7823 (1.7782)\\tPrec@1 75.000 (84.950)\nEpoch: [186][773/774]\\tLoss 0.7110 (1.7578)\\tPrec@1 83.333 (84.935)\n * Prec@1 82.070\nEpoch: [187][0/774]\\tLoss 4.3588 (4.3588)\\tPrec@1 81.250 (81.250)\nEpoch: [187][100/774]\\tLoss 2.0995 (2.2176)\\tPrec@1 87.500 (83.663)\nEpoch: [187][200/774]\\tLoss 1.6591 (1.9486)\\tPrec@1 87.500 (83.706)\nEpoch: [187][300/774]\\tLoss 0.3922 (1.9542)\\tPrec@1 87.500 (83.762)\nEpoch: [187][400/774]\\tLoss 0.6488 (1.8361)\\tPrec@1 87.500 (84.367)\nEpoch: [187][500/774]\\tLoss 0.3367 (1.7911)\\tPrec@1 87.500 (84.905)\nEpoch: [187][600/774]\\tLoss 1.9307 (1.8266)\\tPrec@1 75.000 (84.755)\nEpoch: [187][700/774]\\tLoss 1.3153 (1.8506)\\tPrec@1 93.750 (84.718)\nEpoch: [187][773/774]\\tLoss 1.9343 (1.8566)\\tPrec@1 58.333 (84.669)\n * Prec@1 82.060\nEpoch: [188][0/774]\\tLoss 0.7064 (0.7064)\\tPrec@1 81.250 (81.250)\nEpoch: [188][100/774]\\tLoss 1.4428 (1.3797)\\tPrec@1 93.750 (86.015)\nEpoch: [188][200/774]\\tLoss 3.2933 (1.7801)\\tPrec@1 75.000 (85.230)\nEpoch: [188][300/774]\\tLoss 1.6609 (1.7319)\\tPrec@1 62.500 (85.735)\nEpoch: [188][400/774]\\tLoss 0.4806 (1.6770)\\tPrec@1 93.750 (85.910)\nEpoch: [188][500/774]\\tLoss 10.4095 (1.7197)\\tPrec@1 81.250 (85.816)\nEpoch: [188][600/774]\\tLoss 0.2288 (1.7445)\\tPrec@1 93.750 (85.399)\nEpoch: [188][700/774]\\tLoss 0.2391 (1.7560)\\tPrec@1 87.500 (85.351)\nEpoch: [188][773/774]\\tLoss 0.2180 (1.7860)\\tPrec@1 100.000 (85.234)\n * Prec@1 82.300\nEpoch: [189][0/774]\\tLoss 2.5851 (2.5851)\\tPrec@1 81.250 (81.250)\nEpoch: [189][100/774]\\tLoss 3.0561 (1.4369)\\tPrec@1 56.250 (84.653)\nEpoch: [189][200/774]\\tLoss 1.7209 (1.6412)\\tPrec@1 75.000 (85.261)\nEpoch: [189][300/774]\\tLoss 13.7071 (1.6749)\\tPrec@1 81.250 (85.029)\nEpoch: [189][400/774]\\tLoss 2.7109 (1.7908)\\tPrec@1 87.500 (85.100)\nEpoch: [189][500/774]\\tLoss 1.3629 (1.7682)\\tPrec@1 75.000 (85.242)\nEpoch: [189][600/774]\\tLoss 1.8897 (1.7484)\\tPrec@1 75.000 (85.306)\nEpoch: [189][700/774]\\tLoss 4.2301 (1.7752)\\tPrec@1 62.500 (85.307)\nEpoch: [189][773/774]\\tLoss 0.1048 (1.7880)\\tPrec@1 100.000 (85.283)\n * Prec@1 82.080\nEpoch: [190][0/774]\\tLoss 0.7940 (0.7940)\\tPrec@1 75.000 (75.000)\nEpoch: [190][100/774]\\tLoss 0.6105 (1.5782)\\tPrec@1 81.250 (84.406)\nEpoch: [190][200/774]\\tLoss 0.7411 (1.6530)\\tPrec@1 87.500 (84.453)\nEpoch: [190][300/774]\\tLoss 1.9396 (1.7903)\\tPrec@1 87.500 (83.991)\nEpoch: [190][400/774]\\tLoss 7.9931 (1.7206)\\tPrec@1 87.500 (84.585)\nEpoch: [190][500/774]\\tLoss 1.1941 (1.8477)\\tPrec@1 87.500 (84.518)\nEpoch: [190][600/774]\\tLoss 0.3023 (1.8392)\\tPrec@1 87.500 (84.817)\nEpoch: [190][700/774]\\tLoss 0.4894 (1.8069)\\tPrec@1 87.500 (84.700)\nEpoch: [190][773/774]\\tLoss 0.9644 (1.7683)\\tPrec@1 75.000 (84.838)\n * Prec@1 81.810\nEpoch: [191][0/774]\\tLoss 4.7718 (4.7718)\\tPrec@1 93.750 (93.750)\nEpoch: [191][100/774]\\tLoss 3.5257 (1.6427)\\tPrec@1 81.250 (84.530)\nEpoch: [191][200/774]\\tLoss 0.6064 (1.6195)\\tPrec@1 81.250 (85.075)\nEpoch: [191][300/774]\\tLoss 0.5747 (1.8613)\\tPrec@1 93.750 (84.925)\nEpoch: [191][400/774]\\tLoss 0.6247 (1.8670)\\tPrec@1 87.500 (85.256)\nEpoch: [191][500/774]\\tLoss 0.8859 (1.8225)\\tPrec@1 81.250 (85.329)\nEpoch: [191][600/774]\\tLoss 0.2177 (1.7608)\\tPrec@1 100.000 (85.254)\nEpoch: [191][700/774]\\tLoss 0.3361 (1.7790)\\tPrec@1 93.750 (85.253)\nEpoch: [191][773/774]\\tLoss 0.7806 (1.7557)\\tPrec@1 83.333 (85.267)\n * Prec@1 82.120\nEpoch: [192][0/774]\\tLoss 4.3082 (4.3082)\\tPrec@1 87.500 (87.500)\nEpoch: [192][100/774]\\tLoss 0.2944 (1.3514)\\tPrec@1 93.750 (85.149)\nEpoch: [192][200/774]\\tLoss 0.6405 (1.6153)\\tPrec@1 93.750 (84.453)\nEpoch: [192][300/774]\\tLoss 12.0374 (1.6733)\\tPrec@1 75.000 (84.510)\nEpoch: [192][400/774]\\tLoss 3.7637 (1.6403)\\tPrec@1 75.000 (84.523)\nEpoch: [192][500/774]\\tLoss 0.8032 (1.6511)\\tPrec@1 87.500 (84.556)\nEpoch: [192][600/774]\\tLoss 7.3978 (1.6798)\\tPrec@1 62.500 (84.370)\nEpoch: [192][700/774]\\tLoss 0.7267 (1.7108)\\tPrec@1 93.750 (84.460)\nEpoch: [192][773/774]\\tLoss 0.2186 (1.7046)\\tPrec@1 91.667 (84.548)\n * Prec@1 82.150\nEpoch: [193][0/774]\\tLoss 1.1559 (1.1559)\\tPrec@1 87.500 (87.500)\nEpoch: [193][100/774]\\tLoss 1.0934 (1.7026)\\tPrec@1 87.500 (85.396)\nEpoch: [193][200/774]\\tLoss 0.9261 (1.6049)\\tPrec@1 81.250 (85.448)\nEpoch: [193][300/774]\\tLoss 3.8515 (1.8351)\\tPrec@1 81.250 (85.091)\nEpoch: [193][400/774]\\tLoss 2.4840 (1.7311)\\tPrec@1 100.000 (85.193)\nEpoch: [193][500/774]\\tLoss 6.3003 (1.7946)\\tPrec@1 68.750 (84.880)\nEpoch: [193][600/774]\\tLoss 4.2913 (1.7803)\\tPrec@1 93.750 (84.817)\nEpoch: [193][700/774]\\tLoss 0.5506 (1.7776)\\tPrec@1 87.500 (84.683)\nEpoch: [193][773/774]\\tLoss 0.6372 (1.7736)\\tPrec@1 83.333 (84.855)\n * Prec@1 82.400\nEpoch: [194][0/774]\\tLoss 0.9886 (0.9886)\\tPrec@1 56.250 (56.250)\nEpoch: [194][100/774]\\tLoss 1.8536 (1.6052)\\tPrec@1 87.500 (85.520)\nEpoch: [194][200/774]\\tLoss 1.5325 (1.7023)\\tPrec@1 87.500 (85.572)\nEpoch: [194][300/774]\\tLoss 0.1284 (1.6177)\\tPrec@1 100.000 (85.507)\nEpoch: [194][400/774]\\tLoss 0.1960 (1.7188)\\tPrec@1 93.750 (85.178)\nEpoch: [194][500/774]\\tLoss 10.4354 (1.7628)\\tPrec@1 87.500 (85.142)\nEpoch: [194][600/774]\\tLoss 1.0424 (1.7977)\\tPrec@1 68.750 (85.108)\nEpoch: [194][700/774]\\tLoss 0.4163 (1.7918)\\tPrec@1 93.750 (85.066)\nEpoch: [194][773/774]\\tLoss 0.6617 (1.8093)\\tPrec@1 83.333 (85.032)\n * Prec@1 82.360\nEpoch: [195][0/774]\\tLoss 3.4912 (3.4912)\\tPrec@1 93.750 (93.750)\nEpoch: [195][100/774]\\tLoss 1.1196 (1.9413)\\tPrec@1 81.250 (84.406)\nEpoch: [195][200/774]\\tLoss 0.8988 (1.8990)\\tPrec@1 87.500 (84.764)\nEpoch: [195][300/774]\\tLoss 1.0437 (1.8122)\\tPrec@1 93.750 (84.510)\nEpoch: [195][400/774]\\tLoss 0.4007 (1.7364)\\tPrec@1 93.750 (84.585)\nEpoch: [195][500/774]\\tLoss 0.5602 (1.7295)\\tPrec@1 87.500 (84.618)\nEpoch: [195][600/774]\\tLoss 0.0988 (1.7788)\\tPrec@1 100.000 (84.453)\nEpoch: [195][700/774]\\tLoss 1.2029 (1.7393)\\tPrec@1 87.500 (84.567)\nEpoch: [195][773/774]\\tLoss 0.1335 (1.8402)\\tPrec@1 100.000 (84.564)\n * Prec@1 82.220\nEpoch: [196][0/774]\\tLoss 2.1037 (2.1037)\\tPrec@1 75.000 (75.000)\nEpoch: [196][100/774]\\tLoss 0.8101 (1.6013)\\tPrec@1 87.500 (85.210)\nEpoch: [196][200/774]\\tLoss 0.3483 (1.7450)\\tPrec@1 100.000 (85.510)\nEpoch: [196][300/774]\\tLoss 0.4280 (1.7464)\\tPrec@1 81.250 (85.444)\nEpoch: [196][400/774]\\tLoss 1.4945 (1.7406)\\tPrec@1 93.750 (85.022)\nEpoch: [196][500/774]\\tLoss 0.2567 (1.7104)\\tPrec@1 93.750 (85.080)\nEpoch: [196][600/774]\\tLoss 0.5310 (1.7005)\\tPrec@1 81.250 (84.879)\nEpoch: [196][700/774]\\tLoss 1.6109 (1.6820)\\tPrec@1 81.250 (85.012)\nEpoch: [196][773/774]\\tLoss 0.3723 (1.7035)\\tPrec@1 100.000 (85.073)\n * Prec@1 82.130\nEpoch: [197][0/774]\\tLoss 0.4622 (0.4622)\\tPrec@1 81.250 (81.250)\nEpoch: [197][100/774]\\tLoss 2.2889 (2.0617)\\tPrec@1 81.250 (85.644)\nEpoch: [197][200/774]\\tLoss 0.1802 (1.8528)\\tPrec@1 93.750 (85.075)\nEpoch: [197][300/774]\\tLoss 0.9658 (1.7907)\\tPrec@1 81.250 (84.738)\nEpoch: [197][400/774]\\tLoss 0.7208 (1.7431)\\tPrec@1 75.000 (85.287)\nEpoch: [197][500/774]\\tLoss 0.7925 (1.8485)\\tPrec@1 87.500 (84.918)\nEpoch: [197][600/774]\\tLoss 0.4550 (1.7807)\\tPrec@1 87.500 (85.191)\nEpoch: [197][700/774]\\tLoss 1.0991 (1.7742)\\tPrec@1 81.250 (85.191)\nEpoch: [197][773/774]\\tLoss 0.6630 (1.7686)\\tPrec@1 83.333 (85.170)\n * Prec@1 82.210\nEpoch: [198][0/774]\\tLoss 1.4353 (1.4353)\\tPrec@1 81.250 (81.250)\nEpoch: [198][100/774]\\tLoss 0.3636 (1.6477)\\tPrec@1 93.750 (85.644)\nEpoch: [198][200/774]\\tLoss 0.3496 (1.6110)\\tPrec@1 93.750 (85.448)\nEpoch: [198][300/774]\\tLoss 0.3304 (1.5670)\\tPrec@1 100.000 (85.278)\nEpoch: [198][400/774]\\tLoss 0.9265 (1.6321)\\tPrec@1 93.750 (85.162)\nEpoch: [198][500/774]\\tLoss 4.0977 (1.6561)\\tPrec@1 75.000 (85.092)\nEpoch: [198][600/774]\\tLoss 1.3858 (1.6978)\\tPrec@1 75.000 (85.212)\nEpoch: [198][700/774]\\tLoss 12.4565 (1.7443)\\tPrec@1 87.500 (85.021)\nEpoch: [198][773/774]\\tLoss 4.2987 (1.7812)\\tPrec@1 83.333 (84.960)\n * Prec@1 82.290\nEpoch: [199][0/774]\\tLoss 1.3824 (1.3824)\\tPrec@1 81.250 (81.250)\nEpoch: [199][100/774]\\tLoss 1.0798 (1.9670)\\tPrec@1 87.500 (85.210)\nEpoch: [199][200/774]\\tLoss 2.8703 (1.7828)\\tPrec@1 81.250 (84.919)\nEpoch: [199][300/774]\\tLoss 3.7848 (1.8821)\\tPrec@1 81.250 (84.738)\nEpoch: [199][400/774]\\tLoss 0.9988 (1.8073)\\tPrec@1 87.500 (85.022)\nEpoch: [199][500/774]\\tLoss 0.5240 (1.7339)\\tPrec@1 87.500 (84.918)\nEpoch: [199][600/774]\\tLoss 1.5457 (1.7413)\\tPrec@1 93.750 (84.817)\nEpoch: [199][700/774]\\tLoss 1.4184 (1.7273)\\tPrec@1 87.500 (84.754)\nEpoch: [199][773/774]\\tLoss 0.2221 (1.7476)\\tPrec@1 91.667 (84.822)\n * Prec@1 82.170\nBest accuracy:  82.4\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Imbalance Factor - 200**  ","metadata":{}},{"cell_type":"code","source":"# --- Run Stage 1 Pre-training (IF=200) ---\n\n!python pretrain_stage1.py \\\n--dataset cifar10 \\\n--imb_factor 0.005 \\\n--epochs 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:38:54.629389Z","iopub.execute_input":"2025-06-15T08:38:54.629649Z","iopub.status.idle":"2025-06-15T08:54:09.018416Z","shell.execute_reply.started":"2025-06-15T08:38:54.629625Z","shell.execute_reply":"2025-06-15T08:54:09.017742Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|██████████████████| 88/88 [00:05<00:00, 14.80it/s, Loss=1.390]\nEpoch 2/200: 100%|██████████████████| 88/88 [00:04<00:00, 19.14it/s, Loss=1.242]\nEpoch 3/200: 100%|██████████████████| 88/88 [00:04<00:00, 18.98it/s, Loss=1.102]\nEpoch 4/200: 100%|██████████████████| 88/88 [00:04<00:00, 18.82it/s, Loss=1.009]\nEpoch 5/200: 100%|██████████████████| 88/88 [00:04<00:00, 18.84it/s, Loss=0.747]\nEpoch 6/200: 100%|██████████████████| 88/88 [00:04<00:00, 19.06it/s, Loss=0.741]\nEpoch 7/200: 100%|██████████████████| 88/88 [00:04<00:00, 19.22it/s, Loss=1.092]\nEpoch 8/200: 100%|██████████████████| 88/88 [00:04<00:00, 19.20it/s, Loss=0.777]\nEpoch 9/200: 100%|██████████████████| 88/88 [00:04<00:00, 19.51it/s, Loss=0.960]\nEpoch 10/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.58it/s, Loss=0.813]\nEpoch 11/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.68it/s, Loss=0.773]\nEpoch 12/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.78it/s, Loss=0.469]\nEpoch 13/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.72it/s, Loss=0.976]\nEpoch 14/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.75it/s, Loss=0.779]\nEpoch 15/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.59it/s, Loss=1.058]\nEpoch 16/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.66it/s, Loss=0.816]\nEpoch 17/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.62it/s, Loss=0.396]\nEpoch 18/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.56it/s, Loss=0.681]\nEpoch 19/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.576]\nEpoch 20/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.214]\nEpoch 21/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.31it/s, Loss=0.714]\nEpoch 22/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.09it/s, Loss=0.633]\nEpoch 23/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.13it/s, Loss=0.619]\nEpoch 24/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.27it/s, Loss=0.386]\nEpoch 25/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.24it/s, Loss=0.537]\nEpoch 26/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.34it/s, Loss=0.672]\nEpoch 27/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.28it/s, Loss=0.588]\nEpoch 28/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.31it/s, Loss=0.199]\nEpoch 29/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.20it/s, Loss=0.761]\nEpoch 30/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.697]\nEpoch 31/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.574]\nEpoch 32/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.522]\nEpoch 33/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.315]\nEpoch 34/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.51it/s, Loss=0.480]\nEpoch 35/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.604]\nEpoch 36/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.498]\nEpoch 37/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.25it/s, Loss=0.479]\nEpoch 38/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.347]\nEpoch 39/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.678]\nEpoch 40/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.304]\nEpoch 41/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.29it/s, Loss=0.503]\nEpoch 42/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.437]\nEpoch 43/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.29it/s, Loss=0.285]\nEpoch 44/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.21it/s, Loss=0.462]\nEpoch 45/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.648]\nEpoch 46/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.608]\nEpoch 47/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.333]\nEpoch 48/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.32it/s, Loss=0.598]\nEpoch 49/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.394]\nEpoch 50/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.29it/s, Loss=0.418]\nEpoch 51/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.28it/s, Loss=0.497]\nEpoch 52/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.177]\nEpoch 53/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.484]\nEpoch 54/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.183]\nEpoch 55/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.431]\nEpoch 56/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.336]\nEpoch 57/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.30it/s, Loss=0.589]\nEpoch 58/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.16it/s, Loss=0.300]\nEpoch 59/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.199]\nEpoch 60/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.230]\nEpoch 61/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.221]\nEpoch 62/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.35it/s, Loss=0.878]\nEpoch 63/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.477]\nEpoch 64/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.239]\nEpoch 65/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.14it/s, Loss=0.247]\nEpoch 66/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.224]\nEpoch 67/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.34it/s, Loss=0.518]\nEpoch 68/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.206]\nEpoch 69/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.285]\nEpoch 70/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.436]\nEpoch 71/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.136]\nEpoch 72/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.05it/s, Loss=0.121]\nEpoch 73/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.472]\nEpoch 74/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.297]\nEpoch 75/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.244]\nEpoch 76/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.350]\nEpoch 77/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.462]\nEpoch 78/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.35it/s, Loss=0.315]\nEpoch 79/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.12it/s, Loss=0.303]\nEpoch 80/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.190]\nEpoch 81/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.260]\nEpoch 82/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.31it/s, Loss=0.400]\nEpoch 83/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.398]\nEpoch 84/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.41it/s, Loss=0.219]\nEpoch 85/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.35it/s, Loss=0.265]\nEpoch 86/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.18it/s, Loss=0.296]\nEpoch 87/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.41it/s, Loss=0.313]\nEpoch 88/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.301]\nEpoch 89/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.34it/s, Loss=0.067]\nEpoch 90/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.267]\nEpoch 91/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.241]\nEpoch 92/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.074]\nEpoch 93/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.35it/s, Loss=0.307]\nEpoch 94/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.310]\nEpoch 95/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.187]\nEpoch 96/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.515]\nEpoch 97/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.453]\nEpoch 98/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.53it/s, Loss=0.176]\nEpoch 99/200: 100%|█████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.540]\nEpoch 100/200: 100%|████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.309]\nEpoch 101/200: 100%|████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.398]\nEpoch 102/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.106]\nEpoch 103/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.271]\nEpoch 104/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.248]\nEpoch 105/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.282]\nEpoch 106/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.332]\nEpoch 107/200: 100%|████████████████| 88/88 [00:04<00:00, 19.26it/s, Loss=0.290]\nEpoch 108/200: 100%|████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.178]\nEpoch 109/200: 100%|████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.286]\nEpoch 110/200: 100%|████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.175]\nEpoch 111/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.245]\nEpoch 112/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.184]\nEpoch 113/200: 100%|████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.223]\nEpoch 114/200: 100%|████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.243]\nEpoch 115/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.105]\nEpoch 116/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.230]\nEpoch 117/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.139]\nEpoch 118/200: 100%|████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.207]\nEpoch 119/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.339]\nEpoch 120/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.276]\nEpoch 121/200: 100%|████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.366]\nEpoch 122/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.122]\nEpoch 123/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.157]\nEpoch 124/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.335]\nEpoch 125/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.415]\nEpoch 126/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.295]\nEpoch 127/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.402]\nEpoch 128/200: 100%|████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.139]\nEpoch 129/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.380]\nEpoch 130/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.359]\nEpoch 131/200: 100%|████████████████| 88/88 [00:04<00:00, 19.41it/s, Loss=0.465]\nEpoch 132/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.353]\nEpoch 133/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.218]\nEpoch 134/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.220]\nEpoch 135/200: 100%|████████████████| 88/88 [00:04<00:00, 19.20it/s, Loss=0.252]\nEpoch 136/200: 100%|████████████████| 88/88 [00:04<00:00, 19.29it/s, Loss=0.403]\nEpoch 137/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.130]\nEpoch 138/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.134]\nEpoch 139/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.142]\nEpoch 140/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.351]\nEpoch 141/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.147]\nEpoch 142/200: 100%|████████████████| 88/88 [00:04<00:00, 19.30it/s, Loss=0.380]\nEpoch 143/200: 100%|████████████████| 88/88 [00:04<00:00, 19.26it/s, Loss=0.193]\nEpoch 144/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.150]\nEpoch 145/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.213]\nEpoch 146/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.328]\nEpoch 147/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.766]\nEpoch 148/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.385]\nEpoch 149/200: 100%|████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.354]\nEpoch 150/200: 100%|████████████████| 88/88 [00:04<00:00, 19.26it/s, Loss=0.216]\nEpoch 151/200: 100%|████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.337]\nEpoch 152/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.444]\nEpoch 153/200: 100%|████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.087]\nEpoch 154/200: 100%|████████████████| 88/88 [00:04<00:00, 19.53it/s, Loss=0.198]\nEpoch 155/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.175]\nEpoch 156/200: 100%|████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.406]\nEpoch 157/200: 100%|████████████████| 88/88 [00:04<00:00, 19.38it/s, Loss=0.322]\nEpoch 158/200: 100%|████████████████| 88/88 [00:04<00:00, 19.51it/s, Loss=0.119]\nEpoch 159/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.304]\nEpoch 160/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.131]\nEpoch 161/200: 100%|████████████████| 88/88 [00:04<00:00, 19.57it/s, Loss=0.223]\nEpoch 162/200: 100%|████████████████| 88/88 [00:04<00:00, 19.54it/s, Loss=0.225]\nEpoch 163/200: 100%|████████████████| 88/88 [00:04<00:00, 19.40it/s, Loss=0.018]\nEpoch 164/200: 100%|████████████████| 88/88 [00:04<00:00, 19.33it/s, Loss=0.148]\nEpoch 165/200: 100%|████████████████| 88/88 [00:04<00:00, 19.55it/s, Loss=0.050]\nEpoch 166/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.047]\nEpoch 167/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.013]\nEpoch 168/200: 100%|████████████████| 88/88 [00:04<00:00, 19.43it/s, Loss=0.165]\nEpoch 169/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.054]\nEpoch 170/200: 100%|████████████████| 88/88 [00:04<00:00, 19.39it/s, Loss=0.088]\nEpoch 171/200: 100%|████████████████| 88/88 [00:04<00:00, 19.19it/s, Loss=0.040]\nEpoch 172/200: 100%|████████████████| 88/88 [00:04<00:00, 19.51it/s, Loss=0.139]\nEpoch 173/200: 100%|████████████████| 88/88 [00:04<00:00, 19.52it/s, Loss=0.026]\nEpoch 174/200: 100%|████████████████| 88/88 [00:04<00:00, 19.50it/s, Loss=0.226]\nEpoch 175/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.145]\nEpoch 176/200: 100%|████████████████| 88/88 [00:04<00:00, 19.54it/s, Loss=0.013]\nEpoch 177/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.042]\nEpoch 178/200: 100%|████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.021]\nEpoch 179/200: 100%|████████████████| 88/88 [00:04<00:00, 19.52it/s, Loss=0.024]\nEpoch 180/200: 100%|████████████████| 88/88 [00:04<00:00, 19.53it/s, Loss=0.044]\nEpoch 181/200: 100%|████████████████| 88/88 [00:04<00:00, 19.52it/s, Loss=0.116]\nEpoch 182/200: 100%|████████████████| 88/88 [00:04<00:00, 19.47it/s, Loss=0.035]\nEpoch 183/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.013]\nEpoch 184/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.066]\nEpoch 185/200: 100%|████████████████| 88/88 [00:04<00:00, 19.44it/s, Loss=0.087]\nEpoch 186/200: 100%|████████████████| 88/88 [00:04<00:00, 19.55it/s, Loss=0.050]\nEpoch 187/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.008]\nEpoch 188/200: 100%|████████████████| 88/88 [00:04<00:00, 19.45it/s, Loss=0.012]\nEpoch 189/200: 100%|████████████████| 88/88 [00:04<00:00, 19.55it/s, Loss=0.022]\nEpoch 190/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.020]\nEpoch 191/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.091]\nEpoch 192/200: 100%|████████████████| 88/88 [00:04<00:00, 19.36it/s, Loss=0.058]\nEpoch 193/200: 100%|████████████████| 88/88 [00:04<00:00, 19.42it/s, Loss=0.064]\nEpoch 194/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.078]\nEpoch 195/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.008]\nEpoch 196/200: 100%|████████████████| 88/88 [00:04<00:00, 19.46it/s, Loss=0.197]\nEpoch 197/200: 100%|████████████████| 88/88 [00:04<00:00, 19.49it/s, Loss=0.012]\nEpoch 198/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.015]\nEpoch 199/200: 100%|████████████████| 88/88 [00:04<00:00, 19.37it/s, Loss=0.017]\nEpoch 200/200: 100%|████████████████| 88/88 [00:04<00:00, 19.48it/s, Loss=0.037]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar10_imb0.005_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Run Stage 2 Re-weighting (IF=200) ---\n\n!python OT_train.py \\\n--dataset cifar10 \\\n--imb_factor 0.005 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar10_IF200_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar10_imb0.005_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:54:09.019692Z","iopub.execute_input":"2025-06-15T08:54:09.020007Z","iopub.status.idle":"2025-06-15T09:07:40.293234Z","shell.execute_reply.started":"2025-06-15T08:54:09.019981Z","shell.execute_reply":"2025-06-15T09:07:40.292303Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar10\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=10\nnum_meta=10\nimb_factor=0.005\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar10_IF200_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar10_imb0.005_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/699]\\tLoss 0.0903 (0.0903)\\tPrec@1 100.000 (100.000)\nEpoch: [160][100/699]\\tLoss 0.1325 (2.9888)\\tPrec@1 100.000 (91.151)\nEpoch: [160][200/699]\\tLoss 18.7872 (2.8378)\\tPrec@1 87.500 (90.796)\nEpoch: [160][300/699]\\tLoss 0.3522 (3.1290)\\tPrec@1 75.000 (90.594)\nEpoch: [160][400/699]\\tLoss 1.5338 (2.9339)\\tPrec@1 87.500 (90.726)\nEpoch: [160][500/699]\\tLoss 0.6737 (2.8850)\\tPrec@1 87.500 (90.793)\nEpoch: [160][600/699]\\tLoss 1.1679 (2.9136)\\tPrec@1 93.750 (90.734)\nEpoch: [160][698/699]\\tLoss 0.9615 (2.8989)\\tPrec@1 77.778 (90.561)\n * Prec@1 68.870\nEpoch: [161][0/699]\\tLoss 1.4202 (1.4202)\\tPrec@1 93.750 (93.750)\nEpoch: [161][100/699]\\tLoss 0.4392 (2.7356)\\tPrec@1 87.500 (89.975)\nEpoch: [161][200/699]\\tLoss 0.5735 (2.7034)\\tPrec@1 81.250 (89.801)\nEpoch: [161][300/699]\\tLoss 0.2562 (2.8489)\\tPrec@1 81.250 (89.909)\nEpoch: [161][400/699]\\tLoss 1.3551 (2.7289)\\tPrec@1 93.750 (90.056)\nEpoch: [161][500/699]\\tLoss 0.0564 (2.7250)\\tPrec@1 100.000 (90.082)\nEpoch: [161][600/699]\\tLoss 0.7087 (2.5286)\\tPrec@1 93.750 (90.422)\nEpoch: [161][698/699]\\tLoss 0.0305 (2.7710)\\tPrec@1 100.000 (90.436)\n * Prec@1 70.320\nEpoch: [162][0/699]\\tLoss 0.2697 (0.2697)\\tPrec@1 100.000 (100.000)\nEpoch: [162][100/699]\\tLoss 0.8165 (2.7690)\\tPrec@1 93.750 (90.532)\nEpoch: [162][200/699]\\tLoss 0.1018 (3.1129)\\tPrec@1 93.750 (90.112)\nEpoch: [162][300/699]\\tLoss 0.5583 (2.7836)\\tPrec@1 93.750 (90.096)\nEpoch: [162][400/699]\\tLoss 2.0274 (2.5548)\\tPrec@1 93.750 (90.524)\nEpoch: [162][500/699]\\tLoss 0.2630 (2.5340)\\tPrec@1 100.000 (90.544)\nEpoch: [162][600/699]\\tLoss 1.8917 (2.4894)\\tPrec@1 87.500 (90.599)\nEpoch: [162][698/699]\\tLoss 0.1643 (2.4670)\\tPrec@1 88.889 (90.454)\n * Prec@1 71.490\nEpoch: [163][0/699]\\tLoss 2.9360 (2.9360)\\tPrec@1 87.500 (87.500)\nEpoch: [163][100/699]\\tLoss 4.7536 (3.0542)\\tPrec@1 81.250 (88.676)\nEpoch: [163][200/699]\\tLoss 0.0732 (3.1886)\\tPrec@1 100.000 (89.397)\nEpoch: [163][300/699]\\tLoss 0.0818 (2.9639)\\tPrec@1 100.000 (89.763)\nEpoch: [163][400/699]\\tLoss 1.6430 (2.7563)\\tPrec@1 87.500 (89.885)\nEpoch: [163][500/699]\\tLoss 2.0515 (2.5919)\\tPrec@1 93.750 (90.057)\nEpoch: [163][600/699]\\tLoss 0.4954 (2.4773)\\tPrec@1 93.750 (90.089)\nEpoch: [163][698/699]\\tLoss 0.0349 (2.4219)\\tPrec@1 100.000 (90.087)\n * Prec@1 70.260\nEpoch: [164][0/699]\\tLoss 0.1698 (0.1698)\\tPrec@1 93.750 (93.750)\nEpoch: [164][100/699]\\tLoss 0.6243 (1.7064)\\tPrec@1 87.500 (91.337)\nEpoch: [164][200/699]\\tLoss 0.0433 (1.8161)\\tPrec@1 100.000 (90.951)\nEpoch: [164][300/699]\\tLoss 1.5097 (1.9629)\\tPrec@1 87.500 (91.238)\nEpoch: [164][400/699]\\tLoss 1.7517 (2.0277)\\tPrec@1 81.250 (91.100)\nEpoch: [164][500/699]\\tLoss 0.1031 (1.9851)\\tPrec@1 93.750 (90.906)\nEpoch: [164][600/699]\\tLoss 0.7887 (1.9807)\\tPrec@1 81.250 (90.786)\nEpoch: [164][698/699]\\tLoss 0.1003 (2.0607)\\tPrec@1 88.889 (90.659)\n * Prec@1 73.600\nEpoch: [165][0/699]\\tLoss 0.2087 (0.2087)\\tPrec@1 93.750 (93.750)\nEpoch: [165][100/699]\\tLoss 0.1205 (1.6609)\\tPrec@1 93.750 (90.842)\nEpoch: [165][200/699]\\tLoss 15.1567 (2.1739)\\tPrec@1 93.750 (90.143)\nEpoch: [165][300/699]\\tLoss 0.9740 (1.9659)\\tPrec@1 81.250 (90.635)\nEpoch: [165][400/699]\\tLoss 7.8899 (1.9109)\\tPrec@1 81.250 (90.633)\nEpoch: [165][500/699]\\tLoss 1.1914 (1.8971)\\tPrec@1 87.500 (90.469)\nEpoch: [165][600/699]\\tLoss 1.0434 (1.9417)\\tPrec@1 93.750 (90.277)\nEpoch: [165][698/699]\\tLoss 0.8988 (1.9478)\\tPrec@1 88.889 (90.257)\n * Prec@1 73.180\nEpoch: [166][0/699]\\tLoss 0.1290 (0.1290)\\tPrec@1 100.000 (100.000)\nEpoch: [166][100/699]\\tLoss 1.1420 (1.7957)\\tPrec@1 81.250 (90.532)\nEpoch: [166][200/699]\\tLoss 1.4928 (1.7115)\\tPrec@1 93.750 (89.894)\nEpoch: [166][300/699]\\tLoss 0.3696 (1.5460)\\tPrec@1 87.500 (89.805)\nEpoch: [166][400/699]\\tLoss 0.3101 (1.5785)\\tPrec@1 93.750 (89.807)\nEpoch: [166][500/699]\\tLoss 2.2716 (1.6821)\\tPrec@1 93.750 (89.708)\nEpoch: [166][600/699]\\tLoss 0.0659 (1.7123)\\tPrec@1 93.750 (89.715)\nEpoch: [166][698/699]\\tLoss 0.0187 (1.9263)\\tPrec@1 100.000 (89.720)\n * Prec@1 74.400\nEpoch: [167][0/699]\\tLoss 0.7163 (0.7163)\\tPrec@1 81.250 (81.250)\nEpoch: [167][100/699]\\tLoss 1.3297 (1.4920)\\tPrec@1 93.750 (90.408)\nEpoch: [167][200/699]\\tLoss 0.0881 (1.6495)\\tPrec@1 93.750 (89.459)\nEpoch: [167][300/699]\\tLoss 2.1093 (1.5974)\\tPrec@1 87.500 (89.971)\nEpoch: [167][400/699]\\tLoss 0.1851 (1.6117)\\tPrec@1 87.500 (89.744)\nEpoch: [167][500/699]\\tLoss 0.0890 (1.5803)\\tPrec@1 93.750 (89.733)\nEpoch: [167][600/699]\\tLoss 0.4716 (1.6605)\\tPrec@1 93.750 (89.621)\nEpoch: [167][698/699]\\tLoss 0.1055 (1.6910)\\tPrec@1 88.889 (89.568)\n * Prec@1 75.430\nEpoch: [168][0/699]\\tLoss 0.2057 (0.2057)\\tPrec@1 93.750 (93.750)\nEpoch: [168][100/699]\\tLoss 0.1983 (1.6047)\\tPrec@1 100.000 (89.851)\nEpoch: [168][200/699]\\tLoss 0.0932 (1.4205)\\tPrec@1 93.750 (89.521)\nEpoch: [168][300/699]\\tLoss 0.0829 (1.6575)\\tPrec@1 93.750 (89.410)\nEpoch: [168][400/699]\\tLoss 0.5227 (1.5657)\\tPrec@1 87.500 (89.573)\nEpoch: [168][500/699]\\tLoss 0.6574 (1.6348)\\tPrec@1 93.750 (89.671)\nEpoch: [168][600/699]\\tLoss 1.1169 (1.7022)\\tPrec@1 87.500 (89.486)\nEpoch: [168][698/699]\\tLoss 7.9522 (1.6702)\\tPrec@1 66.667 (89.308)\n * Prec@1 74.510\nEpoch: [169][0/699]\\tLoss 0.5565 (0.5565)\\tPrec@1 93.750 (93.750)\nEpoch: [169][100/699]\\tLoss 0.3886 (1.8460)\\tPrec@1 87.500 (89.604)\nEpoch: [169][200/699]\\tLoss 0.5236 (1.6303)\\tPrec@1 93.750 (89.335)\nEpoch: [169][300/699]\\tLoss 1.0484 (1.6484)\\tPrec@1 81.250 (89.120)\nEpoch: [169][400/699]\\tLoss 0.2357 (1.6125)\\tPrec@1 87.500 (88.872)\nEpoch: [169][500/699]\\tLoss 0.2559 (1.6037)\\tPrec@1 100.000 (88.885)\nEpoch: [169][600/699]\\tLoss 0.3111 (1.5585)\\tPrec@1 93.750 (88.842)\nEpoch: [169][698/699]\\tLoss 0.0226 (1.5642)\\tPrec@1 100.000 (88.718)\n * Prec@1 75.300\nEpoch: [170][0/699]\\tLoss 1.4662 (1.4662)\\tPrec@1 87.500 (87.500)\nEpoch: [170][100/699]\\tLoss 0.9175 (1.5726)\\tPrec@1 87.500 (87.314)\nEpoch: [170][200/699]\\tLoss 3.6664 (1.4596)\\tPrec@1 87.500 (87.655)\nEpoch: [170][300/699]\\tLoss 1.3655 (1.3956)\\tPrec@1 81.250 (88.040)\nEpoch: [170][400/699]\\tLoss 0.8139 (1.5661)\\tPrec@1 87.500 (88.108)\nEpoch: [170][500/699]\\tLoss 0.9942 (1.6276)\\tPrec@1 87.500 (88.161)\nEpoch: [170][600/699]\\tLoss 0.1189 (1.5693)\\tPrec@1 100.000 (88.540)\nEpoch: [170][698/699]\\tLoss 0.8447 (1.5281)\\tPrec@1 77.778 (88.628)\n * Prec@1 76.120\nEpoch: [171][0/699]\\tLoss 6.5718 (6.5718)\\tPrec@1 75.000 (75.000)\nEpoch: [171][100/699]\\tLoss 2.1181 (1.6485)\\tPrec@1 81.250 (88.366)\nEpoch: [171][200/699]\\tLoss 0.6193 (1.3778)\\tPrec@1 68.750 (88.184)\nEpoch: [171][300/699]\\tLoss 0.0922 (1.3946)\\tPrec@1 93.750 (88.227)\nEpoch: [171][400/699]\\tLoss 0.1524 (1.4127)\\tPrec@1 87.500 (87.921)\nEpoch: [171][500/699]\\tLoss 0.2745 (1.3768)\\tPrec@1 93.750 (88.086)\nEpoch: [171][600/699]\\tLoss 0.3055 (1.4464)\\tPrec@1 81.250 (88.280)\nEpoch: [171][698/699]\\tLoss 0.0526 (1.4924)\\tPrec@1 100.000 (88.047)\n * Prec@1 76.700\nEpoch: [172][0/699]\\tLoss 0.3887 (0.3887)\\tPrec@1 93.750 (93.750)\nEpoch: [172][100/699]\\tLoss 0.2027 (1.5394)\\tPrec@1 87.500 (88.119)\nEpoch: [172][200/699]\\tLoss 0.1847 (1.5740)\\tPrec@1 81.250 (87.873)\nEpoch: [172][300/699]\\tLoss 0.2352 (1.5677)\\tPrec@1 81.250 (87.749)\nEpoch: [172][400/699]\\tLoss 0.3354 (1.5255)\\tPrec@1 93.750 (87.531)\nEpoch: [172][500/699]\\tLoss 0.1250 (1.4721)\\tPrec@1 93.750 (87.725)\nEpoch: [172][600/699]\\tLoss 0.1428 (1.4599)\\tPrec@1 100.000 (87.583)\nEpoch: [172][698/699]\\tLoss 0.9456 (1.4561)\\tPrec@1 66.667 (87.635)\n * Prec@1 76.780\nEpoch: [173][0/699]\\tLoss 0.5411 (0.5411)\\tPrec@1 81.250 (81.250)\nEpoch: [173][100/699]\\tLoss 0.4966 (1.1719)\\tPrec@1 93.750 (87.005)\nEpoch: [173][200/699]\\tLoss 0.2048 (1.0220)\\tPrec@1 87.500 (87.687)\nEpoch: [173][300/699]\\tLoss 0.5392 (1.0585)\\tPrec@1 87.500 (87.479)\nEpoch: [173][400/699]\\tLoss 1.3865 (1.2857)\\tPrec@1 81.250 (87.126)\nEpoch: [173][500/699]\\tLoss 0.4742 (1.3083)\\tPrec@1 93.750 (87.163)\nEpoch: [173][600/699]\\tLoss 1.0314 (1.2828)\\tPrec@1 87.500 (87.386)\nEpoch: [173][698/699]\\tLoss 0.6007 (1.3399)\\tPrec@1 100.000 (87.322)\n * Prec@1 76.130\nEpoch: [174][0/699]\\tLoss 0.2882 (0.2882)\\tPrec@1 93.750 (93.750)\nEpoch: [174][100/699]\\tLoss 0.3446 (1.3084)\\tPrec@1 93.750 (86.881)\nEpoch: [174][200/699]\\tLoss 1.3217 (1.2796)\\tPrec@1 81.250 (86.785)\nEpoch: [174][300/699]\\tLoss 5.4130 (1.3155)\\tPrec@1 81.250 (87.105)\nEpoch: [174][400/699]\\tLoss 0.0740 (1.2920)\\tPrec@1 100.000 (87.500)\nEpoch: [174][500/699]\\tLoss 0.7455 (1.3025)\\tPrec@1 93.750 (87.213)\nEpoch: [174][600/699]\\tLoss 1.0148 (1.4098)\\tPrec@1 75.000 (87.126)\nEpoch: [174][698/699]\\tLoss 0.4419 (1.4148)\\tPrec@1 77.778 (87.054)\n * Prec@1 76.950\nEpoch: [175][0/699]\\tLoss 2.3193 (2.3193)\\tPrec@1 68.750 (68.750)\nEpoch: [175][100/699]\\tLoss 1.3887 (1.1311)\\tPrec@1 87.500 (87.252)\nEpoch: [175][200/699]\\tLoss 0.2056 (1.4787)\\tPrec@1 87.500 (86.816)\nEpoch: [175][300/699]\\tLoss 0.3433 (1.7204)\\tPrec@1 81.250 (86.400)\nEpoch: [175][400/699]\\tLoss 0.2999 (1.5994)\\tPrec@1 87.500 (86.425)\nEpoch: [175][500/699]\\tLoss 1.8847 (1.5729)\\tPrec@1 81.250 (86.090)\nEpoch: [175][600/699]\\tLoss 1.7992 (1.4830)\\tPrec@1 87.500 (86.304)\nEpoch: [175][698/699]\\tLoss 19.2250 (1.4562)\\tPrec@1 77.778 (86.079)\n * Prec@1 77.150\nEpoch: [176][0/699]\\tLoss 3.9682 (3.9682)\\tPrec@1 87.500 (87.500)\nEpoch: [176][100/699]\\tLoss 2.3953 (1.3209)\\tPrec@1 75.000 (85.767)\nEpoch: [176][200/699]\\tLoss 0.3629 (1.3255)\\tPrec@1 81.250 (86.132)\nEpoch: [176][300/699]\\tLoss 0.5134 (1.2715)\\tPrec@1 93.750 (86.150)\nEpoch: [176][400/699]\\tLoss 0.6185 (1.3260)\\tPrec@1 87.500 (85.941)\nEpoch: [176][500/699]\\tLoss 1.6323 (1.3321)\\tPrec@1 75.000 (86.053)\nEpoch: [176][600/699]\\tLoss 0.3624 (1.3246)\\tPrec@1 87.500 (85.847)\nEpoch: [176][698/699]\\tLoss 0.4549 (1.3263)\\tPrec@1 88.889 (85.846)\n * Prec@1 76.910\nEpoch: [177][0/699]\\tLoss 1.1400 (1.1400)\\tPrec@1 87.500 (87.500)\nEpoch: [177][100/699]\\tLoss 3.0078 (1.3982)\\tPrec@1 75.000 (85.582)\nEpoch: [177][200/699]\\tLoss 0.3807 (1.2266)\\tPrec@1 68.750 (85.759)\nEpoch: [177][300/699]\\tLoss 4.2111 (1.2700)\\tPrec@1 81.250 (85.569)\nEpoch: [177][400/699]\\tLoss 13.2000 (1.3865)\\tPrec@1 81.250 (85.396)\nEpoch: [177][500/699]\\tLoss 0.2242 (1.2824)\\tPrec@1 87.500 (85.367)\nEpoch: [177][600/699]\\tLoss 0.1466 (1.3150)\\tPrec@1 93.750 (85.420)\nEpoch: [177][698/699]\\tLoss 0.8187 (1.3175)\\tPrec@1 66.667 (85.470)\n * Prec@1 76.980\nEpoch: [178][0/699]\\tLoss 0.7208 (0.7208)\\tPrec@1 93.750 (93.750)\nEpoch: [178][100/699]\\tLoss 1.4800 (1.3082)\\tPrec@1 75.000 (85.829)\nEpoch: [178][200/699]\\tLoss 1.3242 (1.2701)\\tPrec@1 87.500 (85.386)\nEpoch: [178][300/699]\\tLoss 0.6801 (1.2797)\\tPrec@1 81.250 (85.361)\nEpoch: [178][400/699]\\tLoss 0.5049 (1.3030)\\tPrec@1 87.500 (84.959)\nEpoch: [178][500/699]\\tLoss 2.2284 (1.3033)\\tPrec@1 81.250 (84.893)\nEpoch: [178][600/699]\\tLoss 3.1481 (1.2925)\\tPrec@1 87.500 (84.921)\nEpoch: [178][698/699]\\tLoss 0.2202 (1.3321)\\tPrec@1 88.889 (84.853)\n * Prec@1 77.370\nEpoch: [179][0/699]\\tLoss 0.1243 (0.1243)\\tPrec@1 100.000 (100.000)\nEpoch: [179][100/699]\\tLoss 0.9765 (1.2676)\\tPrec@1 81.250 (84.406)\nEpoch: [179][200/699]\\tLoss 0.2889 (1.2004)\\tPrec@1 81.250 (84.608)\nEpoch: [179][300/699]\\tLoss 0.4786 (1.2606)\\tPrec@1 87.500 (84.697)\nEpoch: [179][400/699]\\tLoss 6.2935 (1.3313)\\tPrec@1 81.250 (84.913)\nEpoch: [179][500/699]\\tLoss 1.1769 (1.3524)\\tPrec@1 93.750 (84.693)\nEpoch: [179][600/699]\\tLoss 0.6873 (1.3669)\\tPrec@1 75.000 (84.630)\nEpoch: [179][698/699]\\tLoss 0.0952 (1.3271)\\tPrec@1 88.889 (84.692)\n * Prec@1 77.320\nEpoch: [180][0/699]\\tLoss 1.2303 (1.2303)\\tPrec@1 87.500 (87.500)\nEpoch: [180][100/699]\\tLoss 0.3045 (1.2522)\\tPrec@1 93.750 (85.087)\nEpoch: [180][200/699]\\tLoss 0.6448 (1.3108)\\tPrec@1 81.250 (84.515)\nEpoch: [180][300/699]\\tLoss 0.5695 (1.2329)\\tPrec@1 93.750 (84.510)\nEpoch: [180][400/699]\\tLoss 1.6896 (1.2582)\\tPrec@1 81.250 (84.913)\nEpoch: [180][500/699]\\tLoss 8.6160 (1.3082)\\tPrec@1 81.250 (84.955)\nEpoch: [180][600/699]\\tLoss 4.6383 (1.2478)\\tPrec@1 75.000 (85.004)\nEpoch: [180][698/699]\\tLoss 4.2712 (1.2681)\\tPrec@1 77.778 (85.041)\n * Prec@1 77.250\nEpoch: [181][0/699]\\tLoss 0.5726 (0.5726)\\tPrec@1 81.250 (81.250)\nEpoch: [181][100/699]\\tLoss 0.4191 (1.2156)\\tPrec@1 87.500 (85.210)\nEpoch: [181][200/699]\\tLoss 1.4122 (1.0481)\\tPrec@1 87.500 (85.230)\nEpoch: [181][300/699]\\tLoss 0.3184 (1.1993)\\tPrec@1 87.500 (84.385)\nEpoch: [181][400/699]\\tLoss 0.3529 (1.2672)\\tPrec@1 87.500 (84.289)\nEpoch: [181][500/699]\\tLoss 0.7814 (1.2579)\\tPrec@1 81.250 (84.568)\nEpoch: [181][600/699]\\tLoss 0.8748 (1.2836)\\tPrec@1 87.500 (84.588)\nEpoch: [181][698/699]\\tLoss 0.0852 (1.2712)\\tPrec@1 100.000 (84.423)\n * Prec@1 76.830\nEpoch: [182][0/699]\\tLoss 1.5411 (1.5411)\\tPrec@1 87.500 (87.500)\nEpoch: [182][100/699]\\tLoss 0.2832 (1.4509)\\tPrec@1 87.500 (84.468)\nEpoch: [182][200/699]\\tLoss 4.4729 (1.2520)\\tPrec@1 87.500 (84.173)\nEpoch: [182][300/699]\\tLoss 0.5477 (1.2827)\\tPrec@1 81.250 (83.679)\nEpoch: [182][400/699]\\tLoss 0.5971 (1.2940)\\tPrec@1 75.000 (83.775)\nEpoch: [182][500/699]\\tLoss 0.2790 (1.2541)\\tPrec@1 100.000 (84.219)\nEpoch: [182][600/699]\\tLoss 3.3828 (1.2359)\\tPrec@1 75.000 (84.266)\nEpoch: [182][698/699]\\tLoss 2.4451 (1.2483)\\tPrec@1 88.889 (84.370)\n * Prec@1 77.320\nEpoch: [183][0/699]\\tLoss 0.4137 (0.4137)\\tPrec@1 87.500 (87.500)\nEpoch: [183][100/699]\\tLoss 0.2240 (1.2663)\\tPrec@1 75.000 (81.559)\nEpoch: [183][200/699]\\tLoss 2.1673 (1.3165)\\tPrec@1 87.500 (82.743)\nEpoch: [183][300/699]\\tLoss 0.0931 (1.2697)\\tPrec@1 93.750 (83.721)\nEpoch: [183][400/699]\\tLoss 0.5317 (1.2473)\\tPrec@1 81.250 (83.931)\nEpoch: [183][500/699]\\tLoss 0.0800 (1.2625)\\tPrec@1 100.000 (84.007)\nEpoch: [183][600/699]\\tLoss 0.5361 (1.2512)\\tPrec@1 87.500 (84.214)\nEpoch: [183][698/699]\\tLoss 0.1321 (1.2239)\\tPrec@1 100.000 (84.137)\n * Prec@1 77.200\nEpoch: [184][0/699]\\tLoss 0.3162 (0.3162)\\tPrec@1 93.750 (93.750)\nEpoch: [184][100/699]\\tLoss 1.5892 (1.2284)\\tPrec@1 68.750 (83.973)\nEpoch: [184][200/699]\\tLoss 0.8186 (1.3364)\\tPrec@1 75.000 (83.551)\nEpoch: [184][300/699]\\tLoss 0.1173 (1.2729)\\tPrec@1 93.750 (83.555)\nEpoch: [184][400/699]\\tLoss 0.3203 (1.2830)\\tPrec@1 75.000 (83.791)\nEpoch: [184][500/699]\\tLoss 1.2705 (1.2789)\\tPrec@1 87.500 (84.069)\nEpoch: [184][600/699]\\tLoss 1.6962 (1.2508)\\tPrec@1 75.000 (83.995)\nEpoch: [184][698/699]\\tLoss 0.2160 (1.2955)\\tPrec@1 77.778 (83.967)\n * Prec@1 77.220\nEpoch: [185][0/699]\\tLoss 0.2185 (0.2185)\\tPrec@1 81.250 (81.250)\nEpoch: [185][100/699]\\tLoss 0.2197 (1.0895)\\tPrec@1 87.500 (85.705)\nEpoch: [185][200/699]\\tLoss 4.0886 (1.2451)\\tPrec@1 75.000 (84.733)\nEpoch: [185][300/699]\\tLoss 0.5472 (1.2852)\\tPrec@1 81.250 (84.489)\nEpoch: [185][400/699]\\tLoss 1.9836 (1.2483)\\tPrec@1 75.000 (84.087)\nEpoch: [185][500/699]\\tLoss 1.1056 (1.2348)\\tPrec@1 81.250 (84.107)\nEpoch: [185][600/699]\\tLoss 1.3330 (1.2417)\\tPrec@1 75.000 (84.172)\nEpoch: [185][698/699]\\tLoss 0.3056 (1.2531)\\tPrec@1 77.778 (84.092)\n * Prec@1 77.130\nEpoch: [186][0/699]\\tLoss 0.3938 (0.3938)\\tPrec@1 93.750 (93.750)\nEpoch: [186][100/699]\\tLoss 0.2525 (1.0162)\\tPrec@1 81.250 (86.324)\nEpoch: [186][200/699]\\tLoss 0.8548 (1.1165)\\tPrec@1 81.250 (85.261)\nEpoch: [186][300/699]\\tLoss 0.2869 (1.2583)\\tPrec@1 87.500 (85.008)\nEpoch: [186][400/699]\\tLoss 1.5848 (1.2340)\\tPrec@1 68.750 (84.695)\nEpoch: [186][500/699]\\tLoss 0.5280 (1.2435)\\tPrec@1 87.500 (84.444)\nEpoch: [186][600/699]\\tLoss 0.1638 (1.2739)\\tPrec@1 93.750 (84.214)\nEpoch: [186][698/699]\\tLoss 0.4776 (1.2870)\\tPrec@1 66.667 (83.976)\n * Prec@1 77.350\nEpoch: [187][0/699]\\tLoss 0.4805 (0.4805)\\tPrec@1 75.000 (75.000)\nEpoch: [187][100/699]\\tLoss 0.1514 (1.3267)\\tPrec@1 93.750 (82.921)\nEpoch: [187][200/699]\\tLoss 0.5050 (1.3638)\\tPrec@1 93.750 (82.463)\nEpoch: [187][300/699]\\tLoss 0.2272 (1.2558)\\tPrec@1 100.000 (82.911)\nEpoch: [187][400/699]\\tLoss 0.2514 (1.2793)\\tPrec@1 87.500 (82.793)\nEpoch: [187][500/699]\\tLoss 0.2387 (1.2827)\\tPrec@1 93.750 (83.034)\nEpoch: [187][600/699]\\tLoss 0.8373 (1.2810)\\tPrec@1 87.500 (82.955)\nEpoch: [187][698/699]\\tLoss 0.7090 (1.3338)\\tPrec@1 55.556 (82.893)\n * Prec@1 77.260\nEpoch: [188][0/699]\\tLoss 0.3352 (0.3352)\\tPrec@1 75.000 (75.000)\nEpoch: [188][100/699]\\tLoss 0.4495 (1.0200)\\tPrec@1 75.000 (83.911)\nEpoch: [188][200/699]\\tLoss 0.3439 (1.1268)\\tPrec@1 81.250 (83.893)\nEpoch: [188][300/699]\\tLoss 0.6911 (1.2396)\\tPrec@1 81.250 (83.804)\nEpoch: [188][400/699]\\tLoss 1.2260 (1.2509)\\tPrec@1 68.750 (83.619)\nEpoch: [188][500/699]\\tLoss 0.5182 (1.2831)\\tPrec@1 93.750 (83.770)\nEpoch: [188][600/699]\\tLoss 1.7974 (1.3002)\\tPrec@1 81.250 (83.475)\nEpoch: [188][698/699]\\tLoss 0.3858 (1.2778)\\tPrec@1 66.667 (83.627)\n * Prec@1 77.030\nEpoch: [189][0/699]\\tLoss 5.9572 (5.9572)\\tPrec@1 75.000 (75.000)\nEpoch: [189][100/699]\\tLoss 0.0540 (1.6107)\\tPrec@1 100.000 (82.240)\nEpoch: [189][200/699]\\tLoss 0.3162 (1.4011)\\tPrec@1 75.000 (81.965)\nEpoch: [189][300/699]\\tLoss 0.5885 (1.3420)\\tPrec@1 87.500 (82.766)\nEpoch: [189][400/699]\\tLoss 0.3638 (1.3005)\\tPrec@1 75.000 (82.918)\nEpoch: [189][500/699]\\tLoss 0.6271 (1.2920)\\tPrec@1 87.500 (82.884)\nEpoch: [189][600/699]\\tLoss 2.9776 (1.2766)\\tPrec@1 75.000 (83.174)\nEpoch: [189][698/699]\\tLoss 0.2181 (1.2552)\\tPrec@1 100.000 (83.207)\n * Prec@1 76.940\nEpoch: [190][0/699]\\tLoss 1.7784 (1.7784)\\tPrec@1 87.500 (87.500)\nEpoch: [190][100/699]\\tLoss 0.6551 (1.1776)\\tPrec@1 81.250 (83.725)\nEpoch: [190][200/699]\\tLoss 0.2414 (1.2291)\\tPrec@1 93.750 (83.831)\nEpoch: [190][300/699]\\tLoss 1.5256 (1.1988)\\tPrec@1 87.500 (83.617)\nEpoch: [190][400/699]\\tLoss 0.6949 (1.2324)\\tPrec@1 93.750 (83.339)\nEpoch: [190][500/699]\\tLoss 1.0073 (1.2119)\\tPrec@1 81.250 (83.371)\nEpoch: [190][600/699]\\tLoss 2.7176 (1.2445)\\tPrec@1 75.000 (83.267)\nEpoch: [190][698/699]\\tLoss 0.0589 (1.2324)\\tPrec@1 88.889 (83.296)\n * Prec@1 77.280\nEpoch: [191][0/699]\\tLoss 0.3572 (0.3572)\\tPrec@1 87.500 (87.500)\nEpoch: [191][100/699]\\tLoss 0.9921 (1.3873)\\tPrec@1 75.000 (82.364)\nEpoch: [191][200/699]\\tLoss 0.3724 (1.3938)\\tPrec@1 93.750 (82.400)\nEpoch: [191][300/699]\\tLoss 0.4125 (1.2836)\\tPrec@1 93.750 (82.973)\nEpoch: [191][400/699]\\tLoss 0.3874 (1.2660)\\tPrec@1 81.250 (83.198)\nEpoch: [191][500/699]\\tLoss 0.3132 (1.2783)\\tPrec@1 68.750 (83.084)\nEpoch: [191][600/699]\\tLoss 0.6284 (1.2584)\\tPrec@1 62.500 (83.299)\nEpoch: [191][698/699]\\tLoss 0.5135 (1.2960)\\tPrec@1 44.444 (83.350)\n * Prec@1 77.360\nEpoch: [192][0/699]\\tLoss 0.2975 (0.2975)\\tPrec@1 81.250 (81.250)\nEpoch: [192][100/699]\\tLoss 0.4748 (1.1373)\\tPrec@1 87.500 (83.973)\nEpoch: [192][200/699]\\tLoss 0.3400 (1.3137)\\tPrec@1 81.250 (83.147)\nEpoch: [192][300/699]\\tLoss 1.1882 (1.3747)\\tPrec@1 81.250 (83.223)\nEpoch: [192][400/699]\\tLoss 1.5827 (1.3695)\\tPrec@1 81.250 (83.307)\nEpoch: [192][500/699]\\tLoss 0.2593 (1.3366)\\tPrec@1 93.750 (83.059)\nEpoch: [192][600/699]\\tLoss 1.1882 (1.2959)\\tPrec@1 87.500 (83.070)\nEpoch: [192][698/699]\\tLoss 10.3226 (1.3059)\\tPrec@1 66.667 (82.920)\n * Prec@1 77.210\nEpoch: [193][0/699]\\tLoss 0.1784 (0.1784)\\tPrec@1 93.750 (93.750)\nEpoch: [193][100/699]\\tLoss 0.0613 (1.4308)\\tPrec@1 100.000 (82.426)\nEpoch: [193][200/699]\\tLoss 2.2424 (1.3072)\\tPrec@1 87.500 (82.214)\nEpoch: [193][300/699]\\tLoss 1.6045 (1.2678)\\tPrec@1 81.250 (82.953)\nEpoch: [193][400/699]\\tLoss 0.2611 (1.3702)\\tPrec@1 87.500 (82.731)\nEpoch: [193][500/699]\\tLoss 1.2168 (1.3306)\\tPrec@1 87.500 (82.822)\nEpoch: [193][600/699]\\tLoss 0.5211 (1.3271)\\tPrec@1 75.000 (82.623)\nEpoch: [193][698/699]\\tLoss 0.2274 (1.3575)\\tPrec@1 88.889 (82.580)\n * Prec@1 77.370\nEpoch: [194][0/699]\\tLoss 9.1491 (9.1491)\\tPrec@1 75.000 (75.000)\nEpoch: [194][100/699]\\tLoss 1.4066 (1.1659)\\tPrec@1 75.000 (82.673)\nEpoch: [194][200/699]\\tLoss 1.7158 (1.0806)\\tPrec@1 75.000 (83.427)\nEpoch: [194][300/699]\\tLoss 0.4215 (1.2225)\\tPrec@1 81.250 (83.015)\nEpoch: [194][400/699]\\tLoss 0.8541 (1.2178)\\tPrec@1 75.000 (83.058)\nEpoch: [194][500/699]\\tLoss 3.3690 (1.1895)\\tPrec@1 75.000 (82.797)\nEpoch: [194][600/699]\\tLoss 1.1500 (1.2308)\\tPrec@1 81.250 (82.841)\nEpoch: [194][698/699]\\tLoss 0.4276 (1.2919)\\tPrec@1 77.778 (82.947)\n * Prec@1 77.330\nEpoch: [195][0/699]\\tLoss 0.4044 (0.4044)\\tPrec@1 75.000 (75.000)\nEpoch: [195][100/699]\\tLoss 2.7411 (1.3067)\\tPrec@1 81.250 (82.859)\nEpoch: [195][200/699]\\tLoss 1.8038 (1.5079)\\tPrec@1 75.000 (82.090)\nEpoch: [195][300/699]\\tLoss 0.4157 (1.5425)\\tPrec@1 87.500 (82.413)\nEpoch: [195][400/699]\\tLoss 1.4701 (1.4421)\\tPrec@1 93.750 (82.855)\nEpoch: [195][500/699]\\tLoss 1.1743 (1.4230)\\tPrec@1 87.500 (82.759)\nEpoch: [195][600/699]\\tLoss 0.9186 (1.4047)\\tPrec@1 81.250 (82.768)\nEpoch: [195][698/699]\\tLoss 0.0782 (1.3552)\\tPrec@1 88.889 (82.893)\n * Prec@1 77.060\nEpoch: [196][0/699]\\tLoss 0.2843 (0.2843)\\tPrec@1 87.500 (87.500)\nEpoch: [196][100/699]\\tLoss 3.0463 (1.3038)\\tPrec@1 81.250 (82.797)\nEpoch: [196][200/699]\\tLoss 2.5135 (1.2423)\\tPrec@1 75.000 (83.085)\nEpoch: [196][300/699]\\tLoss 3.1929 (1.2164)\\tPrec@1 81.250 (83.015)\nEpoch: [196][400/699]\\tLoss 0.2912 (1.3157)\\tPrec@1 81.250 (82.824)\nEpoch: [196][500/699]\\tLoss 0.5900 (1.2826)\\tPrec@1 75.000 (82.635)\nEpoch: [196][600/699]\\tLoss 3.6591 (1.3120)\\tPrec@1 75.000 (82.456)\nEpoch: [196][698/699]\\tLoss 0.5580 (1.2854)\\tPrec@1 66.667 (82.509)\n * Prec@1 77.020\nEpoch: [197][0/699]\\tLoss 2.2405 (2.2405)\\tPrec@1 75.000 (75.000)\nEpoch: [197][100/699]\\tLoss 1.7096 (1.2197)\\tPrec@1 81.250 (83.911)\nEpoch: [197][200/699]\\tLoss 0.2780 (1.3301)\\tPrec@1 75.000 (83.427)\nEpoch: [197][300/699]\\tLoss 1.8749 (1.2858)\\tPrec@1 50.000 (82.890)\nEpoch: [197][400/699]\\tLoss 2.5139 (1.3408)\\tPrec@1 75.000 (82.403)\nEpoch: [197][500/699]\\tLoss 0.3230 (1.3174)\\tPrec@1 81.250 (82.148)\nEpoch: [197][600/699]\\tLoss 0.6037 (1.3147)\\tPrec@1 75.000 (82.488)\nEpoch: [197][698/699]\\tLoss 1.7266 (1.3154)\\tPrec@1 55.556 (82.598)\n * Prec@1 77.070\nEpoch: [198][0/699]\\tLoss 1.0598 (1.0598)\\tPrec@1 93.750 (93.750)\nEpoch: [198][100/699]\\tLoss 0.2364 (1.5041)\\tPrec@1 87.500 (82.859)\nEpoch: [198][200/699]\\tLoss 0.8944 (1.2930)\\tPrec@1 81.250 (83.427)\nEpoch: [198][300/699]\\tLoss 0.7010 (1.3252)\\tPrec@1 87.500 (82.787)\nEpoch: [198][400/699]\\tLoss 0.4971 (1.3162)\\tPrec@1 81.250 (82.902)\nEpoch: [198][500/699]\\tLoss 1.0336 (1.3269)\\tPrec@1 87.500 (82.897)\nEpoch: [198][600/699]\\tLoss 1.6236 (1.3172)\\tPrec@1 87.500 (82.779)\nEpoch: [198][698/699]\\tLoss 0.1305 (1.3057)\\tPrec@1 88.889 (82.741)\n * Prec@1 77.310\nEpoch: [199][0/699]\\tLoss 0.2053 (0.2053)\\tPrec@1 93.750 (93.750)\nEpoch: [199][100/699]\\tLoss 0.3637 (1.0543)\\tPrec@1 81.250 (82.116)\nEpoch: [199][200/699]\\tLoss 0.3641 (1.1632)\\tPrec@1 75.000 (81.685)\nEpoch: [199][300/699]\\tLoss 0.4154 (1.1614)\\tPrec@1 93.750 (81.873)\nEpoch: [199][400/699]\\tLoss 0.5063 (1.1680)\\tPrec@1 68.750 (82.668)\nEpoch: [199][500/699]\\tLoss 0.7966 (1.2478)\\tPrec@1 81.250 (82.385)\nEpoch: [199][600/699]\\tLoss 0.7628 (1.2661)\\tPrec@1 93.750 (82.394)\nEpoch: [199][698/699]\\tLoss 0.1125 (1.2324)\\tPrec@1 77.778 (82.464)\n * Prec@1 77.060\nBest accuracy:  77.37\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# **Imbalance Factor - 50**","metadata":{}},{"cell_type":"code","source":"# --- Run Stage 1 Pre-training (IF=50) ---\n\n!python pretrain_stage1.py \\\n--dataset cifar10 \\\n--imb_factor 0.02 \\\n--epochs 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:07:40.294446Z","iopub.execute_input":"2025-06-15T09:07:40.294771Z","iopub.status.idle":"2025-06-15T09:26:33.116325Z","shell.execute_reply.started":"2025-06-15T09:07:40.294728Z","shell.execute_reply":"2025-06-15T09:26:33.115624Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|████████████████| 110/110 [00:07<00:00, 15.51it/s, Loss=1.306]\nEpoch 2/200: 100%|████████████████| 110/110 [00:05<00:00, 19.12it/s, Loss=1.170]\nEpoch 3/200: 100%|████████████████| 110/110 [00:05<00:00, 18.93it/s, Loss=0.648]\nEpoch 4/200: 100%|████████████████| 110/110 [00:05<00:00, 18.93it/s, Loss=1.285]\nEpoch 5/200: 100%|████████████████| 110/110 [00:05<00:00, 19.22it/s, Loss=1.281]\nEpoch 6/200: 100%|████████████████| 110/110 [00:05<00:00, 19.25it/s, Loss=0.880]\nEpoch 7/200: 100%|████████████████| 110/110 [00:05<00:00, 19.65it/s, Loss=0.954]\nEpoch 8/200: 100%|████████████████| 110/110 [00:05<00:00, 19.79it/s, Loss=0.657]\nEpoch 9/200: 100%|████████████████| 110/110 [00:05<00:00, 19.80it/s, Loss=1.076]\nEpoch 10/200: 100%|███████████████| 110/110 [00:05<00:00, 19.91it/s, Loss=0.937]\nEpoch 11/200: 100%|███████████████| 110/110 [00:05<00:00, 19.83it/s, Loss=0.459]\nEpoch 12/200: 100%|███████████████| 110/110 [00:05<00:00, 19.87it/s, Loss=0.571]\nEpoch 13/200: 100%|███████████████| 110/110 [00:05<00:00, 19.83it/s, Loss=0.642]\nEpoch 14/200: 100%|███████████████| 110/110 [00:05<00:00, 19.70it/s, Loss=1.532]\nEpoch 15/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.650]\nEpoch 16/200: 100%|███████████████| 110/110 [00:05<00:00, 19.46it/s, Loss=0.471]\nEpoch 17/200: 100%|███████████████| 110/110 [00:05<00:00, 19.40it/s, Loss=0.533]\nEpoch 18/200: 100%|███████████████| 110/110 [00:05<00:00, 19.39it/s, Loss=0.290]\nEpoch 19/200: 100%|███████████████| 110/110 [00:05<00:00, 19.43it/s, Loss=1.087]\nEpoch 20/200: 100%|███████████████| 110/110 [00:05<00:00, 19.43it/s, Loss=0.948]\nEpoch 21/200: 100%|███████████████| 110/110 [00:05<00:00, 19.46it/s, Loss=0.524]\nEpoch 22/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.506]\nEpoch 23/200: 100%|███████████████| 110/110 [00:05<00:00, 19.42it/s, Loss=0.454]\nEpoch 24/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.646]\nEpoch 25/200: 100%|███████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.764]\nEpoch 26/200: 100%|███████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=1.092]\nEpoch 27/200: 100%|███████████████| 110/110 [00:05<00:00, 19.74it/s, Loss=1.016]\nEpoch 28/200: 100%|███████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.699]\nEpoch 29/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=1.366]\nEpoch 30/200: 100%|███████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.337]\nEpoch 31/200: 100%|███████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.345]\nEpoch 32/200: 100%|███████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.561]\nEpoch 33/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=1.370]\nEpoch 34/200: 100%|███████████████| 110/110 [00:05<00:00, 19.36it/s, Loss=0.624]\nEpoch 35/200: 100%|███████████████| 110/110 [00:05<00:00, 19.50it/s, Loss=0.580]\nEpoch 36/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.485]\nEpoch 37/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=0.660]\nEpoch 38/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.575]\nEpoch 39/200: 100%|███████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.497]\nEpoch 40/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.632]\nEpoch 41/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.297]\nEpoch 42/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.604]\nEpoch 43/200: 100%|███████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.801]\nEpoch 44/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.319]\nEpoch 45/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.437]\nEpoch 46/200: 100%|███████████████| 110/110 [00:05<00:00, 19.50it/s, Loss=1.063]\nEpoch 47/200: 100%|███████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.387]\nEpoch 48/200: 100%|███████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.265]\nEpoch 49/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.055]\nEpoch 50/200: 100%|███████████████| 110/110 [00:05<00:00, 19.42it/s, Loss=0.639]\nEpoch 51/200: 100%|███████████████| 110/110 [00:05<00:00, 19.43it/s, Loss=0.604]\nEpoch 52/200: 100%|███████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.359]\nEpoch 53/200: 100%|███████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.425]\nEpoch 54/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.290]\nEpoch 55/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.146]\nEpoch 56/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.708]\nEpoch 57/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=0.891]\nEpoch 58/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=1.079]\nEpoch 59/200: 100%|███████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.481]\nEpoch 60/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.198]\nEpoch 61/200: 100%|███████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.229]\nEpoch 62/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.768]\nEpoch 63/200: 100%|███████████████| 110/110 [00:05<00:00, 19.38it/s, Loss=0.256]\nEpoch 64/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.407]\nEpoch 65/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.682]\nEpoch 66/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.470]\nEpoch 67/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=1.249]\nEpoch 68/200: 100%|███████████████| 110/110 [00:05<00:00, 19.44it/s, Loss=0.847]\nEpoch 69/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=0.296]\nEpoch 70/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.303]\nEpoch 71/200: 100%|███████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.489]\nEpoch 72/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.229]\nEpoch 73/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=1.311]\nEpoch 74/200: 100%|███████████████| 110/110 [00:05<00:00, 19.42it/s, Loss=0.646]\nEpoch 75/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=0.434]\nEpoch 76/200: 100%|███████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.174]\nEpoch 77/200: 100%|███████████████| 110/110 [00:05<00:00, 19.60it/s, Loss=0.752]\nEpoch 78/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.757]\nEpoch 79/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.822]\nEpoch 80/200: 100%|███████████████| 110/110 [00:05<00:00, 19.28it/s, Loss=0.432]\nEpoch 81/200: 100%|███████████████| 110/110 [00:05<00:00, 19.46it/s, Loss=0.392]\nEpoch 82/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.201]\nEpoch 83/200: 100%|███████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.378]\nEpoch 84/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.889]\nEpoch 85/200: 100%|███████████████| 110/110 [00:05<00:00, 19.49it/s, Loss=0.419]\nEpoch 86/200: 100%|███████████████| 110/110 [00:05<00:00, 19.41it/s, Loss=0.627]\nEpoch 87/200: 100%|███████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.480]\nEpoch 88/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.602]\nEpoch 89/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.427]\nEpoch 90/200: 100%|███████████████| 110/110 [00:05<00:00, 19.49it/s, Loss=0.230]\nEpoch 91/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.528]\nEpoch 92/200: 100%|███████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.456]\nEpoch 93/200: 100%|███████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.463]\nEpoch 94/200: 100%|███████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.332]\nEpoch 95/200: 100%|███████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.513]\nEpoch 96/200: 100%|███████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=0.494]\nEpoch 97/200: 100%|███████████████| 110/110 [00:05<00:00, 19.50it/s, Loss=0.494]\nEpoch 98/200: 100%|███████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.187]\nEpoch 99/200: 100%|███████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.243]\nEpoch 100/200: 100%|██████████████| 110/110 [00:05<00:00, 19.60it/s, Loss=0.594]\nEpoch 101/200: 100%|██████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.543]\nEpoch 102/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.535]\nEpoch 103/200: 100%|██████████████| 110/110 [00:05<00:00, 19.30it/s, Loss=0.816]\nEpoch 104/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.317]\nEpoch 105/200: 100%|██████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.312]\nEpoch 106/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.140]\nEpoch 107/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.471]\nEpoch 108/200: 100%|██████████████| 110/110 [00:05<00:00, 19.28it/s, Loss=0.237]\nEpoch 109/200: 100%|██████████████| 110/110 [00:05<00:00, 19.60it/s, Loss=0.619]\nEpoch 110/200: 100%|██████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.743]\nEpoch 111/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.236]\nEpoch 112/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.579]\nEpoch 113/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.118]\nEpoch 114/200: 100%|██████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.199]\nEpoch 115/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.423]\nEpoch 116/200: 100%|██████████████| 110/110 [00:05<00:00, 19.64it/s, Loss=0.087]\nEpoch 117/200: 100%|██████████████| 110/110 [00:05<00:00, 19.65it/s, Loss=0.547]\nEpoch 118/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.426]\nEpoch 119/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.860]\nEpoch 120/200: 100%|██████████████| 110/110 [00:05<00:00, 19.36it/s, Loss=0.284]\nEpoch 121/200: 100%|██████████████| 110/110 [00:05<00:00, 19.51it/s, Loss=0.244]\nEpoch 122/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.407]\nEpoch 123/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.332]\nEpoch 124/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.125]\nEpoch 125/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.254]\nEpoch 126/200: 100%|██████████████| 110/110 [00:05<00:00, 19.43it/s, Loss=0.439]\nEpoch 127/200: 100%|██████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.463]\nEpoch 128/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.659]\nEpoch 129/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=1.104]\nEpoch 130/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.538]\nEpoch 131/200: 100%|██████████████| 110/110 [00:05<00:00, 19.47it/s, Loss=0.528]\nEpoch 132/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.266]\nEpoch 133/200: 100%|██████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.222]\nEpoch 134/200: 100%|██████████████| 110/110 [00:05<00:00, 19.67it/s, Loss=0.565]\nEpoch 135/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.359]\nEpoch 136/200: 100%|██████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.105]\nEpoch 137/200: 100%|██████████████| 110/110 [00:05<00:00, 19.45it/s, Loss=0.346]\nEpoch 138/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.456]\nEpoch 139/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.219]\nEpoch 140/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.428]\nEpoch 141/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.171]\nEpoch 142/200: 100%|██████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.317]\nEpoch 143/200: 100%|██████████████| 110/110 [00:05<00:00, 19.33it/s, Loss=0.236]\nEpoch 144/200: 100%|██████████████| 110/110 [00:05<00:00, 19.53it/s, Loss=0.455]\nEpoch 145/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=1.243]\nEpoch 146/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.219]\nEpoch 147/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.390]\nEpoch 148/200: 100%|██████████████| 110/110 [00:05<00:00, 19.59it/s, Loss=0.839]\nEpoch 149/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.311]\nEpoch 150/200: 100%|██████████████| 110/110 [00:05<00:00, 19.64it/s, Loss=0.079]\nEpoch 151/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.225]\nEpoch 152/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.091]\nEpoch 153/200: 100%|██████████████| 110/110 [00:05<00:00, 19.45it/s, Loss=0.351]\nEpoch 154/200: 100%|██████████████| 110/110 [00:05<00:00, 19.52it/s, Loss=1.102]\nEpoch 155/200: 100%|██████████████| 110/110 [00:05<00:00, 19.66it/s, Loss=0.517]\nEpoch 156/200: 100%|██████████████| 110/110 [00:05<00:00, 19.64it/s, Loss=0.946]\nEpoch 157/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.252]\nEpoch 158/200: 100%|██████████████| 110/110 [00:05<00:00, 19.67it/s, Loss=0.388]\nEpoch 159/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.629]\nEpoch 160/200: 100%|██████████████| 110/110 [00:05<00:00, 19.36it/s, Loss=0.524]\nEpoch 161/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.217]\nEpoch 162/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.141]\nEpoch 163/200: 100%|██████████████| 110/110 [00:05<00:00, 19.66it/s, Loss=0.117]\nEpoch 164/200: 100%|██████████████| 110/110 [00:05<00:00, 19.68it/s, Loss=0.201]\nEpoch 165/200: 100%|██████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.259]\nEpoch 166/200: 100%|██████████████| 110/110 [00:05<00:00, 19.41it/s, Loss=0.795]\nEpoch 167/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.492]\nEpoch 168/200: 100%|██████████████| 110/110 [00:05<00:00, 19.64it/s, Loss=0.022]\nEpoch 169/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.111]\nEpoch 170/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.006]\nEpoch 171/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.206]\nEpoch 172/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.302]\nEpoch 173/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.913]\nEpoch 174/200: 100%|██████████████| 110/110 [00:05<00:00, 19.60it/s, Loss=0.914]\nEpoch 175/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.004]\nEpoch 176/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.149]\nEpoch 177/200: 100%|██████████████| 110/110 [00:05<00:00, 19.55it/s, Loss=0.198]\nEpoch 178/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.079]\nEpoch 179/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.004]\nEpoch 180/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.076]\nEpoch 181/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.043]\nEpoch 182/200: 100%|██████████████| 110/110 [00:05<00:00, 19.49it/s, Loss=0.461]\nEpoch 183/200: 100%|██████████████| 110/110 [00:05<00:00, 19.36it/s, Loss=0.198]\nEpoch 184/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.242]\nEpoch 185/200: 100%|██████████████| 110/110 [00:05<00:00, 19.50it/s, Loss=0.112]\nEpoch 186/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.037]\nEpoch 187/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.060]\nEpoch 188/200: 100%|██████████████| 110/110 [00:05<00:00, 19.33it/s, Loss=0.020]\nEpoch 189/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.023]\nEpoch 190/200: 100%|██████████████| 110/110 [00:05<00:00, 19.58it/s, Loss=0.003]\nEpoch 191/200: 100%|██████████████| 110/110 [00:05<00:00, 19.56it/s, Loss=0.010]\nEpoch 192/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.007]\nEpoch 193/200: 100%|██████████████| 110/110 [00:05<00:00, 19.61it/s, Loss=0.048]\nEpoch 194/200: 100%|██████████████| 110/110 [00:05<00:00, 19.54it/s, Loss=0.262]\nEpoch 195/200: 100%|██████████████| 110/110 [00:05<00:00, 19.57it/s, Loss=0.029]\nEpoch 196/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.211]\nEpoch 197/200: 100%|██████████████| 110/110 [00:05<00:00, 19.62it/s, Loss=0.150]\nEpoch 198/200: 100%|██████████████| 110/110 [00:05<00:00, 19.63it/s, Loss=0.135]\nEpoch 199/200: 100%|██████████████| 110/110 [00:05<00:00, 19.60it/s, Loss=0.031]\nEpoch 200/200: 100%|██████████████| 110/110 [00:05<00:00, 19.44it/s, Loss=0.058]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar10_imb0.02_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- Run Stage 2 Re-weighting (IF=50) ---\n\n!python OT_train.py \\\n--dataset cifar10 \\\n--imb_factor 0.02 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar10_IF50_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar10_imb0.02_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:26:33.117349Z","iopub.execute_input":"2025-06-15T09:26:33.117592Z","iopub.status.idle":"2025-06-15T09:43:44.690739Z","shell.execute_reply.started":"2025-06-15T09:26:33.117561Z","shell.execute_reply":"2025-06-15T09:43:44.690103Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar10\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=10\nnum_meta=10\nimb_factor=0.02\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar10_IF50_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar10_imb0.02_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/873]\\tLoss 0.0892 (0.0892)\\tPrec@1 100.000 (100.000)\nEpoch: [160][100/873]\\tLoss 0.8891 (5.1745)\\tPrec@1 100.000 (88.243)\nEpoch: [160][200/873]\\tLoss 0.3303 (4.5890)\\tPrec@1 100.000 (88.464)\nEpoch: [160][300/873]\\tLoss 0.7260 (4.8903)\\tPrec@1 93.750 (88.434)\nEpoch: [160][400/873]\\tLoss 2.4066 (4.9543)\\tPrec@1 87.500 (88.903)\nEpoch: [160][500/873]\\tLoss 0.7500 (4.7674)\\tPrec@1 81.250 (88.760)\nEpoch: [160][600/873]\\tLoss 4.4312 (4.7421)\\tPrec@1 75.000 (88.769)\nEpoch: [160][700/873]\\tLoss 0.9357 (4.6288)\\tPrec@1 87.500 (88.713)\nEpoch: [160][800/873]\\tLoss 4.8240 (4.5503)\\tPrec@1 81.250 (88.655)\nEpoch: [160][872/873]\\tLoss 1.0849 (4.4501)\\tPrec@1 93.333 (88.838)\n * Prec@1 79.820\nEpoch: [161][0/873]\\tLoss 0.1235 (0.1235)\\tPrec@1 100.000 (100.000)\nEpoch: [161][100/873]\\tLoss 12.2715 (4.3976)\\tPrec@1 87.500 (89.604)\nEpoch: [161][200/873]\\tLoss 0.4667 (4.5203)\\tPrec@1 93.750 (89.055)\nEpoch: [161][300/873]\\tLoss 0.3415 (4.1412)\\tPrec@1 100.000 (89.120)\nEpoch: [161][400/873]\\tLoss 1.7179 (4.0737)\\tPrec@1 87.500 (88.934)\nEpoch: [161][500/873]\\tLoss 1.9856 (3.9203)\\tPrec@1 81.250 (88.910)\nEpoch: [161][600/873]\\tLoss 8.3992 (3.8177)\\tPrec@1 81.250 (88.935)\nEpoch: [161][700/873]\\tLoss 7.1303 (3.7029)\\tPrec@1 87.500 (88.864)\nEpoch: [161][800/873]\\tLoss 1.7011 (3.6787)\\tPrec@1 87.500 (88.717)\nEpoch: [161][872/873]\\tLoss 15.7621 (3.6684)\\tPrec@1 80.000 (88.802)\n * Prec@1 81.360\nEpoch: [162][0/873]\\tLoss 4.4592 (4.4592)\\tPrec@1 93.750 (93.750)\nEpoch: [162][100/873]\\tLoss 3.0731 (2.9998)\\tPrec@1 81.250 (87.871)\nEpoch: [162][200/873]\\tLoss 7.3921 (2.9821)\\tPrec@1 81.250 (88.961)\nEpoch: [162][300/873]\\tLoss 2.5444 (3.2598)\\tPrec@1 93.750 (88.600)\nEpoch: [162][400/873]\\tLoss 0.0738 (3.3039)\\tPrec@1 100.000 (88.575)\nEpoch: [162][500/873]\\tLoss 3.1236 (3.3150)\\tPrec@1 93.750 (88.673)\nEpoch: [162][600/873]\\tLoss 2.8032 (3.1924)\\tPrec@1 93.750 (88.602)\nEpoch: [162][700/873]\\tLoss 4.2774 (3.1594)\\tPrec@1 75.000 (88.632)\nEpoch: [162][800/873]\\tLoss 1.1757 (3.1557)\\tPrec@1 87.500 (88.553)\nEpoch: [162][872/873]\\tLoss 3.4665 (3.1661)\\tPrec@1 80.000 (88.573)\n * Prec@1 81.700\nEpoch: [163][0/873]\\tLoss 1.9261 (1.9261)\\tPrec@1 81.250 (81.250)\nEpoch: [163][100/873]\\tLoss 0.0708 (3.0755)\\tPrec@1 100.000 (87.809)\nEpoch: [163][200/873]\\tLoss 0.4902 (3.2660)\\tPrec@1 100.000 (88.091)\nEpoch: [163][300/873]\\tLoss 1.0040 (3.3213)\\tPrec@1 81.250 (88.289)\nEpoch: [163][400/873]\\tLoss 2.6968 (3.3022)\\tPrec@1 87.500 (87.858)\nEpoch: [163][500/873]\\tLoss 8.2162 (3.2696)\\tPrec@1 87.500 (87.787)\nEpoch: [163][600/873]\\tLoss 3.1952 (3.2559)\\tPrec@1 93.750 (87.833)\nEpoch: [163][700/873]\\tLoss 0.6914 (3.2194)\\tPrec@1 87.500 (87.785)\nEpoch: [163][800/873]\\tLoss 1.7766 (3.1142)\\tPrec@1 87.500 (87.797)\nEpoch: [163][872/873]\\tLoss 3.7366 (3.1556)\\tPrec@1 80.000 (87.721)\n * Prec@1 81.860\nEpoch: [164][0/873]\\tLoss 6.0910 (6.0910)\\tPrec@1 81.250 (81.250)\nEpoch: [164][100/873]\\tLoss 0.3073 (3.1730)\\tPrec@1 93.750 (87.376)\nEpoch: [164][200/873]\\tLoss 4.8977 (3.5324)\\tPrec@1 93.750 (86.598)\nEpoch: [164][300/873]\\tLoss 3.0591 (3.1806)\\tPrec@1 93.750 (87.292)\nEpoch: [164][400/873]\\tLoss 8.6348 (3.1714)\\tPrec@1 81.250 (87.282)\nEpoch: [164][500/873]\\tLoss 0.2454 (3.0694)\\tPrec@1 93.750 (87.051)\nEpoch: [164][600/873]\\tLoss 1.0767 (3.0294)\\tPrec@1 93.750 (86.949)\nEpoch: [164][700/873]\\tLoss 0.5922 (3.0424)\\tPrec@1 81.250 (86.885)\nEpoch: [164][800/873]\\tLoss 1.9182 (3.0675)\\tPrec@1 62.500 (86.595)\nEpoch: [164][872/873]\\tLoss 4.1044 (3.1012)\\tPrec@1 73.333 (86.490)\n * Prec@1 82.630\nEpoch: [165][0/873]\\tLoss 0.7197 (0.7197)\\tPrec@1 87.500 (87.500)\nEpoch: [165][100/873]\\tLoss 1.5220 (3.1729)\\tPrec@1 68.750 (87.005)\nEpoch: [165][200/873]\\tLoss 5.0926 (3.2890)\\tPrec@1 75.000 (85.697)\nEpoch: [165][300/873]\\tLoss 0.7117 (3.2569)\\tPrec@1 93.750 (85.527)\nEpoch: [165][400/873]\\tLoss 0.2500 (3.2521)\\tPrec@1 100.000 (85.411)\nEpoch: [165][500/873]\\tLoss 0.7886 (3.1128)\\tPrec@1 100.000 (85.541)\nEpoch: [165][600/873]\\tLoss 0.6326 (3.1199)\\tPrec@1 100.000 (85.639)\nEpoch: [165][700/873]\\tLoss 1.9465 (3.1754)\\tPrec@1 81.250 (85.556)\nEpoch: [165][800/873]\\tLoss 2.3054 (3.1501)\\tPrec@1 87.500 (85.698)\nEpoch: [165][872/873]\\tLoss 0.9029 (3.1497)\\tPrec@1 93.333 (85.623)\n * Prec@1 82.370\nEpoch: [166][0/873]\\tLoss 1.3859 (1.3859)\\tPrec@1 81.250 (81.250)\nEpoch: [166][100/873]\\tLoss 2.6125 (2.1981)\\tPrec@1 81.250 (86.386)\nEpoch: [166][200/873]\\tLoss 1.1214 (2.1880)\\tPrec@1 81.250 (87.096)\nEpoch: [166][300/873]\\tLoss 2.5907 (2.4905)\\tPrec@1 87.500 (86.836)\nEpoch: [166][400/873]\\tLoss 0.7967 (2.5891)\\tPrec@1 87.500 (86.612)\nEpoch: [166][500/873]\\tLoss 0.5402 (2.7066)\\tPrec@1 93.750 (86.302)\nEpoch: [166][600/873]\\tLoss 3.8769 (2.7022)\\tPrec@1 75.000 (86.013)\nEpoch: [166][700/873]\\tLoss 2.6707 (2.7394)\\tPrec@1 81.250 (85.851)\nEpoch: [166][800/873]\\tLoss 1.7231 (2.7279)\\tPrec@1 87.500 (85.885)\nEpoch: [166][872/873]\\tLoss 3.6130 (2.7514)\\tPrec@1 86.667 (85.802)\n * Prec@1 82.560\nEpoch: [167][0/873]\\tLoss 5.7538 (5.7538)\\tPrec@1 68.750 (68.750)\nEpoch: [167][100/873]\\tLoss 3.4851 (2.6890)\\tPrec@1 87.500 (85.210)\nEpoch: [167][200/873]\\tLoss 1.5408 (2.7423)\\tPrec@1 75.000 (84.391)\nEpoch: [167][300/873]\\tLoss 12.1102 (2.8980)\\tPrec@1 68.750 (84.863)\nEpoch: [167][400/873]\\tLoss 2.4829 (2.8489)\\tPrec@1 87.500 (84.866)\nEpoch: [167][500/873]\\tLoss 1.0283 (2.8917)\\tPrec@1 87.500 (84.918)\nEpoch: [167][600/873]\\tLoss 0.6879 (2.9195)\\tPrec@1 81.250 (84.703)\nEpoch: [167][700/873]\\tLoss 3.6505 (2.9209)\\tPrec@1 87.500 (84.460)\nEpoch: [167][800/873]\\tLoss 5.3328 (2.8764)\\tPrec@1 81.250 (84.371)\nEpoch: [167][872/873]\\tLoss 0.6635 (2.9013)\\tPrec@1 86.667 (84.220)\n * Prec@1 82.670\nEpoch: [168][0/873]\\tLoss 2.3047 (2.3047)\\tPrec@1 93.750 (93.750)\nEpoch: [168][100/873]\\tLoss 1.3539 (2.7098)\\tPrec@1 81.250 (85.149)\nEpoch: [168][200/873]\\tLoss 2.3904 (2.7260)\\tPrec@1 62.500 (85.168)\nEpoch: [168][300/873]\\tLoss 4.2568 (2.8368)\\tPrec@1 75.000 (84.842)\nEpoch: [168][400/873]\\tLoss 0.3529 (2.8696)\\tPrec@1 100.000 (84.913)\nEpoch: [168][500/873]\\tLoss 4.6570 (2.8667)\\tPrec@1 87.500 (84.631)\nEpoch: [168][600/873]\\tLoss 1.4021 (2.8607)\\tPrec@1 93.750 (84.526)\nEpoch: [168][700/873]\\tLoss 1.2227 (2.7860)\\tPrec@1 81.250 (84.754)\nEpoch: [168][800/873]\\tLoss 0.8585 (2.7852)\\tPrec@1 87.500 (84.699)\nEpoch: [168][872/873]\\tLoss 1.2083 (2.7629)\\tPrec@1 93.333 (84.700)\n * Prec@1 82.180\nEpoch: [169][0/873]\\tLoss 11.6690 (11.6690)\\tPrec@1 81.250 (81.250)\nEpoch: [169][100/873]\\tLoss 3.9189 (2.5642)\\tPrec@1 68.750 (84.097)\nEpoch: [169][200/873]\\tLoss 18.1053 (2.8914)\\tPrec@1 87.500 (84.515)\nEpoch: [169][300/873]\\tLoss 2.4794 (2.9377)\\tPrec@1 81.250 (84.385)\nEpoch: [169][400/873]\\tLoss 1.1071 (2.9136)\\tPrec@1 81.250 (84.180)\nEpoch: [169][500/873]\\tLoss 12.9434 (2.8813)\\tPrec@1 81.250 (83.920)\nEpoch: [169][600/873]\\tLoss 0.5168 (2.7863)\\tPrec@1 93.750 (84.235)\nEpoch: [169][700/873]\\tLoss 2.5495 (2.7985)\\tPrec@1 75.000 (84.326)\nEpoch: [169][800/873]\\tLoss 1.9120 (2.7777)\\tPrec@1 75.000 (84.231)\nEpoch: [169][872/873]\\tLoss 0.5193 (2.7686)\\tPrec@1 93.333 (84.277)\n * Prec@1 82.420\nEpoch: [170][0/873]\\tLoss 0.2544 (0.2544)\\tPrec@1 100.000 (100.000)\nEpoch: [170][100/873]\\tLoss 3.4173 (2.7378)\\tPrec@1 81.250 (84.530)\nEpoch: [170][200/873]\\tLoss 1.5166 (2.9163)\\tPrec@1 87.500 (85.199)\nEpoch: [170][300/873]\\tLoss 1.5354 (2.8431)\\tPrec@1 87.500 (85.341)\nEpoch: [170][400/873]\\tLoss 2.4262 (2.8137)\\tPrec@1 81.250 (85.240)\nEpoch: [170][500/873]\\tLoss 1.4140 (2.8668)\\tPrec@1 93.750 (85.005)\nEpoch: [170][600/873]\\tLoss 0.7673 (2.7992)\\tPrec@1 93.750 (84.765)\nEpoch: [170][700/873]\\tLoss 1.3801 (2.7818)\\tPrec@1 81.250 (84.807)\nEpoch: [170][800/873]\\tLoss 1.3389 (2.7762)\\tPrec@1 75.000 (84.753)\nEpoch: [170][872/873]\\tLoss 4.1468 (2.7812)\\tPrec@1 80.000 (84.650)\n * Prec@1 82.550\nEpoch: [171][0/873]\\tLoss 0.7377 (0.7377)\\tPrec@1 100.000 (100.000)\nEpoch: [171][100/873]\\tLoss 1.3178 (1.9873)\\tPrec@1 87.500 (86.262)\nEpoch: [171][200/873]\\tLoss 0.2899 (2.4860)\\tPrec@1 100.000 (84.888)\nEpoch: [171][300/873]\\tLoss 7.8463 (2.5988)\\tPrec@1 75.000 (84.946)\nEpoch: [171][400/873]\\tLoss 3.4753 (2.7518)\\tPrec@1 81.250 (84.539)\nEpoch: [171][500/873]\\tLoss 1.2284 (2.7974)\\tPrec@1 93.750 (84.394)\nEpoch: [171][600/873]\\tLoss 8.3860 (2.8434)\\tPrec@1 81.250 (84.339)\nEpoch: [171][700/873]\\tLoss 0.7156 (2.8891)\\tPrec@1 93.750 (84.281)\nEpoch: [171][800/873]\\tLoss 0.3334 (2.8983)\\tPrec@1 93.750 (84.106)\nEpoch: [171][872/873]\\tLoss 0.4095 (2.9029)\\tPrec@1 93.333 (84.127)\n * Prec@1 82.390\nEpoch: [172][0/873]\\tLoss 8.0850 (8.0850)\\tPrec@1 75.000 (75.000)\nEpoch: [172][100/873]\\tLoss 1.0408 (3.0194)\\tPrec@1 100.000 (83.601)\nEpoch: [172][200/873]\\tLoss 2.9722 (2.8397)\\tPrec@1 81.250 (84.328)\nEpoch: [172][300/873]\\tLoss 7.3583 (2.7139)\\tPrec@1 87.500 (84.219)\nEpoch: [172][400/873]\\tLoss 1.3364 (2.7245)\\tPrec@1 87.500 (84.523)\nEpoch: [172][500/873]\\tLoss 3.3588 (2.8377)\\tPrec@1 87.500 (84.481)\nEpoch: [172][600/873]\\tLoss 0.8541 (2.8707)\\tPrec@1 87.500 (84.266)\nEpoch: [172][700/873]\\tLoss 2.3013 (2.8671)\\tPrec@1 87.500 (84.379)\nEpoch: [172][800/873]\\tLoss 1.5753 (2.7648)\\tPrec@1 87.500 (84.558)\nEpoch: [172][872/873]\\tLoss 3.1163 (2.7809)\\tPrec@1 73.333 (84.471)\n * Prec@1 82.390\nEpoch: [173][0/873]\\tLoss 1.4018 (1.4018)\\tPrec@1 81.250 (81.250)\nEpoch: [173][100/873]\\tLoss 1.7260 (2.1241)\\tPrec@1 81.250 (85.149)\nEpoch: [173][200/873]\\tLoss 0.4082 (2.5225)\\tPrec@1 100.000 (85.448)\nEpoch: [173][300/873]\\tLoss 10.9717 (2.7122)\\tPrec@1 81.250 (84.821)\nEpoch: [173][400/873]\\tLoss 3.2868 (2.7344)\\tPrec@1 87.500 (84.383)\nEpoch: [173][500/873]\\tLoss 1.1085 (2.7519)\\tPrec@1 81.250 (84.331)\nEpoch: [173][600/873]\\tLoss 0.3482 (2.7622)\\tPrec@1 93.750 (84.110)\nEpoch: [173][700/873]\\tLoss 0.4062 (2.7470)\\tPrec@1 100.000 (84.183)\nEpoch: [173][800/873]\\tLoss 3.1628 (2.7492)\\tPrec@1 75.000 (84.043)\nEpoch: [173][872/873]\\tLoss 0.6414 (2.7465)\\tPrec@1 93.333 (84.077)\n * Prec@1 82.410\nEpoch: [174][0/873]\\tLoss 0.4488 (0.4488)\\tPrec@1 93.750 (93.750)\nEpoch: [174][100/873]\\tLoss 1.5318 (3.1082)\\tPrec@1 81.250 (84.468)\nEpoch: [174][200/873]\\tLoss 0.5313 (2.8334)\\tPrec@1 93.750 (83.986)\nEpoch: [174][300/873]\\tLoss 8.3822 (2.8691)\\tPrec@1 75.000 (84.074)\nEpoch: [174][400/873]\\tLoss 1.9191 (2.8190)\\tPrec@1 75.000 (83.666)\nEpoch: [174][500/873]\\tLoss 1.9165 (2.7900)\\tPrec@1 81.250 (84.182)\nEpoch: [174][600/873]\\tLoss 3.9883 (2.7810)\\tPrec@1 68.750 (83.891)\nEpoch: [174][700/873]\\tLoss 0.9259 (2.7936)\\tPrec@1 87.500 (84.130)\nEpoch: [174][800/873]\\tLoss 0.8460 (2.7735)\\tPrec@1 87.500 (84.324)\nEpoch: [174][872/873]\\tLoss 1.7094 (2.7353)\\tPrec@1 80.000 (84.342)\n * Prec@1 82.710\nEpoch: [175][0/873]\\tLoss 4.2625 (4.2625)\\tPrec@1 81.250 (81.250)\nEpoch: [175][100/873]\\tLoss 1.0409 (2.7458)\\tPrec@1 93.750 (83.292)\nEpoch: [175][200/873]\\tLoss 4.1966 (2.8698)\\tPrec@1 68.750 (84.017)\nEpoch: [175][300/873]\\tLoss 0.6702 (2.8011)\\tPrec@1 93.750 (83.887)\nEpoch: [175][400/873]\\tLoss 1.3000 (2.7770)\\tPrec@1 87.500 (83.697)\nEpoch: [175][500/873]\\tLoss 2.8074 (2.7354)\\tPrec@1 75.000 (83.595)\nEpoch: [175][600/873]\\tLoss 2.8101 (2.6935)\\tPrec@1 75.000 (83.798)\nEpoch: [175][700/873]\\tLoss 1.8824 (2.6817)\\tPrec@1 68.750 (83.782)\nEpoch: [175][800/873]\\tLoss 0.6579 (2.7009)\\tPrec@1 93.750 (83.801)\nEpoch: [175][872/873]\\tLoss 2.5375 (2.7037)\\tPrec@1 86.667 (83.812)\n * Prec@1 82.840\nEpoch: [176][0/873]\\tLoss 2.4811 (2.4811)\\tPrec@1 81.250 (81.250)\nEpoch: [176][100/873]\\tLoss 0.3853 (2.8334)\\tPrec@1 93.750 (84.344)\nEpoch: [176][200/873]\\tLoss 0.9510 (2.6312)\\tPrec@1 93.750 (84.639)\nEpoch: [176][300/873]\\tLoss 1.7346 (2.6738)\\tPrec@1 81.250 (84.697)\nEpoch: [176][400/873]\\tLoss 0.2250 (2.6550)\\tPrec@1 100.000 (84.601)\nEpoch: [176][500/873]\\tLoss 2.5203 (2.6805)\\tPrec@1 81.250 (84.731)\nEpoch: [176][600/873]\\tLoss 5.7179 (2.6996)\\tPrec@1 56.250 (84.713)\nEpoch: [176][700/873]\\tLoss 1.8505 (2.6789)\\tPrec@1 93.750 (84.647)\nEpoch: [176][800/873]\\tLoss 1.7986 (2.7602)\\tPrec@1 75.000 (84.418)\nEpoch: [176][872/873]\\tLoss 1.1409 (2.7480)\\tPrec@1 86.667 (84.349)\n * Prec@1 82.590\nEpoch: [177][0/873]\\tLoss 0.3650 (0.3650)\\tPrec@1 87.500 (87.500)\nEpoch: [177][100/873]\\tLoss 2.1180 (3.2008)\\tPrec@1 81.250 (83.911)\nEpoch: [177][200/873]\\tLoss 1.9393 (2.9550)\\tPrec@1 93.750 (84.608)\nEpoch: [177][300/873]\\tLoss 2.8759 (2.8447)\\tPrec@1 81.250 (84.406)\nEpoch: [177][400/873]\\tLoss 1.8465 (2.8583)\\tPrec@1 75.000 (84.009)\nEpoch: [177][500/873]\\tLoss 3.8220 (2.7959)\\tPrec@1 81.250 (84.094)\nEpoch: [177][600/873]\\tLoss 2.4274 (2.7587)\\tPrec@1 93.750 (83.964)\nEpoch: [177][700/873]\\tLoss 3.6528 (2.7768)\\tPrec@1 75.000 (83.764)\nEpoch: [177][800/873]\\tLoss 2.1500 (2.7506)\\tPrec@1 75.000 (83.817)\nEpoch: [177][872/873]\\tLoss 3.4295 (2.7491)\\tPrec@1 60.000 (83.883)\n * Prec@1 82.850\nEpoch: [178][0/873]\\tLoss 11.7058 (11.7058)\\tPrec@1 75.000 (75.000)\nEpoch: [178][100/873]\\tLoss 3.4726 (2.7968)\\tPrec@1 87.500 (84.777)\nEpoch: [178][200/873]\\tLoss 1.0166 (2.8596)\\tPrec@1 87.500 (84.422)\nEpoch: [178][300/873]\\tLoss 2.6216 (2.8893)\\tPrec@1 81.250 (84.157)\nEpoch: [178][400/873]\\tLoss 1.0151 (2.8492)\\tPrec@1 81.250 (84.087)\nEpoch: [178][500/873]\\tLoss 0.8240 (2.7383)\\tPrec@1 87.500 (84.531)\nEpoch: [178][600/873]\\tLoss 3.6932 (2.7655)\\tPrec@1 93.750 (84.411)\nEpoch: [178][700/873]\\tLoss 0.9065 (2.8287)\\tPrec@1 81.250 (84.326)\nEpoch: [178][800/873]\\tLoss 3.9834 (2.8349)\\tPrec@1 75.000 (84.231)\nEpoch: [178][872/873]\\tLoss 9.9509 (2.7908)\\tPrec@1 86.667 (84.292)\n * Prec@1 82.490\nEpoch: [179][0/873]\\tLoss 1.2241 (1.2241)\\tPrec@1 81.250 (81.250)\nEpoch: [179][100/873]\\tLoss 2.2289 (2.9610)\\tPrec@1 81.250 (83.973)\nEpoch: [179][200/873]\\tLoss 7.2562 (2.8308)\\tPrec@1 75.000 (84.297)\nEpoch: [179][300/873]\\tLoss 0.9662 (3.0350)\\tPrec@1 93.750 (83.949)\nEpoch: [179][400/873]\\tLoss 0.7441 (3.0563)\\tPrec@1 93.750 (83.416)\nEpoch: [179][500/873]\\tLoss 2.2066 (2.9205)\\tPrec@1 87.500 (83.683)\nEpoch: [179][600/873]\\tLoss 0.1845 (2.8928)\\tPrec@1 100.000 (83.819)\nEpoch: [179][700/873]\\tLoss 2.4639 (2.8681)\\tPrec@1 81.250 (83.738)\nEpoch: [179][800/873]\\tLoss 2.8551 (2.8311)\\tPrec@1 81.250 (83.794)\nEpoch: [179][872/873]\\tLoss 2.5544 (2.8066)\\tPrec@1 73.333 (83.848)\n * Prec@1 82.270\nEpoch: [180][0/873]\\tLoss 0.8902 (0.8902)\\tPrec@1 87.500 (87.500)\nEpoch: [180][100/873]\\tLoss 0.9286 (2.4610)\\tPrec@1 93.750 (85.149)\nEpoch: [180][200/873]\\tLoss 2.6086 (2.5203)\\tPrec@1 93.750 (85.230)\nEpoch: [180][300/873]\\tLoss 1.5678 (2.4801)\\tPrec@1 81.250 (85.112)\nEpoch: [180][400/873]\\tLoss 0.5106 (2.4919)\\tPrec@1 93.750 (84.601)\nEpoch: [180][500/873]\\tLoss 1.8129 (2.5557)\\tPrec@1 81.250 (84.319)\nEpoch: [180][600/873]\\tLoss 6.3868 (2.6290)\\tPrec@1 68.750 (83.985)\nEpoch: [180][700/873]\\tLoss 0.5807 (2.6174)\\tPrec@1 93.750 (84.076)\nEpoch: [180][800/873]\\tLoss 0.9094 (2.6462)\\tPrec@1 87.500 (83.895)\nEpoch: [180][872/873]\\tLoss 6.0098 (2.6720)\\tPrec@1 60.000 (83.891)\n * Prec@1 82.770\nEpoch: [181][0/873]\\tLoss 1.2786 (1.2786)\\tPrec@1 93.750 (93.750)\nEpoch: [181][100/873]\\tLoss 1.9978 (2.6784)\\tPrec@1 81.250 (83.911)\nEpoch: [181][200/873]\\tLoss 0.6839 (2.6472)\\tPrec@1 87.500 (83.862)\nEpoch: [181][300/873]\\tLoss 4.5952 (2.5844)\\tPrec@1 81.250 (84.219)\nEpoch: [181][400/873]\\tLoss 5.0460 (2.6303)\\tPrec@1 81.250 (84.071)\nEpoch: [181][500/873]\\tLoss 1.3518 (2.5855)\\tPrec@1 93.750 (84.244)\nEpoch: [181][600/873]\\tLoss 5.8458 (2.6088)\\tPrec@1 93.750 (84.349)\nEpoch: [181][700/873]\\tLoss 1.2488 (2.6364)\\tPrec@1 81.250 (84.344)\nEpoch: [181][800/873]\\tLoss 1.2286 (2.5962)\\tPrec@1 87.500 (84.434)\nEpoch: [181][872/873]\\tLoss 15.1193 (2.6494)\\tPrec@1 80.000 (84.392)\n * Prec@1 82.710\nEpoch: [182][0/873]\\tLoss 3.3670 (3.3670)\\tPrec@1 68.750 (68.750)\nEpoch: [182][100/873]\\tLoss 3.1467 (3.2800)\\tPrec@1 75.000 (81.188)\nEpoch: [182][200/873]\\tLoss 0.8630 (2.9015)\\tPrec@1 87.500 (83.053)\nEpoch: [182][300/873]\\tLoss 2.9737 (2.9145)\\tPrec@1 75.000 (83.576)\nEpoch: [182][400/873]\\tLoss 0.3972 (2.8727)\\tPrec@1 93.750 (84.414)\nEpoch: [182][500/873]\\tLoss 0.8551 (2.6961)\\tPrec@1 93.750 (84.755)\nEpoch: [182][600/873]\\tLoss 1.2982 (2.6463)\\tPrec@1 75.000 (84.609)\nEpoch: [182][700/873]\\tLoss 0.8659 (2.6437)\\tPrec@1 87.500 (84.362)\nEpoch: [182][800/873]\\tLoss 1.8668 (2.6657)\\tPrec@1 75.000 (84.231)\nEpoch: [182][872/873]\\tLoss 1.5604 (2.6443)\\tPrec@1 80.000 (84.213)\n * Prec@1 82.610\nEpoch: [183][0/873]\\tLoss 1.9779 (1.9779)\\tPrec@1 75.000 (75.000)\nEpoch: [183][100/873]\\tLoss 2.8456 (3.2099)\\tPrec@1 81.250 (83.540)\nEpoch: [183][200/873]\\tLoss 0.4880 (2.8059)\\tPrec@1 93.750 (85.168)\nEpoch: [183][300/873]\\tLoss 1.1608 (2.7333)\\tPrec@1 87.500 (84.635)\nEpoch: [183][400/873]\\tLoss 3.0859 (2.7220)\\tPrec@1 87.500 (84.788)\nEpoch: [183][500/873]\\tLoss 1.1020 (2.7177)\\tPrec@1 75.000 (84.743)\nEpoch: [183][600/873]\\tLoss 0.4641 (2.6802)\\tPrec@1 100.000 (84.765)\nEpoch: [183][700/873]\\tLoss 2.4561 (2.6491)\\tPrec@1 81.250 (84.727)\nEpoch: [183][800/873]\\tLoss 1.4861 (2.6679)\\tPrec@1 81.250 (84.527)\nEpoch: [183][872/873]\\tLoss 1.1991 (2.6653)\\tPrec@1 80.000 (84.542)\n * Prec@1 82.590\nEpoch: [184][0/873]\\tLoss 0.5944 (0.5944)\\tPrec@1 93.750 (93.750)\nEpoch: [184][100/873]\\tLoss 2.5651 (2.9936)\\tPrec@1 81.250 (83.663)\nEpoch: [184][200/873]\\tLoss 0.7269 (2.8644)\\tPrec@1 93.750 (84.173)\nEpoch: [184][300/873]\\tLoss 4.0756 (2.6473)\\tPrec@1 81.250 (84.884)\nEpoch: [184][400/873]\\tLoss 1.1268 (2.5635)\\tPrec@1 87.500 (84.663)\nEpoch: [184][500/873]\\tLoss 2.1165 (2.7035)\\tPrec@1 87.500 (84.606)\nEpoch: [184][600/873]\\tLoss 1.8627 (2.7076)\\tPrec@1 81.250 (84.318)\nEpoch: [184][700/873]\\tLoss 10.4349 (2.7149)\\tPrec@1 81.250 (84.371)\nEpoch: [184][800/873]\\tLoss 0.6265 (2.6828)\\tPrec@1 87.500 (84.395)\nEpoch: [184][872/873]\\tLoss 0.4991 (2.6767)\\tPrec@1 93.333 (84.463)\n * Prec@1 82.220\nEpoch: [185][0/873]\\tLoss 7.1422 (7.1422)\\tPrec@1 81.250 (81.250)\nEpoch: [185][100/873]\\tLoss 1.6658 (2.7288)\\tPrec@1 87.500 (83.911)\nEpoch: [185][200/873]\\tLoss 1.4675 (2.7681)\\tPrec@1 68.750 (84.266)\nEpoch: [185][300/873]\\tLoss 0.5692 (2.7893)\\tPrec@1 87.500 (84.219)\nEpoch: [185][400/873]\\tLoss 2.7026 (2.6374)\\tPrec@1 93.750 (84.741)\nEpoch: [185][500/873]\\tLoss 3.5818 (2.6371)\\tPrec@1 93.750 (84.568)\nEpoch: [185][600/873]\\tLoss 6.6175 (2.6433)\\tPrec@1 87.500 (84.359)\nEpoch: [185][700/873]\\tLoss 1.1513 (2.6070)\\tPrec@1 87.500 (84.433)\nEpoch: [185][800/873]\\tLoss 5.2744 (2.6495)\\tPrec@1 68.750 (84.402)\nEpoch: [185][872/873]\\tLoss 0.6358 (2.6011)\\tPrec@1 100.000 (84.549)\n * Prec@1 82.870\nEpoch: [186][0/873]\\tLoss 0.2441 (0.2441)\\tPrec@1 93.750 (93.750)\nEpoch: [186][100/873]\\tLoss 8.5828 (2.8680)\\tPrec@1 75.000 (84.963)\nEpoch: [186][200/873]\\tLoss 2.8770 (2.9202)\\tPrec@1 81.250 (84.950)\nEpoch: [186][300/873]\\tLoss 3.1423 (2.8628)\\tPrec@1 87.500 (84.240)\nEpoch: [186][400/873]\\tLoss 3.5366 (2.8343)\\tPrec@1 87.500 (84.055)\nEpoch: [186][500/873]\\tLoss 0.8770 (2.7694)\\tPrec@1 87.500 (83.995)\nEpoch: [186][600/873]\\tLoss 1.8426 (2.7096)\\tPrec@1 87.500 (84.245)\nEpoch: [186][700/873]\\tLoss 4.3311 (2.7167)\\tPrec@1 81.250 (84.148)\nEpoch: [186][800/873]\\tLoss 0.9675 (2.6463)\\tPrec@1 93.750 (84.199)\nEpoch: [186][872/873]\\tLoss 2.1687 (2.6597)\\tPrec@1 66.667 (84.234)\n * Prec@1 82.630\nEpoch: [187][0/873]\\tLoss 0.7650 (0.7650)\\tPrec@1 93.750 (93.750)\nEpoch: [187][100/873]\\tLoss 1.8461 (2.8292)\\tPrec@1 87.500 (84.468)\nEpoch: [187][200/873]\\tLoss 2.7318 (2.6647)\\tPrec@1 87.500 (84.764)\nEpoch: [187][300/873]\\tLoss 1.7318 (2.6025)\\tPrec@1 87.500 (84.863)\nEpoch: [187][400/873]\\tLoss 0.7308 (2.5445)\\tPrec@1 93.750 (84.897)\nEpoch: [187][500/873]\\tLoss 0.6465 (2.5387)\\tPrec@1 81.250 (84.818)\nEpoch: [187][600/873]\\tLoss 0.3942 (2.6185)\\tPrec@1 100.000 (84.848)\nEpoch: [187][700/873]\\tLoss 2.2499 (2.6107)\\tPrec@1 87.500 (84.665)\nEpoch: [187][800/873]\\tLoss 2.4804 (2.6469)\\tPrec@1 62.500 (84.473)\nEpoch: [187][872/873]\\tLoss 3.5921 (2.6463)\\tPrec@1 80.000 (84.349)\n * Prec@1 82.470\nEpoch: [188][0/873]\\tLoss 6.1750 (6.1750)\\tPrec@1 81.250 (81.250)\nEpoch: [188][100/873]\\tLoss 0.2883 (2.3007)\\tPrec@1 93.750 (84.282)\nEpoch: [188][200/873]\\tLoss 0.5875 (2.5528)\\tPrec@1 93.750 (84.142)\nEpoch: [188][300/873]\\tLoss 1.9017 (2.5107)\\tPrec@1 93.750 (84.593)\nEpoch: [188][400/873]\\tLoss 1.1905 (2.5886)\\tPrec@1 87.500 (84.211)\nEpoch: [188][500/873]\\tLoss 2.6405 (2.5355)\\tPrec@1 68.750 (84.207)\nEpoch: [188][600/873]\\tLoss 0.4043 (2.5375)\\tPrec@1 93.750 (84.318)\nEpoch: [188][700/873]\\tLoss 0.9866 (2.6031)\\tPrec@1 93.750 (84.148)\nEpoch: [188][800/873]\\tLoss 18.5802 (2.7169)\\tPrec@1 81.250 (83.942)\nEpoch: [188][872/873]\\tLoss 1.2705 (2.7299)\\tPrec@1 80.000 (83.762)\n * Prec@1 82.170\nEpoch: [189][0/873]\\tLoss 0.3410 (0.3410)\\tPrec@1 93.750 (93.750)\nEpoch: [189][100/873]\\tLoss 1.2005 (2.5431)\\tPrec@1 87.500 (84.777)\nEpoch: [189][200/873]\\tLoss 1.7742 (2.5961)\\tPrec@1 87.500 (84.080)\nEpoch: [189][300/873]\\tLoss 3.5022 (2.6203)\\tPrec@1 87.500 (84.178)\nEpoch: [189][400/873]\\tLoss 30.6970 (2.6278)\\tPrec@1 81.250 (84.024)\nEpoch: [189][500/873]\\tLoss 0.6141 (2.6518)\\tPrec@1 87.500 (83.845)\nEpoch: [189][600/873]\\tLoss 2.0681 (2.6660)\\tPrec@1 87.500 (83.923)\nEpoch: [189][700/873]\\tLoss 5.1033 (2.7107)\\tPrec@1 75.000 (83.800)\nEpoch: [189][800/873]\\tLoss 0.7393 (2.7202)\\tPrec@1 93.750 (83.645)\nEpoch: [189][872/873]\\tLoss 0.5372 (2.7585)\\tPrec@1 86.667 (83.504)\n * Prec@1 82.940\nEpoch: [190][0/873]\\tLoss 1.0483 (1.0483)\\tPrec@1 93.750 (93.750)\nEpoch: [190][100/873]\\tLoss 1.0653 (2.7668)\\tPrec@1 81.250 (82.488)\nEpoch: [190][200/873]\\tLoss 4.5046 (2.7126)\\tPrec@1 81.250 (83.302)\nEpoch: [190][300/873]\\tLoss 2.0411 (2.6441)\\tPrec@1 75.000 (83.638)\nEpoch: [190][400/873]\\tLoss 1.2045 (2.6994)\\tPrec@1 87.500 (83.479)\nEpoch: [190][500/873]\\tLoss 4.0272 (2.7030)\\tPrec@1 81.250 (83.570)\nEpoch: [190][600/873]\\tLoss 2.9059 (2.7459)\\tPrec@1 81.250 (83.819)\nEpoch: [190][700/873]\\tLoss 3.5304 (2.7285)\\tPrec@1 75.000 (83.987)\nEpoch: [190][800/873]\\tLoss 1.9188 (2.6932)\\tPrec@1 75.000 (84.176)\nEpoch: [190][872/873]\\tLoss 1.1236 (2.6858)\\tPrec@1 86.667 (84.077)\n * Prec@1 82.290\nEpoch: [191][0/873]\\tLoss 4.1232 (4.1232)\\tPrec@1 75.000 (75.000)\nEpoch: [191][100/873]\\tLoss 1.0349 (2.8668)\\tPrec@1 87.500 (82.983)\nEpoch: [191][200/873]\\tLoss 9.6668 (2.5171)\\tPrec@1 75.000 (84.515)\nEpoch: [191][300/873]\\tLoss 0.6059 (2.5910)\\tPrec@1 93.750 (84.012)\nEpoch: [191][400/873]\\tLoss 2.5126 (2.6213)\\tPrec@1 81.250 (83.759)\nEpoch: [191][500/873]\\tLoss 0.8642 (2.5761)\\tPrec@1 87.500 (83.757)\nEpoch: [191][600/873]\\tLoss 6.6760 (2.6135)\\tPrec@1 68.750 (83.434)\nEpoch: [191][700/873]\\tLoss 0.4655 (2.6912)\\tPrec@1 100.000 (83.639)\nEpoch: [191][800/873]\\tLoss 1.0114 (2.6598)\\tPrec@1 81.250 (83.747)\nEpoch: [191][872/873]\\tLoss 0.3323 (2.6408)\\tPrec@1 86.667 (83.855)\n * Prec@1 82.260\nEpoch: [192][0/873]\\tLoss 3.6124 (3.6124)\\tPrec@1 68.750 (68.750)\nEpoch: [192][100/873]\\tLoss 1.0181 (2.6005)\\tPrec@1 81.250 (83.601)\nEpoch: [192][200/873]\\tLoss 13.8362 (2.7298)\\tPrec@1 87.500 (83.800)\nEpoch: [192][300/873]\\tLoss 4.7483 (2.6481)\\tPrec@1 81.250 (84.053)\nEpoch: [192][400/873]\\tLoss 0.4684 (2.6546)\\tPrec@1 87.500 (83.931)\nEpoch: [192][500/873]\\tLoss 2.7406 (2.6709)\\tPrec@1 75.000 (83.782)\nEpoch: [192][600/873]\\tLoss 1.1126 (2.6187)\\tPrec@1 81.250 (84.079)\nEpoch: [192][700/873]\\tLoss 0.6905 (2.6554)\\tPrec@1 93.750 (83.987)\nEpoch: [192][800/873]\\tLoss 0.6047 (2.6726)\\tPrec@1 93.750 (84.090)\nEpoch: [192][872/873]\\tLoss 1.9385 (2.6733)\\tPrec@1 93.333 (83.977)\n * Prec@1 82.560\nEpoch: [193][0/873]\\tLoss 3.3411 (3.3411)\\tPrec@1 75.000 (75.000)\nEpoch: [193][100/873]\\tLoss 0.5339 (2.5307)\\tPrec@1 93.750 (85.334)\nEpoch: [193][200/873]\\tLoss 1.1339 (2.6778)\\tPrec@1 75.000 (84.359)\nEpoch: [193][300/873]\\tLoss 4.1359 (2.7053)\\tPrec@1 75.000 (84.178)\nEpoch: [193][400/873]\\tLoss 1.0164 (2.6189)\\tPrec@1 75.000 (84.398)\nEpoch: [193][500/873]\\tLoss 0.2457 (2.5747)\\tPrec@1 100.000 (84.344)\nEpoch: [193][600/873]\\tLoss 1.8063 (2.5779)\\tPrec@1 87.500 (84.318)\nEpoch: [193][700/873]\\tLoss 3.4970 (2.6195)\\tPrec@1 81.250 (84.005)\nEpoch: [193][800/873]\\tLoss 1.4874 (2.5693)\\tPrec@1 87.500 (84.184)\nEpoch: [193][872/873]\\tLoss 5.4937 (2.5807)\\tPrec@1 86.667 (84.177)\n * Prec@1 81.930\nEpoch: [194][0/873]\\tLoss 2.7803 (2.7803)\\tPrec@1 68.750 (68.750)\nEpoch: [194][100/873]\\tLoss 0.5489 (2.2437)\\tPrec@1 87.500 (85.087)\nEpoch: [194][200/873]\\tLoss 4.1235 (2.4774)\\tPrec@1 81.250 (84.515)\nEpoch: [194][300/873]\\tLoss 3.4945 (2.4892)\\tPrec@1 68.750 (84.489)\nEpoch: [194][400/873]\\tLoss 2.4875 (2.5797)\\tPrec@1 87.500 (84.149)\nEpoch: [194][500/873]\\tLoss 1.4511 (2.6289)\\tPrec@1 87.500 (84.007)\nEpoch: [194][600/873]\\tLoss 1.9675 (2.6356)\\tPrec@1 68.750 (84.224)\nEpoch: [194][700/873]\\tLoss 4.6122 (2.6262)\\tPrec@1 87.500 (84.308)\nEpoch: [194][800/873]\\tLoss 1.9019 (2.6423)\\tPrec@1 93.750 (84.395)\nEpoch: [194][872/873]\\tLoss 1.7094 (2.6282)\\tPrec@1 80.000 (84.485)\n * Prec@1 82.250\nEpoch: [195][0/873]\\tLoss 2.3092 (2.3092)\\tPrec@1 81.250 (81.250)\nEpoch: [195][100/873]\\tLoss 1.2119 (2.2942)\\tPrec@1 93.750 (84.282)\nEpoch: [195][200/873]\\tLoss 2.3413 (2.5102)\\tPrec@1 81.250 (84.577)\nEpoch: [195][300/873]\\tLoss 3.9951 (2.6778)\\tPrec@1 81.250 (84.199)\nEpoch: [195][400/873]\\tLoss 12.0638 (2.6733)\\tPrec@1 81.250 (84.367)\nEpoch: [195][500/873]\\tLoss 1.6722 (2.7755)\\tPrec@1 81.250 (83.857)\nEpoch: [195][600/873]\\tLoss 0.9404 (2.7264)\\tPrec@1 87.500 (84.120)\nEpoch: [195][700/873]\\tLoss 2.1553 (2.6946)\\tPrec@1 87.500 (84.103)\nEpoch: [195][800/873]\\tLoss 5.7530 (2.6237)\\tPrec@1 81.250 (84.309)\nEpoch: [195][872/873]\\tLoss 4.0838 (2.5802)\\tPrec@1 93.333 (84.342)\n * Prec@1 82.610\nEpoch: [196][0/873]\\tLoss 1.9936 (1.9936)\\tPrec@1 75.000 (75.000)\nEpoch: [196][100/873]\\tLoss 2.8722 (2.7036)\\tPrec@1 62.500 (84.282)\nEpoch: [196][200/873]\\tLoss 0.7256 (2.8847)\\tPrec@1 100.000 (84.235)\nEpoch: [196][300/873]\\tLoss 1.8971 (2.7344)\\tPrec@1 68.750 (83.949)\nEpoch: [196][400/873]\\tLoss 8.4523 (2.7381)\\tPrec@1 68.750 (83.931)\nEpoch: [196][500/873]\\tLoss 3.8335 (2.7148)\\tPrec@1 87.500 (84.119)\nEpoch: [196][600/873]\\tLoss 3.7579 (2.7450)\\tPrec@1 75.000 (84.235)\nEpoch: [196][700/873]\\tLoss 3.3700 (2.7017)\\tPrec@1 81.250 (84.165)\nEpoch: [196][800/873]\\tLoss 1.6271 (2.7230)\\tPrec@1 75.000 (84.121)\nEpoch: [196][872/873]\\tLoss 0.4749 (2.7238)\\tPrec@1 100.000 (84.206)\n * Prec@1 82.120\nEpoch: [197][0/873]\\tLoss 1.7949 (1.7949)\\tPrec@1 81.250 (81.250)\nEpoch: [197][100/873]\\tLoss 2.3640 (2.7870)\\tPrec@1 81.250 (83.601)\nEpoch: [197][200/873]\\tLoss 3.5067 (2.6333)\\tPrec@1 68.750 (83.675)\nEpoch: [197][300/873]\\tLoss 1.6836 (2.6415)\\tPrec@1 68.750 (83.596)\nEpoch: [197][400/873]\\tLoss 1.8947 (2.5444)\\tPrec@1 87.500 (83.713)\nEpoch: [197][500/873]\\tLoss 4.3789 (2.5142)\\tPrec@1 87.500 (83.795)\nEpoch: [197][600/873]\\tLoss 2.9385 (2.6062)\\tPrec@1 93.750 (83.839)\nEpoch: [197][700/873]\\tLoss 0.2935 (2.5971)\\tPrec@1 93.750 (83.675)\nEpoch: [197][800/873]\\tLoss 0.6657 (2.5967)\\tPrec@1 87.500 (83.700)\nEpoch: [197][872/873]\\tLoss 2.3571 (2.6023)\\tPrec@1 80.000 (83.776)\n * Prec@1 82.740\nEpoch: [198][0/873]\\tLoss 1.1623 (1.1623)\\tPrec@1 93.750 (93.750)\nEpoch: [198][100/873]\\tLoss 6.8689 (1.9091)\\tPrec@1 68.750 (84.653)\nEpoch: [198][200/873]\\tLoss 2.9443 (2.4966)\\tPrec@1 93.750 (84.919)\nEpoch: [198][300/873]\\tLoss 0.5877 (2.4127)\\tPrec@1 93.750 (84.801)\nEpoch: [198][400/873]\\tLoss 1.1134 (2.5157)\\tPrec@1 93.750 (84.726)\nEpoch: [198][500/873]\\tLoss 3.1273 (2.6315)\\tPrec@1 81.250 (84.306)\nEpoch: [198][600/873]\\tLoss 0.8318 (2.7305)\\tPrec@1 93.750 (84.016)\nEpoch: [198][700/873]\\tLoss 1.4154 (2.6910)\\tPrec@1 81.250 (84.174)\nEpoch: [198][800/873]\\tLoss 7.7678 (2.7664)\\tPrec@1 87.500 (84.059)\nEpoch: [198][872/873]\\tLoss 6.1523 (2.7458)\\tPrec@1 66.667 (84.005)\n * Prec@1 82.620\nEpoch: [199][0/873]\\tLoss 1.2070 (1.2070)\\tPrec@1 93.750 (93.750)\nEpoch: [199][100/873]\\tLoss 2.1086 (2.7045)\\tPrec@1 75.000 (82.240)\nEpoch: [199][200/873]\\tLoss 10.7570 (2.5447)\\tPrec@1 87.500 (83.800)\nEpoch: [199][300/873]\\tLoss 0.8420 (2.5877)\\tPrec@1 87.500 (83.929)\nEpoch: [199][400/873]\\tLoss 1.7234 (2.6575)\\tPrec@1 93.750 (84.118)\nEpoch: [199][500/873]\\tLoss 2.0993 (2.6351)\\tPrec@1 87.500 (84.144)\nEpoch: [199][600/873]\\tLoss 24.9228 (2.5802)\\tPrec@1 75.000 (84.495)\nEpoch: [199][700/873]\\tLoss 3.6534 (2.5658)\\tPrec@1 93.750 (84.558)\nEpoch: [199][800/873]\\tLoss 1.4661 (2.5757)\\tPrec@1 81.250 (84.566)\nEpoch: [199][872/873]\\tLoss 1.5502 (2.6018)\\tPrec@1 80.000 (84.521)\n * Prec@1 82.980\nBest accuracy:  82.98\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# **Imbalance Factor - 20**","metadata":{}},{"cell_type":"code","source":"# --- Run Stage 1 Pre-training (IF=20) ---\n\n!python pretrain_stage1.py \\\n--dataset cifar10 \\\n--imb_factor 0.05 \\\n--epochs 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:43:44.691891Z","iopub.execute_input":"2025-06-15T09:43:44.692182Z","iopub.status.idle":"2025-06-15T10:06:34.222266Z","shell.execute_reply.started":"2025-06-15T09:43:44.692147Z","shell.execute_reply":"2025-06-15T10:06:34.221545Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|████████████████| 133/133 [00:08<00:00, 16.12it/s, Loss=1.849]\nEpoch 2/200: 100%|████████████████| 133/133 [00:06<00:00, 19.13it/s, Loss=1.627]\nEpoch 3/200: 100%|████████████████| 133/133 [00:07<00:00, 18.88it/s, Loss=1.328]\nEpoch 4/200: 100%|████████████████| 133/133 [00:06<00:00, 19.08it/s, Loss=1.600]\nEpoch 5/200: 100%|████████████████| 133/133 [00:06<00:00, 19.26it/s, Loss=1.151]\nEpoch 6/200: 100%|████████████████| 133/133 [00:06<00:00, 19.38it/s, Loss=0.842]\nEpoch 7/200: 100%|████████████████| 133/133 [00:06<00:00, 19.59it/s, Loss=0.927]\nEpoch 8/200: 100%|████████████████| 133/133 [00:06<00:00, 19.80it/s, Loss=0.860]\nEpoch 9/200: 100%|████████████████| 133/133 [00:06<00:00, 19.78it/s, Loss=0.967]\nEpoch 10/200: 100%|███████████████| 133/133 [00:06<00:00, 19.79it/s, Loss=0.807]\nEpoch 11/200: 100%|███████████████| 133/133 [00:06<00:00, 19.80it/s, Loss=0.688]\nEpoch 12/200: 100%|███████████████| 133/133 [00:06<00:00, 19.71it/s, Loss=0.754]\nEpoch 13/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.714]\nEpoch 14/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.692]\nEpoch 15/200: 100%|███████████████| 133/133 [00:06<00:00, 19.34it/s, Loss=0.747]\nEpoch 16/200: 100%|███████████████| 133/133 [00:06<00:00, 19.38it/s, Loss=0.535]\nEpoch 17/200: 100%|███████████████| 133/133 [00:06<00:00, 19.39it/s, Loss=0.864]\nEpoch 18/200: 100%|███████████████| 133/133 [00:06<00:00, 19.42it/s, Loss=0.604]\nEpoch 19/200: 100%|███████████████| 133/133 [00:06<00:00, 19.43it/s, Loss=0.560]\nEpoch 20/200: 100%|███████████████| 133/133 [00:06<00:00, 19.40it/s, Loss=0.845]\nEpoch 21/200: 100%|███████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.407]\nEpoch 22/200: 100%|███████████████| 133/133 [00:06<00:00, 19.67it/s, Loss=0.556]\nEpoch 23/200: 100%|███████████████| 133/133 [00:06<00:00, 19.65it/s, Loss=0.569]\nEpoch 24/200: 100%|███████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.410]\nEpoch 25/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.395]\nEpoch 26/200: 100%|███████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.296]\nEpoch 27/200: 100%|███████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.568]\nEpoch 28/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.396]\nEpoch 29/200: 100%|███████████████| 133/133 [00:06<00:00, 19.48it/s, Loss=0.359]\nEpoch 30/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.459]\nEpoch 31/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.462]\nEpoch 32/200: 100%|███████████████| 133/133 [00:06<00:00, 19.42it/s, Loss=0.531]\nEpoch 33/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.521]\nEpoch 34/200: 100%|███████████████| 133/133 [00:06<00:00, 19.47it/s, Loss=0.352]\nEpoch 35/200: 100%|███████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.438]\nEpoch 36/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.555]\nEpoch 37/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.490]\nEpoch 38/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.466]\nEpoch 39/200: 100%|███████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.357]\nEpoch 40/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.488]\nEpoch 41/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.367]\nEpoch 42/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.598]\nEpoch 43/200: 100%|███████████████| 133/133 [00:06<00:00, 19.46it/s, Loss=0.596]\nEpoch 44/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.372]\nEpoch 45/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.475]\nEpoch 46/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.396]\nEpoch 47/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.368]\nEpoch 48/200: 100%|███████████████| 133/133 [00:06<00:00, 19.46it/s, Loss=0.428]\nEpoch 49/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.379]\nEpoch 50/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.390]\nEpoch 51/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.435]\nEpoch 52/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.511]\nEpoch 53/200: 100%|███████████████| 133/133 [00:06<00:00, 19.33it/s, Loss=0.368]\nEpoch 54/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.452]\nEpoch 55/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.299]\nEpoch 56/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.429]\nEpoch 57/200: 100%|███████████████| 133/133 [00:06<00:00, 19.46it/s, Loss=0.393]\nEpoch 58/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.431]\nEpoch 59/200: 100%|███████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.334]\nEpoch 60/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.313]\nEpoch 61/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.529]\nEpoch 62/200: 100%|███████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.417]\nEpoch 63/200: 100%|███████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.475]\nEpoch 64/200: 100%|███████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.391]\nEpoch 65/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.445]\nEpoch 66/200: 100%|███████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.560]\nEpoch 67/200: 100%|███████████████| 133/133 [00:06<00:00, 19.36it/s, Loss=0.360]\nEpoch 68/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.337]\nEpoch 69/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.471]\nEpoch 70/200: 100%|███████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.447]\nEpoch 71/200: 100%|███████████████| 133/133 [00:06<00:00, 19.47it/s, Loss=0.487]\nEpoch 72/200: 100%|███████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.331]\nEpoch 73/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.334]\nEpoch 74/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.339]\nEpoch 75/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.459]\nEpoch 76/200: 100%|███████████████| 133/133 [00:06<00:00, 19.42it/s, Loss=0.391]\nEpoch 77/200: 100%|███████████████| 133/133 [00:06<00:00, 19.46it/s, Loss=0.490]\nEpoch 78/200: 100%|███████████████| 133/133 [00:06<00:00, 19.48it/s, Loss=0.549]\nEpoch 79/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.380]\nEpoch 80/200: 100%|███████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.358]\nEpoch 81/200: 100%|███████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.233]\nEpoch 82/200: 100%|███████████████| 133/133 [00:06<00:00, 19.47it/s, Loss=0.317]\nEpoch 83/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.302]\nEpoch 84/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.366]\nEpoch 85/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.431]\nEpoch 86/200: 100%|███████████████| 133/133 [00:06<00:00, 19.43it/s, Loss=0.386]\nEpoch 87/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.425]\nEpoch 88/200: 100%|███████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.390]\nEpoch 89/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.524]\nEpoch 90/200: 100%|███████████████| 133/133 [00:06<00:00, 19.44it/s, Loss=0.286]\nEpoch 91/200: 100%|███████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.356]\nEpoch 92/200: 100%|███████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.283]\nEpoch 93/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.328]\nEpoch 94/200: 100%|███████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.247]\nEpoch 95/200: 100%|███████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.339]\nEpoch 96/200: 100%|███████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.342]\nEpoch 97/200: 100%|███████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.430]\nEpoch 98/200: 100%|███████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.287]\nEpoch 99/200: 100%|███████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.297]\nEpoch 100/200: 100%|██████████████| 133/133 [00:06<00:00, 19.35it/s, Loss=0.445]\nEpoch 101/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.454]\nEpoch 102/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.273]\nEpoch 103/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.635]\nEpoch 104/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.317]\nEpoch 105/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.276]\nEpoch 106/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.439]\nEpoch 107/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.600]\nEpoch 108/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.243]\nEpoch 109/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.229]\nEpoch 110/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.318]\nEpoch 111/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.176]\nEpoch 112/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.455]\nEpoch 113/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.386]\nEpoch 114/200: 100%|██████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.433]\nEpoch 115/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.288]\nEpoch 116/200: 100%|██████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.403]\nEpoch 117/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.339]\nEpoch 118/200: 100%|██████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.476]\nEpoch 119/200: 100%|██████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.337]\nEpoch 120/200: 100%|██████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.438]\nEpoch 121/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.339]\nEpoch 122/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.227]\nEpoch 123/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.328]\nEpoch 124/200: 100%|██████████████| 133/133 [00:06<00:00, 19.61it/s, Loss=0.363]\nEpoch 125/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.358]\nEpoch 126/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.401]\nEpoch 127/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.370]\nEpoch 128/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.604]\nEpoch 129/200: 100%|██████████████| 133/133 [00:06<00:00, 19.59it/s, Loss=0.360]\nEpoch 130/200: 100%|██████████████| 133/133 [00:06<00:00, 19.64it/s, Loss=0.315]\nEpoch 131/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.508]\nEpoch 132/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.392]\nEpoch 133/200: 100%|██████████████| 133/133 [00:06<00:00, 19.40it/s, Loss=0.445]\nEpoch 134/200: 100%|██████████████| 133/133 [00:06<00:00, 19.62it/s, Loss=0.210]\nEpoch 135/200: 100%|██████████████| 133/133 [00:06<00:00, 19.68it/s, Loss=0.357]\nEpoch 136/200: 100%|██████████████| 133/133 [00:06<00:00, 19.62it/s, Loss=0.233]\nEpoch 137/200: 100%|██████████████| 133/133 [00:06<00:00, 19.61it/s, Loss=0.515]\nEpoch 138/200: 100%|██████████████| 133/133 [00:06<00:00, 19.64it/s, Loss=0.293]\nEpoch 139/200: 100%|██████████████| 133/133 [00:06<00:00, 19.65it/s, Loss=0.328]\nEpoch 140/200: 100%|██████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.287]\nEpoch 141/200: 100%|██████████████| 133/133 [00:06<00:00, 19.69it/s, Loss=0.247]\nEpoch 142/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.361]\nEpoch 143/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.316]\nEpoch 144/200: 100%|██████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.358]\nEpoch 145/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.370]\nEpoch 146/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.280]\nEpoch 147/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.261]\nEpoch 148/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.195]\nEpoch 149/200: 100%|██████████████| 133/133 [00:06<00:00, 19.64it/s, Loss=0.283]\nEpoch 150/200: 100%|██████████████| 133/133 [00:06<00:00, 19.61it/s, Loss=0.276]\nEpoch 151/200: 100%|██████████████| 133/133 [00:06<00:00, 19.49it/s, Loss=0.305]\nEpoch 152/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.415]\nEpoch 153/200: 100%|██████████████| 133/133 [00:06<00:00, 19.62it/s, Loss=0.240]\nEpoch 154/200: 100%|██████████████| 133/133 [00:06<00:00, 19.61it/s, Loss=0.242]\nEpoch 155/200: 100%|██████████████| 133/133 [00:06<00:00, 19.63it/s, Loss=0.386]\nEpoch 156/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.330]\nEpoch 157/200: 100%|██████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.361]\nEpoch 158/200: 100%|██████████████| 133/133 [00:06<00:00, 19.59it/s, Loss=0.397]\nEpoch 159/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.429]\nEpoch 160/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.234]\nEpoch 161/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.178]\nEpoch 162/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.135]\nEpoch 163/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.197]\nEpoch 164/200: 100%|██████████████| 133/133 [00:06<00:00, 19.61it/s, Loss=0.236]\nEpoch 165/200: 100%|██████████████| 133/133 [00:06<00:00, 19.60it/s, Loss=0.064]\nEpoch 166/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.175]\nEpoch 167/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.062]\nEpoch 168/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.030]\nEpoch 169/200: 100%|██████████████| 133/133 [00:06<00:00, 19.59it/s, Loss=0.058]\nEpoch 170/200: 100%|██████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.066]\nEpoch 171/200: 100%|██████████████| 133/133 [00:06<00:00, 19.47it/s, Loss=0.033]\nEpoch 172/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.096]\nEpoch 173/200: 100%|██████████████| 133/133 [00:06<00:00, 19.59it/s, Loss=0.063]\nEpoch 174/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.047]\nEpoch 175/200: 100%|██████████████| 133/133 [00:06<00:00, 19.48it/s, Loss=0.095]\nEpoch 176/200: 100%|██████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.073]\nEpoch 177/200: 100%|██████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.047]\nEpoch 178/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.027]\nEpoch 179/200: 100%|██████████████| 133/133 [00:06<00:00, 19.58it/s, Loss=0.047]\nEpoch 180/200: 100%|██████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.112]\nEpoch 181/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.048]\nEpoch 182/200: 100%|██████████████| 133/133 [00:06<00:00, 19.56it/s, Loss=0.027]\nEpoch 183/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.021]\nEpoch 184/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.033]\nEpoch 185/200: 100%|██████████████| 133/133 [00:06<00:00, 19.50it/s, Loss=0.025]\nEpoch 186/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.073]\nEpoch 187/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.053]\nEpoch 188/200: 100%|██████████████| 133/133 [00:06<00:00, 19.51it/s, Loss=0.021]\nEpoch 189/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.022]\nEpoch 190/200: 100%|██████████████| 133/133 [00:06<00:00, 19.57it/s, Loss=0.031]\nEpoch 191/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.056]\nEpoch 192/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.051]\nEpoch 193/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.069]\nEpoch 194/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.047]\nEpoch 195/200: 100%|██████████████| 133/133 [00:06<00:00, 19.42it/s, Loss=0.036]\nEpoch 196/200: 100%|██████████████| 133/133 [00:06<00:00, 19.53it/s, Loss=0.007]\nEpoch 197/200: 100%|██████████████| 133/133 [00:06<00:00, 19.55it/s, Loss=0.014]\nEpoch 198/200: 100%|██████████████| 133/133 [00:06<00:00, 19.52it/s, Loss=0.021]\nEpoch 199/200: 100%|██████████████| 133/133 [00:06<00:00, 19.45it/s, Loss=0.051]\nEpoch 200/200: 100%|██████████████| 133/133 [00:06<00:00, 19.54it/s, Loss=0.036]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar10_imb0.05_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# --- Run Stage 2 Re-weighting (IF=20) ---\n\n!python OT_train.py \\\n--dataset cifar10 \\\n--imb_factor 0.05 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar10_IF20_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar10_imb0.05_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T10:06:34.223293Z","iopub.execute_input":"2025-06-15T10:06:34.223551Z","iopub.status.idle":"2025-06-15T10:26:42.276289Z","shell.execute_reply.started":"2025-06-15T10:06:34.223525Z","shell.execute_reply":"2025-06-15T10:26:42.275556Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar10\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=10\nnum_meta=10\nimb_factor=0.05\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar10_IF20_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar10_imb0.05_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/1062]\\tLoss 15.8343 (15.8343)\\tPrec@1 81.250 (81.250)\nEpoch: [160][100/1062]\\tLoss 6.3510 (5.0155)\\tPrec@1 75.000 (89.418)\nEpoch: [160][200/1062]\\tLoss 1.1517 (5.1683)\\tPrec@1 93.750 (89.614)\nEpoch: [160][300/1062]\\tLoss 13.7146 (5.2761)\\tPrec@1 75.000 (89.514)\nEpoch: [160][400/1062]\\tLoss 4.4599 (5.4860)\\tPrec@1 93.750 (89.308)\nEpoch: [160][500/1062]\\tLoss 25.8930 (5.2894)\\tPrec@1 81.250 (89.371)\nEpoch: [160][600/1062]\\tLoss 9.4774 (5.2133)\\tPrec@1 75.000 (89.382)\nEpoch: [160][700/1062]\\tLoss 3.1356 (5.0671)\\tPrec@1 87.500 (89.479)\nEpoch: [160][800/1062]\\tLoss 1.2626 (4.9938)\\tPrec@1 81.250 (89.404)\nEpoch: [160][900/1062]\\tLoss 6.3539 (4.9000)\\tPrec@1 75.000 (89.359)\nEpoch: [160][1000/1062]\\tLoss 19.9853 (4.9681)\\tPrec@1 93.750 (89.336)\nEpoch: [160][1061/1062]\\tLoss 14.7374 (4.9189)\\tPrec@1 69.231 (89.399)\n * Prec@1 86.130\nEpoch: [161][0/1062]\\tLoss 28.5052 (28.5052)\\tPrec@1 68.750 (68.750)\nEpoch: [161][100/1062]\\tLoss 2.2498 (3.9119)\\tPrec@1 87.500 (90.656)\nEpoch: [161][200/1062]\\tLoss 3.8700 (3.9667)\\tPrec@1 87.500 (90.299)\nEpoch: [161][300/1062]\\tLoss 8.2654 (4.1462)\\tPrec@1 87.500 (89.722)\nEpoch: [161][400/1062]\\tLoss 1.6109 (4.2610)\\tPrec@1 93.750 (89.261)\nEpoch: [161][500/1062]\\tLoss 0.5295 (4.2965)\\tPrec@1 93.750 (89.446)\nEpoch: [161][600/1062]\\tLoss 1.6670 (4.2824)\\tPrec@1 93.750 (89.372)\nEpoch: [161][700/1062]\\tLoss 0.5447 (4.2676)\\tPrec@1 100.000 (89.328)\nEpoch: [161][800/1062]\\tLoss 7.6545 (4.2629)\\tPrec@1 87.500 (89.357)\nEpoch: [161][900/1062]\\tLoss 3.7884 (4.3321)\\tPrec@1 75.000 (89.297)\nEpoch: [161][1000/1062]\\tLoss 5.7875 (4.3093)\\tPrec@1 93.750 (89.105)\nEpoch: [161][1061/1062]\\tLoss 2.8860 (4.3036)\\tPrec@1 76.923 (89.028)\n * Prec@1 86.290\nEpoch: [162][0/1062]\\tLoss 3.9942 (3.9942)\\tPrec@1 81.250 (81.250)\nEpoch: [162][100/1062]\\tLoss 2.5676 (3.7456)\\tPrec@1 93.750 (89.109)\nEpoch: [162][200/1062]\\tLoss 3.8086 (4.1565)\\tPrec@1 93.750 (88.215)\nEpoch: [162][300/1062]\\tLoss 2.7666 (4.0456)\\tPrec@1 93.750 (88.476)\nEpoch: [162][400/1062]\\tLoss 1.2922 (4.0588)\\tPrec@1 93.750 (88.373)\nEpoch: [162][500/1062]\\tLoss 2.1834 (3.9331)\\tPrec@1 87.500 (88.510)\nEpoch: [162][600/1062]\\tLoss 0.2324 (4.0588)\\tPrec@1 100.000 (88.342)\nEpoch: [162][700/1062]\\tLoss 0.3987 (4.0020)\\tPrec@1 100.000 (88.436)\nEpoch: [162][800/1062]\\tLoss 3.7063 (4.0509)\\tPrec@1 81.250 (88.327)\nEpoch: [162][900/1062]\\tLoss 12.6990 (3.9709)\\tPrec@1 81.250 (88.430)\nEpoch: [162][1000/1062]\\tLoss 0.3243 (3.9217)\\tPrec@1 100.000 (88.474)\nEpoch: [162][1061/1062]\\tLoss 1.2572 (3.8502)\\tPrec@1 100.000 (88.522)\n * Prec@1 86.680\nEpoch: [163][0/1062]\\tLoss 0.3359 (0.3359)\\tPrec@1 100.000 (100.000)\nEpoch: [163][100/1062]\\tLoss 0.7856 (3.3865)\\tPrec@1 100.000 (88.738)\nEpoch: [163][200/1062]\\tLoss 2.0075 (3.8657)\\tPrec@1 87.500 (88.277)\nEpoch: [163][300/1062]\\tLoss 0.8659 (3.6270)\\tPrec@1 93.750 (88.684)\nEpoch: [163][400/1062]\\tLoss 2.2299 (3.5532)\\tPrec@1 87.500 (88.529)\nEpoch: [163][500/1062]\\tLoss 0.4578 (3.5916)\\tPrec@1 93.750 (88.535)\nEpoch: [163][600/1062]\\tLoss 3.7412 (3.6453)\\tPrec@1 93.750 (88.478)\nEpoch: [163][700/1062]\\tLoss 3.7087 (3.6077)\\tPrec@1 93.750 (88.285)\nEpoch: [163][800/1062]\\tLoss 2.2525 (3.5763)\\tPrec@1 81.250 (88.304)\nEpoch: [163][900/1062]\\tLoss 3.4864 (3.6948)\\tPrec@1 93.750 (88.131)\nEpoch: [163][1000/1062]\\tLoss 3.7161 (3.6977)\\tPrec@1 87.500 (88.118)\nEpoch: [163][1061/1062]\\tLoss 7.3798 (3.7109)\\tPrec@1 76.923 (88.128)\n * Prec@1 86.470\nEpoch: [164][0/1062]\\tLoss 5.7788 (5.7788)\\tPrec@1 87.500 (87.500)\nEpoch: [164][100/1062]\\tLoss 1.5582 (3.8943)\\tPrec@1 81.250 (87.005)\nEpoch: [164][200/1062]\\tLoss 2.2237 (4.0682)\\tPrec@1 87.500 (87.096)\nEpoch: [164][300/1062]\\tLoss 3.6741 (4.0126)\\tPrec@1 87.500 (86.919)\nEpoch: [164][400/1062]\\tLoss 0.5755 (3.9051)\\tPrec@1 93.750 (86.908)\nEpoch: [164][500/1062]\\tLoss 5.1093 (3.8946)\\tPrec@1 81.250 (86.814)\nEpoch: [164][600/1062]\\tLoss 1.2720 (3.7266)\\tPrec@1 93.750 (87.240)\nEpoch: [164][700/1062]\\tLoss 1.3825 (3.8512)\\tPrec@1 87.500 (87.010)\nEpoch: [164][800/1062]\\tLoss 0.3978 (3.7940)\\tPrec@1 93.750 (87.094)\nEpoch: [164][900/1062]\\tLoss 10.0010 (3.7960)\\tPrec@1 81.250 (87.125)\nEpoch: [164][1000/1062]\\tLoss 2.7279 (3.7305)\\tPrec@1 87.500 (87.213)\nEpoch: [164][1061/1062]\\tLoss 10.9974 (3.7401)\\tPrec@1 76.923 (87.203)\n * Prec@1 86.510\nEpoch: [165][0/1062]\\tLoss 2.0131 (2.0131)\\tPrec@1 87.500 (87.500)\nEpoch: [165][100/1062]\\tLoss 7.5623 (3.8023)\\tPrec@1 75.000 (86.324)\nEpoch: [165][200/1062]\\tLoss 2.7411 (3.9775)\\tPrec@1 93.750 (86.754)\nEpoch: [165][300/1062]\\tLoss 7.6378 (3.9881)\\tPrec@1 87.500 (86.752)\nEpoch: [165][400/1062]\\tLoss 1.8263 (3.9292)\\tPrec@1 87.500 (87.001)\nEpoch: [165][500/1062]\\tLoss 4.7050 (3.8286)\\tPrec@1 87.500 (87.201)\nEpoch: [165][600/1062]\\tLoss 0.1669 (3.7692)\\tPrec@1 100.000 (87.115)\nEpoch: [165][700/1062]\\tLoss 9.9510 (3.7348)\\tPrec@1 81.250 (86.983)\nEpoch: [165][800/1062]\\tLoss 4.2660 (3.7174)\\tPrec@1 75.000 (86.837)\nEpoch: [165][900/1062]\\tLoss 1.9811 (3.7248)\\tPrec@1 87.500 (86.834)\nEpoch: [165][1000/1062]\\tLoss 0.2258 (3.7407)\\tPrec@1 100.000 (86.838)\nEpoch: [165][1061/1062]\\tLoss 1.3866 (3.6914)\\tPrec@1 84.615 (86.956)\n * Prec@1 86.660\nEpoch: [166][0/1062]\\tLoss 1.4485 (1.4485)\\tPrec@1 93.750 (93.750)\nEpoch: [166][100/1062]\\tLoss 4.2524 (3.1149)\\tPrec@1 75.000 (87.624)\nEpoch: [166][200/1062]\\tLoss 5.3250 (3.4502)\\tPrec@1 87.500 (87.189)\nEpoch: [166][300/1062]\\tLoss 3.7055 (3.5009)\\tPrec@1 87.500 (87.126)\nEpoch: [166][400/1062]\\tLoss 3.9855 (3.5483)\\tPrec@1 81.250 (86.892)\nEpoch: [166][500/1062]\\tLoss 2.8030 (3.5874)\\tPrec@1 81.250 (86.876)\nEpoch: [166][600/1062]\\tLoss 0.7062 (3.4996)\\tPrec@1 93.750 (86.782)\nEpoch: [166][700/1062]\\tLoss 4.8375 (3.5564)\\tPrec@1 93.750 (86.760)\nEpoch: [166][800/1062]\\tLoss 7.7025 (3.4817)\\tPrec@1 68.750 (86.852)\nEpoch: [166][900/1062]\\tLoss 5.2543 (3.5650)\\tPrec@1 87.500 (86.959)\nEpoch: [166][1000/1062]\\tLoss 0.1021 (3.6370)\\tPrec@1 100.000 (86.851)\nEpoch: [166][1061/1062]\\tLoss 0.4487 (3.6417)\\tPrec@1 92.308 (86.827)\n * Prec@1 86.650\nEpoch: [167][0/1062]\\tLoss 6.6544 (6.6544)\\tPrec@1 87.500 (87.500)\nEpoch: [167][100/1062]\\tLoss 0.5607 (3.9390)\\tPrec@1 100.000 (87.624)\nEpoch: [167][200/1062]\\tLoss 9.0383 (4.1845)\\tPrec@1 81.250 (86.474)\nEpoch: [167][300/1062]\\tLoss 0.4207 (3.8542)\\tPrec@1 100.000 (86.669)\nEpoch: [167][400/1062]\\tLoss 1.9223 (3.7918)\\tPrec@1 87.500 (86.939)\nEpoch: [167][500/1062]\\tLoss 8.7754 (3.7909)\\tPrec@1 75.000 (86.801)\nEpoch: [167][600/1062]\\tLoss 0.4438 (3.7187)\\tPrec@1 93.750 (86.918)\nEpoch: [167][700/1062]\\tLoss 0.3321 (3.6570)\\tPrec@1 100.000 (87.108)\nEpoch: [167][800/1062]\\tLoss 8.2262 (3.6259)\\tPrec@1 75.000 (87.219)\nEpoch: [167][900/1062]\\tLoss 2.5264 (3.6908)\\tPrec@1 93.750 (87.070)\nEpoch: [167][1000/1062]\\tLoss 1.4120 (3.7398)\\tPrec@1 87.500 (86.907)\nEpoch: [167][1061/1062]\\tLoss 2.9285 (3.7032)\\tPrec@1 92.308 (86.886)\n * Prec@1 86.640\nEpoch: [168][0/1062]\\tLoss 4.1528 (4.1528)\\tPrec@1 81.250 (81.250)\nEpoch: [168][100/1062]\\tLoss 2.4816 (2.7678)\\tPrec@1 81.250 (88.552)\nEpoch: [168][200/1062]\\tLoss 3.3018 (3.0339)\\tPrec@1 93.750 (88.557)\nEpoch: [168][300/1062]\\tLoss 2.0350 (3.1551)\\tPrec@1 75.000 (88.248)\nEpoch: [168][400/1062]\\tLoss 4.2972 (3.3223)\\tPrec@1 68.750 (87.438)\nEpoch: [168][500/1062]\\tLoss 0.6813 (3.3802)\\tPrec@1 87.500 (87.425)\nEpoch: [168][600/1062]\\tLoss 4.4292 (3.4564)\\tPrec@1 87.500 (87.219)\nEpoch: [168][700/1062]\\tLoss 12.1299 (3.4369)\\tPrec@1 93.750 (87.126)\nEpoch: [168][800/1062]\\tLoss 1.9888 (3.3993)\\tPrec@1 81.250 (87.219)\nEpoch: [168][900/1062]\\tLoss 2.8620 (3.4681)\\tPrec@1 87.500 (87.181)\nEpoch: [168][1000/1062]\\tLoss 0.4032 (3.5259)\\tPrec@1 93.750 (87.175)\nEpoch: [168][1061/1062]\\tLoss 0.3597 (3.4965)\\tPrec@1 100.000 (87.198)\n * Prec@1 86.660\nEpoch: [169][0/1062]\\tLoss 0.1286 (0.1286)\\tPrec@1 100.000 (100.000)\nEpoch: [169][100/1062]\\tLoss 2.2276 (3.7182)\\tPrec@1 81.250 (87.562)\nEpoch: [169][200/1062]\\tLoss 1.9612 (3.6947)\\tPrec@1 93.750 (87.189)\nEpoch: [169][300/1062]\\tLoss 5.8596 (3.6528)\\tPrec@1 87.500 (87.147)\nEpoch: [169][400/1062]\\tLoss 2.7778 (3.4969)\\tPrec@1 87.500 (87.313)\nEpoch: [169][500/1062]\\tLoss 3.0043 (3.5625)\\tPrec@1 81.250 (86.764)\nEpoch: [169][600/1062]\\tLoss 5.8392 (3.5114)\\tPrec@1 87.500 (86.990)\nEpoch: [169][700/1062]\\tLoss 0.6716 (3.5553)\\tPrec@1 93.750 (86.858)\nEpoch: [169][800/1062]\\tLoss 0.9330 (3.5378)\\tPrec@1 93.750 (86.969)\nEpoch: [169][900/1062]\\tLoss 1.5936 (3.4953)\\tPrec@1 87.500 (86.938)\nEpoch: [169][1000/1062]\\tLoss 1.3240 (3.5129)\\tPrec@1 93.750 (86.926)\nEpoch: [169][1061/1062]\\tLoss 4.0795 (3.5394)\\tPrec@1 92.308 (86.933)\n * Prec@1 86.640\nEpoch: [170][0/1062]\\tLoss 1.3509 (1.3509)\\tPrec@1 93.750 (93.750)\nEpoch: [170][100/1062]\\tLoss 4.5964 (3.4552)\\tPrec@1 81.250 (88.490)\nEpoch: [170][200/1062]\\tLoss 4.9653 (3.5457)\\tPrec@1 87.500 (87.749)\nEpoch: [170][300/1062]\\tLoss 0.6395 (3.6336)\\tPrec@1 93.750 (87.479)\nEpoch: [170][400/1062]\\tLoss 4.0440 (3.7610)\\tPrec@1 87.500 (87.204)\nEpoch: [170][500/1062]\\tLoss 3.1278 (3.8027)\\tPrec@1 87.500 (86.789)\nEpoch: [170][600/1062]\\tLoss 2.0942 (3.7446)\\tPrec@1 75.000 (86.855)\nEpoch: [170][700/1062]\\tLoss 10.3359 (3.7349)\\tPrec@1 62.500 (86.947)\nEpoch: [170][800/1062]\\tLoss 12.4067 (3.7162)\\tPrec@1 68.750 (87.024)\nEpoch: [170][900/1062]\\tLoss 0.8516 (3.6797)\\tPrec@1 93.750 (87.112)\nEpoch: [170][1000/1062]\\tLoss 3.9068 (3.6694)\\tPrec@1 87.500 (87.000)\nEpoch: [170][1061/1062]\\tLoss 3.4028 (3.6923)\\tPrec@1 84.615 (86.915)\n * Prec@1 86.570\nEpoch: [171][0/1062]\\tLoss 14.3858 (14.3858)\\tPrec@1 87.500 (87.500)\nEpoch: [171][100/1062]\\tLoss 1.7840 (3.5505)\\tPrec@1 87.500 (86.943)\nEpoch: [171][200/1062]\\tLoss 1.6606 (3.7654)\\tPrec@1 81.250 (86.256)\nEpoch: [171][300/1062]\\tLoss 4.8359 (3.8111)\\tPrec@1 81.250 (86.130)\nEpoch: [171][400/1062]\\tLoss 2.9721 (3.7312)\\tPrec@1 81.250 (86.331)\nEpoch: [171][500/1062]\\tLoss 2.9148 (3.6673)\\tPrec@1 93.750 (86.377)\nEpoch: [171][600/1062]\\tLoss 1.8632 (3.6532)\\tPrec@1 93.750 (86.387)\nEpoch: [171][700/1062]\\tLoss 16.5167 (3.6695)\\tPrec@1 81.250 (86.457)\nEpoch: [171][800/1062]\\tLoss 3.8369 (3.6101)\\tPrec@1 81.250 (86.564)\nEpoch: [171][900/1062]\\tLoss 1.0341 (3.6370)\\tPrec@1 93.750 (86.536)\nEpoch: [171][1000/1062]\\tLoss 5.8504 (3.6169)\\tPrec@1 81.250 (86.526)\nEpoch: [171][1061/1062]\\tLoss 3.8781 (3.6055)\\tPrec@1 84.615 (86.544)\n * Prec@1 86.270\nEpoch: [172][0/1062]\\tLoss 4.0550 (4.0550)\\tPrec@1 81.250 (81.250)\nEpoch: [172][100/1062]\\tLoss 4.7162 (3.7702)\\tPrec@1 68.750 (86.262)\nEpoch: [172][200/1062]\\tLoss 0.7943 (3.6795)\\tPrec@1 87.500 (86.225)\nEpoch: [172][300/1062]\\tLoss 4.5417 (3.5857)\\tPrec@1 87.500 (87.085)\nEpoch: [172][400/1062]\\tLoss 0.4385 (3.4930)\\tPrec@1 100.000 (86.783)\nEpoch: [172][500/1062]\\tLoss 3.1158 (3.5503)\\tPrec@1 93.750 (86.776)\nEpoch: [172][600/1062]\\tLoss 2.8933 (3.4581)\\tPrec@1 87.500 (86.814)\nEpoch: [172][700/1062]\\tLoss 7.0785 (3.4705)\\tPrec@1 62.500 (86.813)\nEpoch: [172][800/1062]\\tLoss 7.9242 (3.5239)\\tPrec@1 87.500 (86.650)\nEpoch: [172][900/1062]\\tLoss 1.7318 (3.5677)\\tPrec@1 93.750 (86.758)\nEpoch: [172][1000/1062]\\tLoss 1.0446 (3.5150)\\tPrec@1 93.750 (86.757)\nEpoch: [172][1061/1062]\\tLoss 0.5844 (3.5379)\\tPrec@1 100.000 (86.780)\n * Prec@1 86.490\nEpoch: [173][0/1062]\\tLoss 0.8551 (0.8551)\\tPrec@1 93.750 (93.750)\nEpoch: [173][100/1062]\\tLoss 3.5472 (3.4752)\\tPrec@1 87.500 (86.881)\nEpoch: [173][200/1062]\\tLoss 9.0194 (3.6600)\\tPrec@1 68.750 (86.847)\nEpoch: [173][300/1062]\\tLoss 3.1961 (3.6363)\\tPrec@1 87.500 (86.939)\nEpoch: [173][400/1062]\\tLoss 2.6401 (3.7133)\\tPrec@1 87.500 (86.736)\nEpoch: [173][500/1062]\\tLoss 0.3937 (3.7033)\\tPrec@1 93.750 (86.639)\nEpoch: [173][600/1062]\\tLoss 14.5782 (3.6872)\\tPrec@1 81.250 (86.762)\nEpoch: [173][700/1062]\\tLoss 3.3914 (3.6437)\\tPrec@1 75.000 (86.662)\nEpoch: [173][800/1062]\\tLoss 2.3017 (3.6596)\\tPrec@1 87.500 (86.650)\nEpoch: [173][900/1062]\\tLoss 3.0950 (3.5920)\\tPrec@1 75.000 (86.709)\nEpoch: [173][1000/1062]\\tLoss 5.5631 (3.5393)\\tPrec@1 75.000 (86.670)\nEpoch: [173][1061/1062]\\tLoss 1.1284 (3.5362)\\tPrec@1 84.615 (86.738)\n * Prec@1 86.490\nEpoch: [174][0/1062]\\tLoss 6.0425 (6.0425)\\tPrec@1 93.750 (93.750)\nEpoch: [174][100/1062]\\tLoss 1.8040 (3.2325)\\tPrec@1 87.500 (87.438)\nEpoch: [174][200/1062]\\tLoss 1.5416 (3.4456)\\tPrec@1 81.250 (87.407)\nEpoch: [174][300/1062]\\tLoss 3.4459 (3.3770)\\tPrec@1 87.500 (86.919)\nEpoch: [174][400/1062]\\tLoss 5.3089 (3.5034)\\tPrec@1 87.500 (86.799)\nEpoch: [174][500/1062]\\tLoss 0.4639 (3.4415)\\tPrec@1 93.750 (87.001)\nEpoch: [174][600/1062]\\tLoss 2.0168 (3.4739)\\tPrec@1 87.500 (86.918)\nEpoch: [174][700/1062]\\tLoss 4.4368 (3.4533)\\tPrec@1 81.250 (86.965)\nEpoch: [174][800/1062]\\tLoss 1.2866 (3.4769)\\tPrec@1 93.750 (87.032)\nEpoch: [174][900/1062]\\tLoss 6.2458 (3.4530)\\tPrec@1 68.750 (87.167)\nEpoch: [174][1000/1062]\\tLoss 1.3751 (3.4445)\\tPrec@1 87.500 (87.169)\nEpoch: [174][1061/1062]\\tLoss 6.4499 (3.4640)\\tPrec@1 53.846 (87.209)\n * Prec@1 86.540\nEpoch: [175][0/1062]\\tLoss 13.3540 (13.3540)\\tPrec@1 75.000 (75.000)\nEpoch: [175][100/1062]\\tLoss 0.7263 (3.4223)\\tPrec@1 93.750 (87.686)\nEpoch: [175][200/1062]\\tLoss 1.1282 (3.0611)\\tPrec@1 87.500 (88.122)\nEpoch: [175][300/1062]\\tLoss 9.6057 (3.3134)\\tPrec@1 75.000 (87.687)\nEpoch: [175][400/1062]\\tLoss 7.3098 (3.3201)\\tPrec@1 62.500 (87.484)\nEpoch: [175][500/1062]\\tLoss 4.6977 (3.3589)\\tPrec@1 75.000 (87.488)\nEpoch: [175][600/1062]\\tLoss 10.1766 (3.3837)\\tPrec@1 87.500 (87.448)\nEpoch: [175][700/1062]\\tLoss 2.4711 (3.4664)\\tPrec@1 87.500 (87.518)\nEpoch: [175][800/1062]\\tLoss 1.5570 (3.5362)\\tPrec@1 100.000 (87.313)\nEpoch: [175][900/1062]\\tLoss 0.6330 (3.4712)\\tPrec@1 93.750 (87.354)\nEpoch: [175][1000/1062]\\tLoss 0.6298 (3.4790)\\tPrec@1 93.750 (87.225)\nEpoch: [175][1061/1062]\\tLoss 0.3182 (3.4638)\\tPrec@1 100.000 (87.192)\n * Prec@1 86.500\nEpoch: [176][0/1062]\\tLoss 2.7251 (2.7251)\\tPrec@1 87.500 (87.500)\nEpoch: [176][100/1062]\\tLoss 0.2356 (3.6939)\\tPrec@1 100.000 (86.881)\nEpoch: [176][200/1062]\\tLoss 3.6034 (3.3330)\\tPrec@1 87.500 (87.282)\nEpoch: [176][300/1062]\\tLoss 1.6545 (3.3070)\\tPrec@1 87.500 (87.355)\nEpoch: [176][400/1062]\\tLoss 2.6816 (3.3564)\\tPrec@1 87.500 (87.484)\nEpoch: [176][500/1062]\\tLoss 2.4177 (3.3678)\\tPrec@1 87.500 (87.562)\nEpoch: [176][600/1062]\\tLoss 2.2419 (3.3445)\\tPrec@1 93.750 (87.531)\nEpoch: [176][700/1062]\\tLoss 8.4981 (3.3322)\\tPrec@1 87.500 (87.357)\nEpoch: [176][800/1062]\\tLoss 3.7356 (3.4168)\\tPrec@1 87.500 (87.274)\nEpoch: [176][900/1062]\\tLoss 1.8541 (3.3774)\\tPrec@1 93.750 (87.403)\nEpoch: [176][1000/1062]\\tLoss 7.8354 (3.3616)\\tPrec@1 81.250 (87.369)\nEpoch: [176][1061/1062]\\tLoss 3.0062 (3.3727)\\tPrec@1 84.615 (87.368)\n * Prec@1 86.610\nEpoch: [177][0/1062]\\tLoss 1.0646 (1.0646)\\tPrec@1 93.750 (93.750)\nEpoch: [177][100/1062]\\tLoss 7.0754 (3.0481)\\tPrec@1 81.250 (87.809)\nEpoch: [177][200/1062]\\tLoss 2.0276 (3.7242)\\tPrec@1 100.000 (86.692)\nEpoch: [177][300/1062]\\tLoss 1.6251 (3.5169)\\tPrec@1 93.750 (87.126)\nEpoch: [177][400/1062]\\tLoss 5.1167 (3.3517)\\tPrec@1 93.750 (87.235)\nEpoch: [177][500/1062]\\tLoss 0.9255 (3.3458)\\tPrec@1 87.500 (87.325)\nEpoch: [177][600/1062]\\tLoss 2.7988 (3.2858)\\tPrec@1 81.250 (87.469)\nEpoch: [177][700/1062]\\tLoss 2.9234 (3.3097)\\tPrec@1 81.250 (87.331)\nEpoch: [177][800/1062]\\tLoss 0.6279 (3.2986)\\tPrec@1 100.000 (87.321)\nEpoch: [177][900/1062]\\tLoss 2.8405 (3.3775)\\tPrec@1 93.750 (87.313)\nEpoch: [177][1000/1062]\\tLoss 1.3575 (3.4546)\\tPrec@1 93.750 (87.300)\nEpoch: [177][1061/1062]\\tLoss 2.0227 (3.4570)\\tPrec@1 69.231 (87.245)\n * Prec@1 86.720\nEpoch: [178][0/1062]\\tLoss 7.0042 (7.0042)\\tPrec@1 93.750 (93.750)\nEpoch: [178][100/1062]\\tLoss 1.1156 (3.0025)\\tPrec@1 100.000 (86.077)\nEpoch: [178][200/1062]\\tLoss 2.5766 (3.3227)\\tPrec@1 87.500 (86.225)\nEpoch: [178][300/1062]\\tLoss 12.3547 (3.3448)\\tPrec@1 75.000 (86.586)\nEpoch: [178][400/1062]\\tLoss 1.4795 (3.4617)\\tPrec@1 87.500 (86.674)\nEpoch: [178][500/1062]\\tLoss 0.4729 (3.4818)\\tPrec@1 100.000 (86.776)\nEpoch: [178][600/1062]\\tLoss 0.9347 (3.4236)\\tPrec@1 93.750 (86.855)\nEpoch: [178][700/1062]\\tLoss 1.8173 (3.4421)\\tPrec@1 81.250 (86.876)\nEpoch: [178][800/1062]\\tLoss 5.1283 (3.4661)\\tPrec@1 93.750 (86.813)\nEpoch: [178][900/1062]\\tLoss 5.3653 (3.4636)\\tPrec@1 87.500 (86.799)\nEpoch: [178][1000/1062]\\tLoss 8.0260 (3.4408)\\tPrec@1 75.000 (86.869)\nEpoch: [178][1061/1062]\\tLoss 1.5674 (3.4610)\\tPrec@1 92.308 (86.921)\n * Prec@1 86.520\nEpoch: [179][0/1062]\\tLoss 3.2027 (3.2027)\\tPrec@1 81.250 (81.250)\nEpoch: [179][100/1062]\\tLoss 2.0011 (3.3310)\\tPrec@1 93.750 (85.767)\nEpoch: [179][200/1062]\\tLoss 3.3829 (3.2993)\\tPrec@1 81.250 (86.318)\nEpoch: [179][300/1062]\\tLoss 7.6173 (3.3158)\\tPrec@1 81.250 (86.898)\nEpoch: [179][400/1062]\\tLoss 6.6656 (3.3534)\\tPrec@1 75.000 (86.954)\nEpoch: [179][500/1062]\\tLoss 2.0480 (3.3921)\\tPrec@1 93.750 (86.876)\nEpoch: [179][600/1062]\\tLoss 4.8935 (3.3779)\\tPrec@1 75.000 (87.022)\nEpoch: [179][700/1062]\\tLoss 8.4162 (3.3639)\\tPrec@1 75.000 (86.894)\nEpoch: [179][800/1062]\\tLoss 2.0473 (3.3775)\\tPrec@1 87.500 (86.946)\nEpoch: [179][900/1062]\\tLoss 3.5819 (3.3378)\\tPrec@1 81.250 (86.931)\nEpoch: [179][1000/1062]\\tLoss 1.8780 (3.3122)\\tPrec@1 87.500 (87.050)\nEpoch: [179][1061/1062]\\tLoss 1.8112 (3.3332)\\tPrec@1 69.231 (87.074)\n * Prec@1 86.720\nEpoch: [180][0/1062]\\tLoss 7.3205 (7.3205)\\tPrec@1 81.250 (81.250)\nEpoch: [180][100/1062]\\tLoss 0.6357 (3.1827)\\tPrec@1 93.750 (86.634)\nEpoch: [180][200/1062]\\tLoss 10.0545 (3.6259)\\tPrec@1 68.750 (86.629)\nEpoch: [180][300/1062]\\tLoss 3.1717 (3.6006)\\tPrec@1 81.250 (86.752)\nEpoch: [180][400/1062]\\tLoss 4.9876 (3.5187)\\tPrec@1 87.500 (87.064)\nEpoch: [180][500/1062]\\tLoss 1.0914 (3.5144)\\tPrec@1 93.750 (87.151)\nEpoch: [180][600/1062]\\tLoss 10.2573 (3.4497)\\tPrec@1 75.000 (87.167)\nEpoch: [180][700/1062]\\tLoss 7.5249 (3.4858)\\tPrec@1 81.250 (87.090)\nEpoch: [180][800/1062]\\tLoss 4.3037 (3.4088)\\tPrec@1 81.250 (87.079)\nEpoch: [180][900/1062]\\tLoss 2.4346 (3.4238)\\tPrec@1 75.000 (87.091)\nEpoch: [180][1000/1062]\\tLoss 1.7738 (3.4463)\\tPrec@1 87.500 (87.107)\nEpoch: [180][1061/1062]\\tLoss 6.7930 (3.4444)\\tPrec@1 69.231 (87.045)\n * Prec@1 86.560\nEpoch: [181][0/1062]\\tLoss 1.6057 (1.6057)\\tPrec@1 87.500 (87.500)\nEpoch: [181][100/1062]\\tLoss 12.3558 (4.0755)\\tPrec@1 81.250 (87.005)\nEpoch: [181][200/1062]\\tLoss 2.0223 (3.6154)\\tPrec@1 87.500 (87.034)\nEpoch: [181][300/1062]\\tLoss 7.0766 (3.5777)\\tPrec@1 68.750 (86.711)\nEpoch: [181][400/1062]\\tLoss 10.3418 (3.5934)\\tPrec@1 93.750 (86.721)\nEpoch: [181][500/1062]\\tLoss 3.1102 (3.5821)\\tPrec@1 81.250 (86.502)\nEpoch: [181][600/1062]\\tLoss 2.0908 (3.4618)\\tPrec@1 87.500 (86.710)\nEpoch: [181][700/1062]\\tLoss 0.2287 (3.4635)\\tPrec@1 100.000 (86.769)\nEpoch: [181][800/1062]\\tLoss 5.9624 (3.5160)\\tPrec@1 87.500 (86.657)\nEpoch: [181][900/1062]\\tLoss 1.4460 (3.4656)\\tPrec@1 93.750 (86.834)\nEpoch: [181][1000/1062]\\tLoss 3.6711 (3.4335)\\tPrec@1 81.250 (86.932)\nEpoch: [181][1061/1062]\\tLoss 2.4966 (3.4424)\\tPrec@1 84.615 (86.909)\n * Prec@1 86.550\nEpoch: [182][0/1062]\\tLoss 1.6654 (1.6654)\\tPrec@1 87.500 (87.500)\nEpoch: [182][100/1062]\\tLoss 0.9224 (3.9821)\\tPrec@1 87.500 (85.458)\nEpoch: [182][200/1062]\\tLoss 5.7764 (3.5551)\\tPrec@1 81.250 (86.785)\nEpoch: [182][300/1062]\\tLoss 3.6950 (3.4623)\\tPrec@1 81.250 (87.375)\nEpoch: [182][400/1062]\\tLoss 0.8767 (3.4875)\\tPrec@1 93.750 (87.032)\nEpoch: [182][500/1062]\\tLoss 6.4646 (3.3774)\\tPrec@1 75.000 (86.976)\nEpoch: [182][600/1062]\\tLoss 1.3572 (3.3100)\\tPrec@1 87.500 (87.042)\nEpoch: [182][700/1062]\\tLoss 2.1939 (3.3293)\\tPrec@1 75.000 (87.001)\nEpoch: [182][800/1062]\\tLoss 3.2889 (3.3434)\\tPrec@1 93.750 (87.227)\nEpoch: [182][900/1062]\\tLoss 1.3800 (3.2943)\\tPrec@1 81.250 (87.299)\nEpoch: [182][1000/1062]\\tLoss 1.2176 (3.3610)\\tPrec@1 87.500 (87.200)\nEpoch: [182][1061/1062]\\tLoss 5.4407 (3.3839)\\tPrec@1 92.308 (87.239)\n * Prec@1 86.730\nEpoch: [183][0/1062]\\tLoss 4.7791 (4.7791)\\tPrec@1 68.750 (68.750)\nEpoch: [183][100/1062]\\tLoss 1.7897 (3.4744)\\tPrec@1 87.500 (86.696)\nEpoch: [183][200/1062]\\tLoss 6.2517 (3.3976)\\tPrec@1 81.250 (86.971)\nEpoch: [183][300/1062]\\tLoss 1.4785 (3.3834)\\tPrec@1 93.750 (87.105)\nEpoch: [183][400/1062]\\tLoss 3.7453 (3.3160)\\tPrec@1 81.250 (87.313)\nEpoch: [183][500/1062]\\tLoss 0.6675 (3.3289)\\tPrec@1 93.750 (87.375)\nEpoch: [183][600/1062]\\tLoss 0.9528 (3.2960)\\tPrec@1 93.750 (87.438)\nEpoch: [183][700/1062]\\tLoss 10.8347 (3.2670)\\tPrec@1 81.250 (87.402)\nEpoch: [183][800/1062]\\tLoss 2.2385 (3.3224)\\tPrec@1 93.750 (87.141)\nEpoch: [183][900/1062]\\tLoss 8.7094 (3.3655)\\tPrec@1 87.500 (87.014)\nEpoch: [183][1000/1062]\\tLoss 0.9266 (3.4023)\\tPrec@1 93.750 (86.938)\nEpoch: [183][1061/1062]\\tLoss 0.5054 (3.3703)\\tPrec@1 100.000 (86.944)\n * Prec@1 86.870\nEpoch: [184][0/1062]\\tLoss 3.9646 (3.9646)\\tPrec@1 68.750 (68.750)\nEpoch: [184][100/1062]\\tLoss 2.3362 (3.4784)\\tPrec@1 87.500 (85.644)\nEpoch: [184][200/1062]\\tLoss 1.5914 (3.5805)\\tPrec@1 87.500 (85.728)\nEpoch: [184][300/1062]\\tLoss 5.1866 (3.4736)\\tPrec@1 81.250 (86.109)\nEpoch: [184][400/1062]\\tLoss 3.7555 (3.4792)\\tPrec@1 81.250 (86.269)\nEpoch: [184][500/1062]\\tLoss 1.0922 (3.5314)\\tPrec@1 93.750 (85.941)\nEpoch: [184][600/1062]\\tLoss 0.5683 (3.4879)\\tPrec@1 100.000 (86.242)\nEpoch: [184][700/1062]\\tLoss 1.4290 (3.4536)\\tPrec@1 87.500 (86.626)\nEpoch: [184][800/1062]\\tLoss 5.7233 (3.4238)\\tPrec@1 81.250 (86.759)\nEpoch: [184][900/1062]\\tLoss 4.6247 (3.4041)\\tPrec@1 93.750 (86.779)\nEpoch: [184][1000/1062]\\tLoss 2.3300 (3.4074)\\tPrec@1 81.250 (86.838)\nEpoch: [184][1061/1062]\\tLoss 0.9632 (3.4249)\\tPrec@1 100.000 (86.791)\n * Prec@1 86.310\nEpoch: [185][0/1062]\\tLoss 0.7188 (0.7188)\\tPrec@1 100.000 (100.000)\nEpoch: [185][100/1062]\\tLoss 3.3644 (3.7177)\\tPrec@1 93.750 (87.500)\nEpoch: [185][200/1062]\\tLoss 5.8444 (3.6162)\\tPrec@1 75.000 (86.598)\nEpoch: [185][300/1062]\\tLoss 0.9919 (3.5975)\\tPrec@1 93.750 (86.483)\nEpoch: [185][400/1062]\\tLoss 1.1238 (3.4515)\\tPrec@1 87.500 (86.830)\nEpoch: [185][500/1062]\\tLoss 4.3077 (3.5145)\\tPrec@1 81.250 (86.739)\nEpoch: [185][600/1062]\\tLoss 1.5062 (3.4523)\\tPrec@1 93.750 (86.699)\nEpoch: [185][700/1062]\\tLoss 1.1129 (3.4455)\\tPrec@1 93.750 (86.849)\nEpoch: [185][800/1062]\\tLoss 3.5153 (3.4310)\\tPrec@1 87.500 (86.751)\nEpoch: [185][900/1062]\\tLoss 6.2541 (3.4694)\\tPrec@1 75.000 (86.668)\nEpoch: [185][1000/1062]\\tLoss 5.8386 (3.4636)\\tPrec@1 81.250 (86.651)\nEpoch: [185][1061/1062]\\tLoss 4.5715 (3.4513)\\tPrec@1 84.615 (86.615)\n * Prec@1 86.950\nEpoch: [186][0/1062]\\tLoss 1.5937 (1.5937)\\tPrec@1 87.500 (87.500)\nEpoch: [186][100/1062]\\tLoss 8.3842 (3.9444)\\tPrec@1 75.000 (87.067)\nEpoch: [186][200/1062]\\tLoss 0.3515 (3.5886)\\tPrec@1 93.750 (87.096)\nEpoch: [186][300/1062]\\tLoss 3.8970 (3.5268)\\tPrec@1 81.250 (86.877)\nEpoch: [186][400/1062]\\tLoss 1.4690 (3.4761)\\tPrec@1 81.250 (86.752)\nEpoch: [186][500/1062]\\tLoss 0.7236 (3.4319)\\tPrec@1 93.750 (86.789)\nEpoch: [186][600/1062]\\tLoss 3.5981 (3.4581)\\tPrec@1 81.250 (86.803)\nEpoch: [186][700/1062]\\tLoss 4.0767 (3.4052)\\tPrec@1 68.750 (86.831)\nEpoch: [186][800/1062]\\tLoss 3.9825 (3.3425)\\tPrec@1 93.750 (86.946)\nEpoch: [186][900/1062]\\tLoss 5.1793 (3.3496)\\tPrec@1 87.500 (86.959)\nEpoch: [186][1000/1062]\\tLoss 1.4357 (3.3670)\\tPrec@1 87.500 (86.907)\nEpoch: [186][1061/1062]\\tLoss 0.3480 (3.3770)\\tPrec@1 92.308 (86.874)\n * Prec@1 86.590\nEpoch: [187][0/1062]\\tLoss 7.2664 (7.2664)\\tPrec@1 81.250 (81.250)\nEpoch: [187][100/1062]\\tLoss 0.4814 (2.7527)\\tPrec@1 100.000 (87.871)\nEpoch: [187][200/1062]\\tLoss 1.8095 (3.1635)\\tPrec@1 81.250 (87.251)\nEpoch: [187][300/1062]\\tLoss 9.0259 (3.1086)\\tPrec@1 75.000 (87.853)\nEpoch: [187][400/1062]\\tLoss 5.0581 (3.0740)\\tPrec@1 75.000 (87.812)\nEpoch: [187][500/1062]\\tLoss 4.8290 (3.1444)\\tPrec@1 87.500 (87.849)\nEpoch: [187][600/1062]\\tLoss 0.8145 (3.2062)\\tPrec@1 93.750 (87.521)\nEpoch: [187][700/1062]\\tLoss 7.9223 (3.2164)\\tPrec@1 68.750 (87.375)\nEpoch: [187][800/1062]\\tLoss 2.0894 (3.2631)\\tPrec@1 87.500 (87.266)\nEpoch: [187][900/1062]\\tLoss 3.2201 (3.2046)\\tPrec@1 87.500 (87.438)\nEpoch: [187][1000/1062]\\tLoss 1.8190 (3.2113)\\tPrec@1 81.250 (87.294)\nEpoch: [187][1061/1062]\\tLoss 1.8822 (3.2626)\\tPrec@1 84.615 (87.186)\n * Prec@1 86.570\nEpoch: [188][0/1062]\\tLoss 0.5721 (0.5721)\\tPrec@1 100.000 (100.000)\nEpoch: [188][100/1062]\\tLoss 0.6706 (3.5885)\\tPrec@1 100.000 (86.324)\nEpoch: [188][200/1062]\\tLoss 4.7584 (3.3164)\\tPrec@1 81.250 (87.034)\nEpoch: [188][300/1062]\\tLoss 1.0775 (3.4969)\\tPrec@1 93.750 (87.022)\nEpoch: [188][400/1062]\\tLoss 9.2546 (3.5101)\\tPrec@1 87.500 (86.861)\nEpoch: [188][500/1062]\\tLoss 1.8684 (3.5660)\\tPrec@1 87.500 (86.976)\nEpoch: [188][600/1062]\\tLoss 2.1064 (3.4191)\\tPrec@1 75.000 (86.970)\nEpoch: [188][700/1062]\\tLoss 4.2600 (3.3999)\\tPrec@1 87.500 (86.867)\nEpoch: [188][800/1062]\\tLoss 2.3733 (3.3880)\\tPrec@1 93.750 (86.806)\nEpoch: [188][900/1062]\\tLoss 2.7244 (3.3335)\\tPrec@1 87.500 (86.994)\nEpoch: [188][1000/1062]\\tLoss 0.8543 (3.3518)\\tPrec@1 87.500 (86.907)\nEpoch: [188][1061/1062]\\tLoss 2.4640 (3.3441)\\tPrec@1 76.923 (86.903)\n * Prec@1 86.660\nEpoch: [189][0/1062]\\tLoss 9.4802 (9.4802)\\tPrec@1 68.750 (68.750)\nEpoch: [189][100/1062]\\tLoss 0.3100 (3.4907)\\tPrec@1 100.000 (87.871)\nEpoch: [189][200/1062]\\tLoss 1.3314 (3.4553)\\tPrec@1 93.750 (88.122)\nEpoch: [189][300/1062]\\tLoss 9.2468 (3.4480)\\tPrec@1 81.250 (87.915)\nEpoch: [189][400/1062]\\tLoss 10.2675 (3.3854)\\tPrec@1 81.250 (87.656)\nEpoch: [189][500/1062]\\tLoss 1.5208 (3.3354)\\tPrec@1 93.750 (87.762)\nEpoch: [189][600/1062]\\tLoss 1.6268 (3.3238)\\tPrec@1 87.500 (87.562)\nEpoch: [189][700/1062]\\tLoss 6.6038 (3.2940)\\tPrec@1 87.500 (87.669)\nEpoch: [189][800/1062]\\tLoss 2.3402 (3.2669)\\tPrec@1 87.500 (87.578)\nEpoch: [189][900/1062]\\tLoss 0.8998 (3.2734)\\tPrec@1 100.000 (87.507)\nEpoch: [189][1000/1062]\\tLoss 2.7930 (3.3080)\\tPrec@1 75.000 (87.481)\nEpoch: [189][1061/1062]\\tLoss 2.0889 (3.3393)\\tPrec@1 69.231 (87.386)\n * Prec@1 86.640\nEpoch: [190][0/1062]\\tLoss 11.6054 (11.6054)\\tPrec@1 81.250 (81.250)\nEpoch: [190][100/1062]\\tLoss 1.9882 (3.5646)\\tPrec@1 93.750 (87.129)\nEpoch: [190][200/1062]\\tLoss 3.6065 (3.4316)\\tPrec@1 75.000 (87.002)\nEpoch: [190][300/1062]\\tLoss 0.3885 (3.1183)\\tPrec@1 100.000 (87.438)\nEpoch: [190][400/1062]\\tLoss 4.8922 (3.3453)\\tPrec@1 87.500 (87.064)\nEpoch: [190][500/1062]\\tLoss 1.5272 (3.3025)\\tPrec@1 100.000 (87.325)\nEpoch: [190][600/1062]\\tLoss 1.1598 (3.3280)\\tPrec@1 93.750 (87.250)\nEpoch: [190][700/1062]\\tLoss 7.2856 (3.2785)\\tPrec@1 87.500 (87.580)\nEpoch: [190][800/1062]\\tLoss 2.8114 (3.2973)\\tPrec@1 87.500 (87.406)\nEpoch: [190][900/1062]\\tLoss 3.2496 (3.2747)\\tPrec@1 75.000 (87.320)\nEpoch: [190][1000/1062]\\tLoss 0.9096 (3.2292)\\tPrec@1 93.750 (87.375)\nEpoch: [190][1061/1062]\\tLoss 1.6199 (3.1999)\\tPrec@1 92.308 (87.427)\n * Prec@1 86.540\nEpoch: [191][0/1062]\\tLoss 1.8131 (1.8131)\\tPrec@1 87.500 (87.500)\nEpoch: [191][100/1062]\\tLoss 2.4639 (2.9290)\\tPrec@1 87.500 (88.304)\nEpoch: [191][200/1062]\\tLoss 0.7556 (3.0268)\\tPrec@1 93.750 (87.345)\nEpoch: [191][300/1062]\\tLoss 5.9044 (3.1392)\\tPrec@1 62.500 (87.147)\nEpoch: [191][400/1062]\\tLoss 0.2191 (3.1109)\\tPrec@1 100.000 (87.344)\nEpoch: [191][500/1062]\\tLoss 3.4271 (3.0356)\\tPrec@1 81.250 (87.550)\nEpoch: [191][600/1062]\\tLoss 5.0327 (3.0341)\\tPrec@1 87.500 (87.469)\nEpoch: [191][700/1062]\\tLoss 2.8916 (3.0980)\\tPrec@1 81.250 (87.438)\nEpoch: [191][800/1062]\\tLoss 8.6733 (3.1187)\\tPrec@1 87.500 (87.266)\nEpoch: [191][900/1062]\\tLoss 3.8398 (3.1378)\\tPrec@1 87.500 (87.195)\nEpoch: [191][1000/1062]\\tLoss 8.5269 (3.1682)\\tPrec@1 75.000 (87.144)\nEpoch: [191][1061/1062]\\tLoss 1.8875 (3.1483)\\tPrec@1 84.615 (87.145)\n * Prec@1 86.710\nEpoch: [192][0/1062]\\tLoss 2.9126 (2.9126)\\tPrec@1 87.500 (87.500)\nEpoch: [192][100/1062]\\tLoss 1.8463 (3.5470)\\tPrec@1 87.500 (86.510)\nEpoch: [192][200/1062]\\tLoss 0.7503 (3.5670)\\tPrec@1 93.750 (86.505)\nEpoch: [192][300/1062]\\tLoss 5.9192 (3.5484)\\tPrec@1 81.250 (86.607)\nEpoch: [192][400/1062]\\tLoss 1.8710 (3.3812)\\tPrec@1 81.250 (87.079)\nEpoch: [192][500/1062]\\tLoss 5.9666 (3.2654)\\tPrec@1 75.000 (87.488)\nEpoch: [192][600/1062]\\tLoss 0.4077 (3.2821)\\tPrec@1 100.000 (87.365)\nEpoch: [192][700/1062]\\tLoss 1.7974 (3.2755)\\tPrec@1 87.500 (87.268)\nEpoch: [192][800/1062]\\tLoss 1.7230 (3.2435)\\tPrec@1 93.750 (87.211)\nEpoch: [192][900/1062]\\tLoss 1.5654 (3.2532)\\tPrec@1 93.750 (87.229)\nEpoch: [192][1000/1062]\\tLoss 4.6852 (3.2381)\\tPrec@1 75.000 (87.182)\nEpoch: [192][1061/1062]\\tLoss 0.1668 (3.2466)\\tPrec@1 100.000 (87.198)\n * Prec@1 86.610\nEpoch: [193][0/1062]\\tLoss 1.0767 (1.0767)\\tPrec@1 87.500 (87.500)\nEpoch: [193][100/1062]\\tLoss 0.4915 (3.1565)\\tPrec@1 100.000 (86.943)\nEpoch: [193][200/1062]\\tLoss 1.7142 (3.3884)\\tPrec@1 81.250 (86.723)\nEpoch: [193][300/1062]\\tLoss 5.9006 (3.3795)\\tPrec@1 75.000 (86.254)\nEpoch: [193][400/1062]\\tLoss 0.4500 (3.2973)\\tPrec@1 100.000 (86.565)\nEpoch: [193][500/1062]\\tLoss 3.6063 (3.2974)\\tPrec@1 87.500 (86.727)\nEpoch: [193][600/1062]\\tLoss 4.0944 (3.3134)\\tPrec@1 81.250 (86.658)\nEpoch: [193][700/1062]\\tLoss 8.9821 (3.3611)\\tPrec@1 87.500 (86.760)\nEpoch: [193][800/1062]\\tLoss 3.4130 (3.3045)\\tPrec@1 75.000 (86.946)\nEpoch: [193][900/1062]\\tLoss 11.0687 (3.3322)\\tPrec@1 75.000 (86.813)\nEpoch: [193][1000/1062]\\tLoss 2.8880 (3.3438)\\tPrec@1 93.750 (86.851)\nEpoch: [193][1061/1062]\\tLoss 5.7307 (3.3455)\\tPrec@1 76.923 (86.809)\n * Prec@1 86.670\nEpoch: [194][0/1062]\\tLoss 2.3041 (2.3041)\\tPrec@1 93.750 (93.750)\nEpoch: [194][100/1062]\\tLoss 2.4739 (3.1524)\\tPrec@1 81.250 (87.191)\nEpoch: [194][200/1062]\\tLoss 5.1671 (3.1861)\\tPrec@1 87.500 (87.189)\nEpoch: [194][300/1062]\\tLoss 13.4598 (3.4340)\\tPrec@1 62.500 (86.628)\nEpoch: [194][400/1062]\\tLoss 2.4323 (3.3976)\\tPrec@1 81.250 (86.534)\nEpoch: [194][500/1062]\\tLoss 2.9113 (3.3730)\\tPrec@1 87.500 (86.689)\nEpoch: [194][600/1062]\\tLoss 11.3016 (3.3295)\\tPrec@1 81.250 (86.855)\nEpoch: [194][700/1062]\\tLoss 0.7299 (3.3208)\\tPrec@1 93.750 (86.796)\nEpoch: [194][800/1062]\\tLoss 2.9736 (3.2673)\\tPrec@1 81.250 (86.767)\nEpoch: [194][900/1062]\\tLoss 3.3216 (3.2735)\\tPrec@1 75.000 (86.862)\nEpoch: [194][1000/1062]\\tLoss 1.6674 (3.2678)\\tPrec@1 87.500 (86.782)\nEpoch: [194][1061/1062]\\tLoss 1.6722 (3.2842)\\tPrec@1 84.615 (86.756)\n * Prec@1 86.880\nEpoch: [195][0/1062]\\tLoss 2.4416 (2.4416)\\tPrec@1 81.250 (81.250)\nEpoch: [195][100/1062]\\tLoss 0.5330 (2.9523)\\tPrec@1 100.000 (86.757)\nEpoch: [195][200/1062]\\tLoss 1.0400 (3.2691)\\tPrec@1 93.750 (86.754)\nEpoch: [195][300/1062]\\tLoss 5.3336 (3.2362)\\tPrec@1 81.250 (87.209)\nEpoch: [195][400/1062]\\tLoss 1.9390 (3.2076)\\tPrec@1 100.000 (87.329)\nEpoch: [195][500/1062]\\tLoss 1.8788 (3.2301)\\tPrec@1 87.500 (87.275)\nEpoch: [195][600/1062]\\tLoss 0.5625 (3.2103)\\tPrec@1 93.750 (87.344)\nEpoch: [195][700/1062]\\tLoss 2.4443 (3.3145)\\tPrec@1 87.500 (87.081)\nEpoch: [195][800/1062]\\tLoss 0.9372 (3.2997)\\tPrec@1 87.500 (86.954)\nEpoch: [195][900/1062]\\tLoss 0.9013 (3.2744)\\tPrec@1 93.750 (87.209)\nEpoch: [195][1000/1062]\\tLoss 1.4580 (3.2955)\\tPrec@1 93.750 (87.163)\nEpoch: [195][1061/1062]\\tLoss 0.2145 (3.2811)\\tPrec@1 100.000 (87.115)\n * Prec@1 86.540\nEpoch: [196][0/1062]\\tLoss 1.6931 (1.6931)\\tPrec@1 93.750 (93.750)\nEpoch: [196][100/1062]\\tLoss 1.7523 (3.1137)\\tPrec@1 87.500 (86.881)\nEpoch: [196][200/1062]\\tLoss 1.1848 (3.2821)\\tPrec@1 100.000 (86.536)\nEpoch: [196][300/1062]\\tLoss 1.2242 (3.4939)\\tPrec@1 93.750 (86.503)\nEpoch: [196][400/1062]\\tLoss 0.8498 (3.4467)\\tPrec@1 93.750 (86.767)\nEpoch: [196][500/1062]\\tLoss 3.7563 (3.5304)\\tPrec@1 93.750 (86.751)\nEpoch: [196][600/1062]\\tLoss 3.8882 (3.5167)\\tPrec@1 81.250 (86.834)\nEpoch: [196][700/1062]\\tLoss 5.9989 (3.5277)\\tPrec@1 75.000 (86.778)\nEpoch: [196][800/1062]\\tLoss 1.9371 (3.4943)\\tPrec@1 87.500 (86.829)\nEpoch: [196][900/1062]\\tLoss 2.2714 (3.4496)\\tPrec@1 100.000 (86.903)\nEpoch: [196][1000/1062]\\tLoss 4.0754 (3.4459)\\tPrec@1 68.750 (86.794)\nEpoch: [196][1061/1062]\\tLoss 0.3410 (3.4094)\\tPrec@1 100.000 (86.809)\n * Prec@1 86.770\nEpoch: [197][0/1062]\\tLoss 0.4515 (0.4515)\\tPrec@1 93.750 (93.750)\nEpoch: [197][100/1062]\\tLoss 7.4311 (3.1498)\\tPrec@1 87.500 (88.057)\nEpoch: [197][200/1062]\\tLoss 1.1326 (3.4858)\\tPrec@1 93.750 (87.313)\nEpoch: [197][300/1062]\\tLoss 1.7687 (3.4794)\\tPrec@1 87.500 (87.043)\nEpoch: [197][400/1062]\\tLoss 4.3110 (3.3096)\\tPrec@1 87.500 (87.297)\nEpoch: [197][500/1062]\\tLoss 0.5610 (3.2898)\\tPrec@1 93.750 (87.250)\nEpoch: [197][600/1062]\\tLoss 5.7284 (3.2113)\\tPrec@1 81.250 (87.240)\nEpoch: [197][700/1062]\\tLoss 8.4025 (3.2422)\\tPrec@1 68.750 (87.170)\nEpoch: [197][800/1062]\\tLoss 2.8361 (3.2030)\\tPrec@1 81.250 (87.336)\nEpoch: [197][900/1062]\\tLoss 0.2378 (3.2179)\\tPrec@1 100.000 (87.313)\nEpoch: [197][1000/1062]\\tLoss 2.7989 (3.2215)\\tPrec@1 87.500 (87.313)\nEpoch: [197][1061/1062]\\tLoss 2.3637 (3.2238)\\tPrec@1 84.615 (87.268)\n * Prec@1 86.760\nEpoch: [198][0/1062]\\tLoss 1.8598 (1.8598)\\tPrec@1 93.750 (93.750)\nEpoch: [198][100/1062]\\tLoss 0.6443 (3.4297)\\tPrec@1 93.750 (86.757)\nEpoch: [198][200/1062]\\tLoss 9.9386 (3.4379)\\tPrec@1 75.000 (86.816)\nEpoch: [198][300/1062]\\tLoss 3.8383 (3.2923)\\tPrec@1 87.500 (87.251)\nEpoch: [198][400/1062]\\tLoss 11.2005 (3.3219)\\tPrec@1 87.500 (87.313)\nEpoch: [198][500/1062]\\tLoss 4.0005 (3.2982)\\tPrec@1 81.250 (87.363)\nEpoch: [198][600/1062]\\tLoss 1.8945 (3.3075)\\tPrec@1 81.250 (87.271)\nEpoch: [198][700/1062]\\tLoss 2.7195 (3.2520)\\tPrec@1 81.250 (87.259)\nEpoch: [198][800/1062]\\tLoss 5.0924 (3.2446)\\tPrec@1 93.750 (87.328)\nEpoch: [198][900/1062]\\tLoss 3.0134 (3.2989)\\tPrec@1 75.000 (87.271)\nEpoch: [198][1000/1062]\\tLoss 1.8702 (3.2645)\\tPrec@1 93.750 (87.356)\nEpoch: [198][1061/1062]\\tLoss 1.5764 (3.3030)\\tPrec@1 69.231 (87.215)\n * Prec@1 86.370\nEpoch: [199][0/1062]\\tLoss 2.5096 (2.5096)\\tPrec@1 81.250 (81.250)\nEpoch: [199][100/1062]\\tLoss 0.3356 (3.1940)\\tPrec@1 100.000 (87.871)\nEpoch: [199][200/1062]\\tLoss 1.0854 (3.2470)\\tPrec@1 93.750 (87.624)\nEpoch: [199][300/1062]\\tLoss 4.5067 (3.4021)\\tPrec@1 75.000 (86.898)\nEpoch: [199][400/1062]\\tLoss 3.7877 (3.4352)\\tPrec@1 87.500 (87.079)\nEpoch: [199][500/1062]\\tLoss 4.1205 (3.3912)\\tPrec@1 87.500 (87.013)\nEpoch: [199][600/1062]\\tLoss 0.2283 (3.3383)\\tPrec@1 100.000 (87.282)\nEpoch: [199][700/1062]\\tLoss 2.1336 (3.2815)\\tPrec@1 87.500 (87.206)\nEpoch: [199][800/1062]\\tLoss 3.2536 (3.3146)\\tPrec@1 87.500 (87.001)\nEpoch: [199][900/1062]\\tLoss 3.9843 (3.2788)\\tPrec@1 87.500 (87.077)\nEpoch: [199][1000/1062]\\tLoss 6.8182 (3.2509)\\tPrec@1 75.000 (87.175)\nEpoch: [199][1061/1062]\\tLoss 1.1070 (3.2634)\\tPrec@1 100.000 (87.156)\n * Prec@1 86.720\nBest accuracy:  86.95\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# **Plot**","metadata":{}},{"cell_type":"code","source":"%%writefile replicate_figure_1.py\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport argparse\nimport os\n\n# Import from your existing files\nfrom data_utils import build_dataset, new_dataset\nfrom resnet import ResNet32\nfrom Sinkhorn_distance import SinkhornDistance\nfrom Sinkhorn_distance_fl import SinkhornDistance as SinkhornDistance_fl\n\n# Define the to_categorical helper function inside this script\ndef to_categorical(labels, num_classes):\n    return F.one_hot(labels.long(), num_classes=num_classes)\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Figure 1 Replication')\n    # Make sure this checkpoint from your previous CIFAR-10 run exists\n    parser.add_argument('--ckpt_path', type=str,\n                        default='checkpoint/ours/OT_cifar10_IF100_full_run_ckpt.pth.tar',\n                        help='Path to a pre-trained model checkpoint.')\n    parser.add_argument('--num_classes', type=int, default=10)\n    parser.add_argument('--num_meta', type=int, default=10)\n    parser.add_argument('--gpu', default=0, type=int)\n    return parser.parse_args(args=[]) # Use args=[] to prevent conflicts in Colab\n\ndef build_model(ckpt_path, num_classes):\n    model = ResNet32(num_classes)\n    if not os.path.exists(ckpt_path):\n        print(f\"ERROR: Checkpoint file not found at {ckpt_path}. Please ensure Stage 2 training was completed for CIFAR-10.\")\n        return None\n    checkpoint = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(checkpoint['state_dict'])\n    if torch.cuda.is_available():\n        model.cuda()\n    model.eval()\n    return model\n\ndef main():\n    args = get_args()\n    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Load pre-trained model\n    print(\"--> Loading pre-trained model...\")\n    model = build_model(args.ckpt_path, args.num_classes)\n    if model is None: return\n\n    # 2. Create the specific data splits as per the paper\n    print(\"--> Preparing data...\")\n    meta_dataset, train_dataset, _ = build_dataset('cifar10', args.num_meta)\n\n    # Create meta-set prototypes\n    meta_loader = torch.utils.data.DataLoader(new_dataset(meta_dataset, train=False), batch_size=100, shuffle=False)\n    meta_images, meta_labels, _ = next(iter(meta_loader))\n\n    proto_images = torch.zeros([args.num_classes, 3, 32, 32])\n    for i_cls in range(args.num_classes):\n        proto_images[i_cls] = meta_images[meta_labels.squeeze() == i_cls].mean(dim=0)\n    proto_images = proto_images.to(device)\n    proto_labels_onehot = to_categorical(torch.arange(args.num_classes), args.num_classes).to(device)\n\n    # Create the 55-sample imbalanced batch\n    batch_images_list, batch_labels_list = [], []\n    for i_cls in range(args.num_classes):\n        num_samples = 10 - i_cls\n        class_indices = [j for j, label in enumerate(train_dataset.targets) if label == i_cls]\n\n        for idx in class_indices[:num_samples]:\n            img, label, _ = new_dataset(train_dataset, train=False)[idx]\n            batch_images_list.append(img)\n            batch_labels_list.append(label.item())\n\n    batch_images = torch.stack(batch_images_list).to(device)\n    batch_labels = torch.tensor(batch_labels_list).to(device)\n    batch_labels_onehot = to_categorical(batch_labels, args.num_classes).to(device)\n\n    # 3. Get model features\n    with torch.no_grad():\n        proto_features, _ = model(proto_images)\n        batch_features, _ = model(batch_images)\n\n    # 4. Calculate cost matrices and learn weights\n    cost_matrices = {}\n    learned_weights = {}\n\n    criterion_cos = SinkhornDistance(eps=0.1, max_iter=200, dis='cos').to(device)\n    criterion_euc = SinkhornDistance(eps=0.1, max_iter=200, dis='euc').to(device)\n    criterion_comb = SinkhornDistance_fl(eps=0.1, max_iter=200).to(device)\n\n    print(\"--> Calculating matrices and learning weights...\")\n    for cost_type in ['feature', 'label', 'combined']:\n        \n        weights = torch.ones(55, requires_grad=True, device=device)\n       \n        \n        Attoptimizer = torch.optim.SGD([weights], lr=0.1, momentum=0.9)\n\n        for _ in range(20): # Small number of iterations to learn weights\n            probability_train = F.softmax(weights, dim=0)\n            if cost_type == 'feature':\n                cost_matrix = 1 - F.cosine_similarity(batch_features.unsqueeze(1), proto_features.unsqueeze(0), dim=-1)\n                OTloss = criterion_cos(proto_features.detach(), batch_features.detach(), probability_train.squeeze())\n            elif cost_type == 'label':\n                # Use torch.cdist for a clean euclidean distance matrix\n                cost_matrix = torch.cdist(batch_labels_onehot.float(), proto_labels_onehot.float(), p=2.0)\n                OTloss = criterion_euc(proto_labels_onehot.float(), batch_labels_onehot.float(), probability_train.squeeze())\n            else: # combined\n                cost_matrix_feat = 1 - F.cosine_similarity(batch_features.unsqueeze(1), proto_features.unsqueeze(0), dim=-1)\n                cost_matrix_lab = torch.cdist(batch_labels_onehot.float(), proto_labels_onehot.float(), p=2.0)\n                cost_matrix = 0.5 * cost_matrix_feat + 0.5 * cost_matrix_lab\n                OTloss = criterion_comb(proto_features.detach(), batch_features.detach(),\n                                        proto_labels_onehot.float(), batch_labels_onehot.float(), probability_train.squeeze())\n            Attoptimizer.zero_grad()\n            OTloss.backward()\n            Attoptimizer.step()\n\n        cost_matrices[cost_type] = cost_matrix.detach().cpu().numpy()\n        learned_weights[cost_type] = F.softmax(weights.detach(), dim=0).cpu().numpy()\n\n    # 5. Plotting\n    print(\"--> Generating plot...\")\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    fig.tight_layout(pad=6.0)\n\n    plot_order = ['feature', 'label', 'combined']\n    titles = ['Feature-aware Cost Matrix', 'Label-aware Cost Matrix', 'Combined Cost Matrix']\n\n    # Sort samples by class for clearer visualization\n    sorted_indices = np.argsort(batch_labels.cpu().numpy())\n    sorted_labels = batch_labels.cpu().numpy()[sorted_indices]\n\n    for i, cost_type in enumerate(plot_order):\n        # Sort the rows of the cost matrix to group by class\n        sorted_cost_matrix = cost_matrices[cost_type][sorted_indices, :]\n        \n        sns.heatmap(sorted_cost_matrix, ax=axes[0, i], cmap='viridis', cbar=True)\n        axes[0, i].set_title(titles[i], fontsize=14)\n        axes[0, i].set_xlabel(\"Meta-Set Prototypes (Class 0-9)\", fontsize=12)\n        axes[0, i].set_ylabel(\"Imbalanced Batch Samples (Sorted by Class)\", fontsize=12)\n\n        # Sort the weights for plotting\n        sorted_weights = learned_weights[cost_type][sorted_indices]\n        \n        scatter = axes[1, i].scatter(range(55), sorted_weights, c=sorted_labels, cmap='tab10', alpha=0.8)\n        axes[1, i].set_title(f\"Learned Weight Vector ({cost_type.capitalize()} Cost)\", fontsize=14)\n        axes[1, i].set_xlabel(\"Sample Index (Sorted by Class)\", fontsize=12)\n        axes[1, i].set_ylabel(\"Learned Weight\", fontsize=12)\n        \n        # Create a legend\n        handles, _ = scatter.legend_elements()\n        legend_labels = [f'Class {c}' for c in range(args.num_classes)]\n        axes[1, i].legend(handles, legend_labels, title=\"Classes\")\n\n\n    plt.savefig(\"replicated_figure_1.png\")\n    print(\"--> Plot saved as replicated_figure_1.png\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T10:26:42.277311Z","iopub.execute_input":"2025-06-15T10:26:42.277585Z","iopub.status.idle":"2025-06-15T10:26:42.285615Z","shell.execute_reply.started":"2025-06-15T10:26:42.277546Z","shell.execute_reply":"2025-06-15T10:26:42.284852Z"}},"outputs":[{"name":"stdout","text":"Overwriting replicate_figure_1.py\n","output_type":"stream"}],"execution_count":17}]}