{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create directories for saving model checkpoints and data\n!mkdir -p checkpoint/ours/pretrain/\n!mkdir -p ../cifar-100\n\n# Install compatible dependencies\n!pip install \"numpy<2.0\" -q\n!pip install torch==2.0.0 torchvision==0.15.1 tqdm -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:10:37.322707Z","iopub.execute_input":"2025-06-15T08:10:37.323657Z","iopub.status.idle":"2025-06-15T08:12:42.613149Z","shell.execute_reply.started":"2025-06-15T08:10:37.323634Z","shell.execute_reply":"2025-06-15T08:12:42.612419Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, but you have torch 2.0.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile resnet.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\ndef to_var(x, requires_grad=True):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, requires_grad=requires_grad)\n\nclass resnet_attention(nn.Module):\n    def __init__(self, enc_hid_dim=64, dec_hid_dim=100):\n        super(resnet_attention, self).__init__()\n        self.attn = nn.Linear(enc_hid_dim , dec_hid_dim, bias=True)\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n    def forward(self, s):\n        energy = torch.tanh(self.attn(s))\n        attention = self.v(energy)\n        return  F.softmax(attention, dim=0)\n\nclass MetaModule(nn.Module):\n    def params(self):\n        for name, param in self.named_params(self):\n            yield param\n    def named_leaves(self):\n        return []\n    def named_submodules(self):\n        return []\n    def named_params(self, curr_module=None, memo=None, prefix=''):\n        if memo is None:\n            memo = set()\n        if hasattr(curr_module, 'named_leaves'):\n            for name, p in curr_module.named_leaves():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n        else:\n            for name, p in curr_module._parameters.items():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n        for mname, module in curr_module.named_children():\n            submodule_prefix = prefix + ('.' if prefix else '') + mname\n            for name, p in self.named_params(module, memo, submodule_prefix):\n                yield name, p\n    def update_params(self, lr_inner, first_order=False, source_params=None, detach=False):\n        if source_params is not None:\n            for tgt, src in zip(self.named_params(self), source_params):\n                name_t, param_t = tgt\n                grad = src\n                if first_order:\n                    grad = to_var(grad.detach().data)\n                tmp = param_t - lr_inner * grad\n                self.set_param(self, name_t, tmp)\n        else:\n            for name, param in self.named_params(self):\n                if not detach:\n                    grad = param.grad\n                    if first_order:\n                        grad = to_var(grad.detach().data)\n                    tmp = param - lr_inner * grad\n                    self.set_param(self, name, tmp)\n                else:\n                    param = param.detach_()\n                    self.set_param(self, name, param)\n    def set_param(self, curr_mod, name, param):\n        if '.' in name:\n            n = name.split('.')\n            module_name = n[0]\n            rest = '.'.join(n[1:])\n            for name, mod in curr_mod.named_children():\n                if module_name == name:\n                    self.set_param(mod, rest, param)\n                    break\n        else:\n            setattr(curr_mod, name, param)\n    def detach_params(self):\n        for name, param in self.named_params(self):\n            self.set_param(self, name, param.detach())\n    def copy(self, other, same_var=False):\n        for name, param in other.named_params():\n            if not same_var:\n                param = to_var(param.data.clone(), requires_grad=True)\n            self.set_param(name, param)\n\nclass MetaLinear(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Linear(*args, **kwargs)\n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\nclass MetaConv2d(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Conv2d(*args, **kwargs)\n        self.in_channels = ignore.in_channels\n        self.out_channels = ignore.out_channels\n        self.stride = ignore.stride\n        self.padding = ignore.padding\n        self.dilation = ignore.dilation\n        self.groups = ignore.groups\n        self.kernel_size = ignore.kernel_size\n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        if ignore.bias is not None:\n            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        else:\n            self.register_buffer('bias', None)\n    def forward(self, x):\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\nclass MetaBatchNorm2d(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.BatchNorm2d(*args, **kwargs)\n        self.num_features = ignore.num_features\n        self.eps = ignore.eps\n        self.momentum = ignore.momentum\n        self.affine = ignore.affine\n        self.track_running_stats = ignore.track_running_stats\n        if self.affine:\n            self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(self.num_features))\n            self.register_buffer('running_var', torch.ones(self.num_features))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n    def forward(self, x):\n        return F.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                            self.training or not self.track_running_stats, self.momentum, self.eps)\n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, MetaLinear) or isinstance(m, MetaConv2d):\n        init.kaiming_normal_(m.weight)\n\nclass LambdaLayer(MetaModule):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n    def forward(self, x):\n        return self.lambd(x)\n\nclass BasicBlock(MetaModule):\n    expansion = 1\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = MetaConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = MetaBatchNorm2d(planes)\n        self.conv2 = MetaConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = MetaBatchNorm2d(planes)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     MetaConv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     MetaBatchNorm2d(self.expansion * planes)\n                )\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet32(MetaModule):\n    def __init__(self, num_classes, block=BasicBlock, num_blocks=[5, 5, 5]):\n        super(ResNet32, self).__init__()\n        self.in_planes = 16\n        self.conv1 = MetaConv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = MetaBatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        self.linear = MetaLinear(64, num_classes)\n        self.apply(_weights_init)\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        y = self.linear(out)\n        return out, y\n\nprint(\"File 'resnet.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:42.615243Z","iopub.execute_input":"2025-06-15T08:12:42.615501Z","iopub.status.idle":"2025-06-15T08:12:42.624663Z","shell.execute_reply.started":"2025-06-15T08:12:42.615476Z","shell.execute_reply":"2025-06-15T08:12:42.623846Z"}},"outputs":[{"name":"stdout","text":"Writing resnet.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile data_utils.py\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport copy\nfrom torch.utils.data import Dataset\n\nnp.random.seed(6)\n\ndef build_dataset(dataset,num_meta):\n    normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n    transform_train = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0), (4, 4, 4, 4), mode='reflect').squeeze()),\n        transforms.ToPILImage(),\n        transforms.RandomCrop(32),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])\n    if dataset == 'cifar10':\n        train_dataset = torchvision.datasets.CIFAR10(root='../cifar-10', train=True, download=True, transform=transform_train)\n        test_dataset = torchvision.datasets.CIFAR10('../cifar-10', train=False, transform=transform_test)\n        img_num_list = [num_meta] * 10\n        num_classes = 10\n    if dataset == 'cifar100':\n        train_dataset = torchvision.datasets.CIFAR100(root='../cifar-100', train=True, download=True, transform=transform_train)\n        test_dataset = torchvision.datasets.CIFAR100('../cifar-100', train=False, transform=transform_test)\n        img_num_list = [num_meta] * 100\n        num_classes = 100\n    data_list_val = {}\n    for j in range(num_classes):\n        data_list_val[j] = [i for i, label in enumerate(train_dataset.targets) if label == j]\n    idx_to_meta = []\n    idx_to_train = []\n    for cls_idx, img_id_list in data_list_val.items():\n        np.random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_meta.extend(img_id_list[:img_num])\n        idx_to_train.extend(img_id_list[img_num:])\n    train_data = copy.deepcopy(train_dataset)\n    train_data_meta = copy.deepcopy(train_dataset)\n    train_data_meta.data = np.delete(train_dataset.data, idx_to_train,axis=0)\n    train_data_meta.targets = np.delete(train_dataset.targets, idx_to_train, axis=0)\n    train_data.data = np.delete(train_dataset.data, idx_to_meta, axis=0)\n    train_data.targets = np.delete(train_dataset.targets, idx_to_meta, axis=0)\n    return train_data_meta, train_data, test_dataset\n\ndef get_img_num_per_cls(dataset, imb_factor=None, num_meta=None):\n    if dataset == 'cifar10':\n        img_max = (50000-num_meta)/10\n        cls_num = 10\n    if dataset == 'cifar100':\n        img_max = (50000-num_meta)/100\n        cls_num = 100\n    if imb_factor is None:\n        return [int(img_max)] * cls_num\n    img_num_per_cls = []\n    for cls_idx in range(cls_num):\n        num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n        img_num_per_cls.append(int(num))\n    return img_num_per_cls\n\nclass new_dataset(Dataset):\n    def __init__(self, dataset, train=None):\n        self.data = dataset.data\n        self.targets = dataset.targets\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n        if train:\n            self.transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),(4, 4, 4, 4), mode='reflect').squeeze()),\n                                transforms.ToPILImage(),\n                                transforms.RandomCrop(32),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ToTensor(),\n                                normalize,\n                            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                normalize\n            ])\n    def __getitem__(self, index):\n        img, label = self.data[index, ::], self.targets[index]\n        img = self.transform(img)\n        label = torch.LongTensor([np.int64(label)])\n        return img, label, index\n    def __len__(self):\n        return len(self.data)\n\nprint(\"File 'data_utils.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:42.630140Z","iopub.execute_input":"2025-06-15T08:12:42.630403Z","iopub.status.idle":"2025-06-15T08:12:44.106041Z","shell.execute_reply.started":"2025-06-15T08:12:42.630380Z","shell.execute_reply":"2025-06-15T08:12:44.105264Z"}},"outputs":[{"name":"stdout","text":"Writing data_utils.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile Sinkhorn_distance.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd.variable import Variable\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nd_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)\n\nclass SinkhornDistance(nn.Module):\n    def __init__(self, eps, max_iter, dis, reduction='none'):\n        super(SinkhornDistance, self).__init__()\n        self.eps = eps\n        self.max_iter = max_iter\n        self.reduction = reduction\n        self.dis = dis\n    def forward(self, x, y, nu):\n        if self.dis == 'cos':\n            C = self._cost_matrix(x, y, 'cos')\n        elif self.dis == 'euc':\n            C = self._cost_matrix(x, y, 'euc')\n        x_points = x.shape[-2]\n        if x.dim() == 2:\n            batch_size = 1\n        else:\n            batch_size = x.shape[0]\n        mu = torch.empty(batch_size, x_points, dtype=torch.float, requires_grad=False).fill_(1.0 / x_points).to(device).squeeze()\n        u = torch.zeros_like(mu).to(device)\n        v = torch.zeros_like(nu).to(device)\n        for i in range(self.max_iter):\n            u1 = u\n            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n            err = (u - u1).abs().sum(-1).mean()\n            if err.item() < 1e-1:\n                break\n        U, V = u, v\n        pi = torch.exp(self.M(C, U, V))\n        cost = torch.sum(pi * C, dim=(-2, -1))\n        if self.reduction == 'mean':\n            cost = cost.mean()\n        elif self.reduction == 'sum':\n            cost = cost.sum()\n        return cost\n    def M(self, C, u, v):\n        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n    @staticmethod\n    def _cost_matrix(x, y, dis, p=2):\n        x_col = x.unsqueeze(-2)\n        y_lin = y.unsqueeze(-3)\n        if dis == 'cos':\n            C = 1 - d_cosine(x_col, y_lin)\n        elif dis == 'euc':\n            C = torch.mean((torch.abs(x_col - y_lin)) ** p, -1)\n        return C\nprint(\"File 'Sinkhorn_distance.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:44.107928Z","iopub.execute_input":"2025-06-15T08:12:44.108339Z","iopub.status.idle":"2025-06-15T08:12:44.124746Z","shell.execute_reply.started":"2025-06-15T08:12:44.108287Z","shell.execute_reply":"2025-06-15T08:12:44.124226Z"}},"outputs":[{"name":"stdout","text":"Writing Sinkhorn_distance.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile Sinkhorn_distance_fl.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd.variable import Variable\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nd_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)\n\nclass SinkhornDistance(nn.Module):\n    def __init__(self, eps, max_iter, reduction='none'):\n        super(SinkhornDistance, self).__init__()\n        self.eps = eps\n        self.max_iter = max_iter\n        self.reduction = reduction\n    def forward(self, x, y, x1, y1, nu):\n        C1 = self._cost_matrix(x, y, dis='cos')\n        C2 = self._cost_matrix(x1, y1, dis='euc')\n        C = 0.5*C1 + 0.5*C2\n        x_points = x.shape[-2]\n        if x.dim() == 2:\n            batch_size = 1\n        else:\n            batch_size = x.shape[0]\n        mu = torch.empty(batch_size, x_points, dtype=torch.float, requires_grad=False).fill_(1.0 / x_points).to(device).squeeze()\n        u = torch.zeros_like(mu).to(device)\n        v = torch.zeros_like(nu).to(device)\n        for i in range(self.max_iter):\n            u1 = u\n            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n            err = (u - u1).abs().sum(-1).mean()\n            if err.item() < 1e-1:\n                break\n        U, V = u, v\n        pi = torch.exp(self.M(C, U, V))\n        cost = torch.sum(pi * C, dim=(-2, -1))\n        if self.reduction == 'mean':\n            cost = cost.mean()\n        elif self.reduction == 'sum':\n            cost = cost.sum()\n        return cost\n    def M(self, C, u, v):\n        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n    @staticmethod\n    def _cost_matrix(x, y, dis, p=2):\n        x_col = x.unsqueeze(-2)\n        y_lin = y.unsqueeze(-3)\n        if dis == 'cos':\n            C = 1 - d_cosine(x_col , y_lin)\n        elif dis == 'euc':\n            C = torch.mean((torch.abs(x_col - y_lin)) ** p, -1)\n        return C\nprint(\"File 'Sinkhorn_distance_fl.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:44.125573Z","iopub.execute_input":"2025-06-15T08:12:44.125808Z","iopub.status.idle":"2025-06-15T08:12:44.143430Z","shell.execute_reply.started":"2025-06-15T08:12:44.125784Z","shell.execute_reply":"2025-06-15T08:12:44.142690Z"}},"outputs":[{"name":"stdout","text":"Writing Sinkhorn_distance_fl.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile pretrain_stage1.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport os\nfrom tqdm import tqdm\nimport random\nimport copy\nimport numpy as np\nfrom data_utils import build_dataset, get_img_num_per_cls, new_dataset\nfrom resnet import ResNet32\nfrom torch.utils.data import DataLoader\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Stage 1 Pre-training')\n    parser.add_argument('--dataset', default='cifar10', type=str)\n    parser.add_argument('--imb_factor', default=0.01, type=float)\n    parser.add_argument('--batch_size', type=int, default=128)\n    parser.add_argument('--epochs', type=int, default=200)\n    parser.add_argument('--lr', default=0.1, type=float)\n    parser.add_argument('--gpu', default=0, type=int)\n    # Use parse_known_args to avoid issues in environments like Colab\n    args, _ = parser.parse_known_args()\n    return args\n\ndef main():\n    args = get_args()\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_meta = 10\n    if args.dataset == 'cifar10':\n        num_classes = 10\n    elif args.dataset == 'cifar100':\n        num_classes = 100\n    else:\n        raise ValueError(\"Dataset not supported!\")\n\n    print(\"--> Creating Imbalanced Dataset...\")\n    _, train_data, test_dataset = build_dataset(args.dataset, num_meta)\n    img_num_list = get_img_num_per_cls(args.dataset, args.imb_factor, num_meta * num_classes)\n    data_list = {j: [i for i, label in enumerate(train_data.targets) if label == j] for j in range(num_classes)}\n    idx_to_del = []\n    for cls_idx, img_id_list in data_list.items():\n        random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_del.extend(img_id_list[img_num:])\n    imbalanced_train_dataset = copy.deepcopy(train_data)\n    imbalanced_train_dataset.targets = np.delete(train_data.targets, idx_to_del, axis=0)\n    imbalanced_train_dataset.data = np.delete(train_data.data, idx_to_del, axis=0)\n    imbalanced_train_loader = DataLoader(new_dataset(imbalanced_train_dataset, train=True), batch_size=args.batch_size, shuffle=True, num_workers=2)\n\n    print(\"--> Building and Training Model for Stage 1...\")\n    model = ResNet32(num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.params(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[160, 180], gamma=0.1)\n\n    for epoch in range(args.epochs):\n        model.train()\n        pbar = tqdm(imbalanced_train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')\n        for inputs, labels, _ in pbar:\n            inputs, labels = inputs.to(device), labels.to(device).squeeze()\n            _, outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix({'Loss': f'{loss.item():.3f}'})\n        scheduler.step()\n\n    save_dir = 'checkpoint/ours/pretrain/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    save_path = f'{save_dir}/{args.dataset}_imb{args.imb_factor}_stage1.pth'\n    print(f\"--> Stage 1 training complete. Saving model to {save_path}\")\n    torch.save({'state_dict': model.state_dict()}, save_path)\n\nif __name__ == '__main__':\n    main()\n\nprint(\"File 'pretrain_stage1.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:44.144084Z","iopub.execute_input":"2025-06-15T08:12:44.144743Z","iopub.status.idle":"2025-06-15T08:12:44.185325Z","shell.execute_reply.started":"2025-06-15T08:12:44.144721Z","shell.execute_reply":"2025-06-15T08:12:44.184807Z"}},"outputs":[{"name":"stdout","text":"Writing pretrain_stage1.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile OT_train.py\nimport os\nimport time\nimport argparse\nimport random\nimport copy\nimport torch\nimport torchvision\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom data_utils import *\nfrom resnet import *\nimport shutil\nfrom Sinkhorn_distance import SinkhornDistance\nfrom Sinkhorn_distance_fl import SinkhornDistance as SinkhornDistance_fl\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.backends.cudnn as cudnn\n\nparser = argparse.ArgumentParser(description='Imbalanced Example')\nparser.add_argument('--dataset', default='cifar10', type=str)\nparser.add_argument('--cost', default='combined', type=str)\nparser.add_argument('--meta_set', default='prototype', type=str)\nparser.add_argument('--batch-size', type=int, default=16, metavar='N')\nparser.add_argument('--num_classes', type=int, default=10)\nparser.add_argument('--num_meta', type=int, default=10)\nparser.add_argument('--imb_factor', type=float, default=0.01)\nparser.add_argument('--epochs', type=int, default=200, metavar='N')\nparser.add_argument('--lr', '--learning-rate', default=2e-5, type=float)\nparser.add_argument('--momentum', default=0.9, type=float)\nparser.add_argument('--nesterov', default=True, type=bool)\nparser.add_argument('--weight-decay', '--wd', default=5e-4, type=float)\nparser.add_argument('--no-cuda', action='store_true', default=False)\nparser.add_argument('--seed', type=int, default=42, metavar='S')\nparser.add_argument('--print-freq', '-p', default=100, type=int)\nparser.add_argument('--gpu', default=0, type=int)\nparser.add_argument('--save_name', default='OT_run', type=str)\nparser.add_argument('--idx', default='ours', type=str)\nparser.add_argument('--ckpt_path', type=str, help='Path to pre-trained model checkpoint')\n\n\ndef main():\n    # Use parse_known_args to avoid issues in environments like Colab\n    args, _ = parser.parse_known_args()\n\n    for arg in vars(args):\n        print(f\"{arg}={getattr(args, arg)}\")\n\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n    kwargs = {'num_workers': 0, 'pin_memory': False}\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_data_meta, train_data, test_dataset = build_dataset(args.dataset, args.num_meta)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    np.random.seed(42)\n    random.seed(42)\n    torch.manual_seed(args.seed)\n\n    data_list = {}\n    for j in range(args.num_classes):\n        data_list[j] = [i for i, label in enumerate(train_loader.dataset.targets) if label == j]\n\n    img_num_list = get_img_num_per_cls(args.dataset, args.imb_factor, args.num_meta*args.num_classes)\n\n    idx_to_del = []\n    for cls_idx, img_id_list in data_list.items():\n        random.shuffle(img_id_list)\n        img_num = img_num_list[int(cls_idx)]\n        idx_to_del.extend(img_id_list[img_num:])\n\n    imbalanced_train_dataset = copy.deepcopy(train_data)\n    imbalanced_train_dataset.targets = np.delete(train_loader.dataset.targets, idx_to_del, axis=0)\n    imbalanced_train_dataset.data = np.delete(train_loader.dataset.data, idx_to_del, axis=0)\n\n    imbalanced_train_loader = DataLoader(new_dataset(imbalanced_train_dataset, train=True),\n                                         batch_size=args.batch_size, shuffle=True, **kwargs)\n    validation_loader = DataLoader(new_dataset(train_data_meta, train=True),\n                                   batch_size=args.num_classes*args.num_meta, shuffle=False, **kwargs)\n    test_loader = DataLoader(new_dataset(test_dataset, train=False),\n                             batch_size=args.batch_size, shuffle=False, **kwargs)\n\n    best_prec1 = 0\n\n    beta = 0.9999\n    effective_num = 1.0 - np.power(beta, img_num_list)\n    per_cls_weights = (1.0 - beta) / np.array(effective_num)\n    per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(img_num_list)\n    per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n    weightsbuffer = torch.tensor([per_cls_weights[cls_i] for cls_i in imbalanced_train_dataset.targets]).to(device)\n\n    eplisons = 0.1\n    criterion = SinkhornDistance(eps=eplisons, max_iter=200, dis='cos').to(device)\n    criterion_label = SinkhornDistance(eps=eplisons, max_iter=200, dis='euc').to(device)\n    criterion_fl = SinkhornDistance_fl(eps=eplisons, max_iter=200).to(device)\n\n    model = build_model(load_pretrain=True, ckpt_path=args.ckpt_path, num_classes=args.num_classes)\n    if not model:\n        print(\"Exiting: Failed to build model.\")\n        return\n\n    optimizer_a = torch.optim.SGD(model.linear.params(), args.lr,\n                                  momentum=args.momentum, nesterov=args.nesterov,\n                                  weight_decay=args.weight_decay)\n\n    cudnn.benchmark = True\n    criterion_classifier = nn.CrossEntropyLoss(reduction='none').to(device)\n\n    for epoch in range(160, args.epochs):\n\n        train_OT(imbalanced_train_loader, validation_loader, weightsbuffer,\n                 model, optimizer_a, epoch, criterion_classifier, device, args,\n                 criterion, criterion_label, criterion_fl)\n\n        prec1, _, _ = validate(test_loader, model, device)\n\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n\n        if is_best:\n            save_checkpoint(args, {\n                'epoch': epoch + 1,\n                'state_dict': model.state_dict(),\n                'best_acc1': best_prec1,\n                'optimizer': optimizer_a.state_dict(),\n            }, is_best)\n\n    print('Best accuracy: ', best_prec1)\n\n\ndef train_OT(train_loader, validation_loader, weightsbuffer, model, optimizer, epoch, criterion_classifier, device, args, criterion, criterion_label, criterion_fl):\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    model.train()\n\n    val_data, val_labels, _ = next(iter(validation_loader))\n    val_data = to_var(val_data.to(device), requires_grad=False)\n    val_labels = to_var(val_labels.to(device), requires_grad=False).squeeze()\n\n    if args.meta_set == 'prototype':\n        val_data_bycls = torch.zeros([args.num_classes, 3, 32, 32]).to(device)\n        for i_cls in range(args.num_classes):\n            class_samples = val_data[val_labels == i_cls]\n            if len(class_samples) > 0:\n                 val_data_bycls[i_cls, ::] = class_samples.mean(dim=0)\n        val_labels_bycls = torch.tensor([i_l for i_l in range(args.num_classes)]).to(device)\n    else: # 'whole'\n        val_data_bycls = val_data\n        val_labels_bycls = val_labels\n\n    val_labels_onehot = to_categorical(val_labels_bycls, num_classes=args.num_classes).to(device)\n    with torch.no_grad():\n        feature_val, _ = model(val_data_bycls)\n\n    for i, batch in enumerate(train_loader):\n        inputs, labels, ids = tuple(t.to(device) for t in batch)\n        labels = labels.squeeze()\n        labels_onehot = to_categorical(labels, num_classes=args.num_classes).to(device)\n        weights = to_var(weightsbuffer[ids])\n        model.eval()\n        Attoptimizer = torch.optim.SGD([weights], lr=0.01, momentum=0.9, weight_decay=5e-4)\n        with torch.no_grad():\n            feature_train, _ = model(inputs)\n        probability_train = softmax_normalize(weights)\n\n        if args.cost == 'feature':\n            OTloss = criterion(feature_val.detach(), feature_train.detach(), probability_train.squeeze())\n        elif args.cost == 'label':\n            OTloss = criterion_label(val_labels_onehot.float(),\n                                     labels_onehot.float(),\n                                     probability_train.squeeze())\n        elif args.cost == 'combined':\n            OTloss = criterion_fl(feature_val.detach(), feature_train.detach(),\n                                  val_labels_onehot.float(),\n                                  labels_onehot.float(),\n                                  probability_train.squeeze())\n\n        Attoptimizer.zero_grad()\n        OTloss.backward()\n        Attoptimizer.step()\n        weightsbuffer[ids] = weights.data\n\n        model.train()\n        optimizer.zero_grad()\n        _, logits = model(inputs)\n        loss_train = criterion_classifier(logits, labels.long())\n\n        loss = torch.sum(loss_train * weights.detach())\n        loss.backward()\n        optimizer.step()\n\n        prec_train = accuracy(logits.data, labels, topk=(1,))[0]\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec_train.item(), inputs.size(0))\n        if i % args.print_freq == 0 or i == len(train_loader) -1:\n            print(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\\\t'\n                  f'Loss {losses.val:.4f} ({losses.avg:.4f})\\\\t'\n                  f'Prec@1 {top1.val:.3f} ({top1.avg:.3f})')\n\n\ndef validate(val_loader, model, device):\n    top1 = AverageMeter()\n    model.eval()\n    with torch.no_grad():\n      for i, batch in enumerate(val_loader):\n          input, target, _ = tuple(t.to(device) for t in batch)\n          target = target.squeeze().to(device)\n\n          _, output = model(input)\n          prec1 = accuracy(output.data, target, topk=(1,))[0]\n          top1.update(prec1.item(), input.size(0))\n    print(f' * Prec@1 {top1.avg:.3f}')\n    return top1.avg, None, None\n\n\ndef build_model(load_pretrain, ckpt_path=None, num_classes=10):\n    model = ResNet32(num_classes)\n    if load_pretrain:\n        if not ckpt_path or not os.path.exists(ckpt_path):\n            print(f\"ERROR: Checkpoint file not found at {ckpt_path}\")\n            return None\n        checkpoint = torch.load(ckpt_path)\n        model.load_state_dict(checkpoint['state_dict'])\n    if torch.cuda.is_available():\n        model.cuda()\n        torch.backends.cudnn.benchmark = True\n    return model\n\ndef softmax_normalize(weights, temperature=1.):\n    return F.softmax(weights / temperature, dim=0)\n\nclass AverageMeter(object):\n    def __init__(self): self.reset()\n    def reset(self): self.val = 0; self.avg = 0; self.sum = 0; self.count = 0\n    def update(self, val, n=1):\n        self.val = val; self.sum += val * n; self.count += n; self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef save_checkpoint(args, state, is_best):\n    path = 'checkpoint/ours/'\n    save_name = args.save_name\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filename = path + save_name + '_ckpt.pth.tar'\n    if is_best:\n        torch.save(state, filename)\n\ndef to_categorical(labels, num_classes=10):\n    return F.one_hot(labels.long(), num_classes=num_classes)\n\nif __name__ == '__main__':\n    main()\nprint(\"File 'OT_train.py' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:44.186170Z","iopub.execute_input":"2025-06-15T08:12:44.186430Z","iopub.status.idle":"2025-06-15T08:12:44.207241Z","shell.execute_reply.started":"2025-06-15T08:12:44.186408Z","shell.execute_reply":"2025-06-15T08:12:44.206763Z"}},"outputs":[{"name":"stdout","text":"Writing OT_train.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Stage 1 (IF=200)\n!python pretrain_stage1.py --dataset cifar100 --imb_factor 0.005 --epochs 200\n\n# Stage 2 (IF=200)\n!python OT_train.py \\\n--dataset cifar100 \\\n--num_classes 100 \\\n--imb_factor 0.005 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar100_IF200_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar100_imb0.005_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:12:44.207904Z","iopub.execute_input":"2025-06-15T08:12:44.208141Z","iopub.status.idle":"2025-06-15T08:37:31.184039Z","shell.execute_reply.started":"2025-06-15T08:12:44.208117Z","shell.execute_reply":"2025-06-15T08:37:31.183196Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nDownloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../cifar-100/cifar-100-python.tar.gz\n100%|███████████████████████| 169001437/169001437 [00:04<00:00, 35182285.65it/s]\nExtracting ../cifar-100/cifar-100-python.tar.gz to ../cifar-100\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|██████████████████| 73/73 [00:04<00:00, 15.05it/s, Loss=3.706]\nEpoch 2/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.09it/s, Loss=3.320]\nEpoch 3/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.07it/s, Loss=3.218]\nEpoch 4/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.02it/s, Loss=3.290]\nEpoch 5/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.03it/s, Loss=3.098]\nEpoch 6/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.18it/s, Loss=2.813]\nEpoch 7/200: 100%|██████████████████| 73/73 [00:03<00:00, 21.18it/s, Loss=2.874]\nEpoch 8/200: 100%|██████████████████| 73/73 [00:03<00:00, 20.76it/s, Loss=2.807]\nEpoch 9/200: 100%|██████████████████| 73/73 [00:03<00:00, 20.80it/s, Loss=2.824]\nEpoch 10/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.99it/s, Loss=2.563]\nEpoch 11/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.92it/s, Loss=2.290]\nEpoch 12/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.98it/s, Loss=2.386]\nEpoch 13/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.91it/s, Loss=1.900]\nEpoch 14/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.76it/s, Loss=2.089]\nEpoch 15/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.88it/s, Loss=1.830]\nEpoch 16/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.77it/s, Loss=2.561]\nEpoch 17/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.81it/s, Loss=1.820]\nEpoch 18/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.54it/s, Loss=2.261]\nEpoch 19/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.64it/s, Loss=2.003]\nEpoch 20/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.45it/s, Loss=2.165]\nEpoch 21/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.66it/s, Loss=1.824]\nEpoch 22/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.51it/s, Loss=1.731]\nEpoch 23/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.58it/s, Loss=1.776]\nEpoch 24/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.49it/s, Loss=1.640]\nEpoch 25/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.35it/s, Loss=1.737]\nEpoch 26/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.47it/s, Loss=1.360]\nEpoch 27/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.19it/s, Loss=1.241]\nEpoch 28/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.01it/s, Loss=1.895]\nEpoch 29/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.19it/s, Loss=1.591]\nEpoch 30/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.21it/s, Loss=1.583]\nEpoch 31/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.02it/s, Loss=1.274]\nEpoch 32/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.14it/s, Loss=1.107]\nEpoch 33/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.95it/s, Loss=1.365]\nEpoch 34/200: 100%|█████████████████| 73/73 [00:03<00:00, 20.02it/s, Loss=1.374]\nEpoch 35/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.95it/s, Loss=1.116]\nEpoch 36/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.38it/s, Loss=1.511]\nEpoch 37/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.83it/s, Loss=1.151]\nEpoch 38/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.80it/s, Loss=1.362]\nEpoch 39/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=1.035]\nEpoch 40/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.915]\nEpoch 41/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.43it/s, Loss=1.470]\nEpoch 42/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.32it/s, Loss=1.208]\nEpoch 43/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.53it/s, Loss=1.253]\nEpoch 44/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.45it/s, Loss=1.344]\nEpoch 45/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.38it/s, Loss=1.160]\nEpoch 46/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.46it/s, Loss=1.316]\nEpoch 47/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=1.061]\nEpoch 48/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.74it/s, Loss=1.168]\nEpoch 49/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=1.076]\nEpoch 50/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=1.212]\nEpoch 51/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.82it/s, Loss=1.169]\nEpoch 52/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.78it/s, Loss=1.134]\nEpoch 53/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=1.229]\nEpoch 54/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.922]\nEpoch 55/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.77it/s, Loss=1.056]\nEpoch 56/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.87it/s, Loss=1.071]\nEpoch 57/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.88it/s, Loss=1.038]\nEpoch 58/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=1.057]\nEpoch 59/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=0.927]\nEpoch 60/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=1.234]\nEpoch 61/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.80it/s, Loss=1.038]\nEpoch 62/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.68it/s, Loss=0.992]\nEpoch 63/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.75it/s, Loss=0.891]\nEpoch 64/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=1.153]\nEpoch 65/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=0.832]\nEpoch 66/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=1.176]\nEpoch 67/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.862]\nEpoch 68/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=1.081]\nEpoch 69/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.70it/s, Loss=1.180]\nEpoch 70/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=1.220]\nEpoch 71/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.50it/s, Loss=1.117]\nEpoch 72/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=1.265]\nEpoch 73/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.864]\nEpoch 74/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.832]\nEpoch 75/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.815]\nEpoch 76/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.59it/s, Loss=0.845]\nEpoch 77/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.673]\nEpoch 78/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.72it/s, Loss=1.112]\nEpoch 79/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.50it/s, Loss=1.008]\nEpoch 80/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.52it/s, Loss=0.828]\nEpoch 81/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.72it/s, Loss=0.898]\nEpoch 82/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.698]\nEpoch 83/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.915]\nEpoch 84/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.875]\nEpoch 85/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.945]\nEpoch 86/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.746]\nEpoch 87/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.64it/s, Loss=0.772]\nEpoch 88/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.954]\nEpoch 89/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=0.826]\nEpoch 90/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.74it/s, Loss=0.757]\nEpoch 91/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.72it/s, Loss=0.908]\nEpoch 92/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.832]\nEpoch 93/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=0.935]\nEpoch 94/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.73it/s, Loss=1.104]\nEpoch 95/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.767]\nEpoch 96/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=0.722]\nEpoch 97/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.45it/s, Loss=0.593]\nEpoch 98/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.574]\nEpoch 99/200: 100%|█████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.712]\nEpoch 100/200: 100%|████████████████| 73/73 [00:03<00:00, 19.75it/s, Loss=1.003]\nEpoch 101/200: 100%|████████████████| 73/73 [00:03<00:00, 19.56it/s, Loss=0.809]\nEpoch 102/200: 100%|████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=0.893]\nEpoch 103/200: 100%|████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.808]\nEpoch 104/200: 100%|████████████████| 73/73 [00:03<00:00, 19.70it/s, Loss=0.838]\nEpoch 105/200: 100%|████████████████| 73/73 [00:03<00:00, 19.58it/s, Loss=0.935]\nEpoch 106/200: 100%|████████████████| 73/73 [00:03<00:00, 19.49it/s, Loss=0.699]\nEpoch 107/200: 100%|████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.873]\nEpoch 108/200: 100%|████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.915]\nEpoch 109/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.953]\nEpoch 110/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.702]\nEpoch 111/200: 100%|████████████████| 73/73 [00:03<00:00, 19.68it/s, Loss=0.787]\nEpoch 112/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.918]\nEpoch 113/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.727]\nEpoch 114/200: 100%|████████████████| 73/73 [00:03<00:00, 19.41it/s, Loss=0.748]\nEpoch 115/200: 100%|████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=1.025]\nEpoch 116/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.666]\nEpoch 117/200: 100%|████████████████| 73/73 [00:03<00:00, 19.56it/s, Loss=0.753]\nEpoch 118/200: 100%|████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.782]\nEpoch 119/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=1.277]\nEpoch 120/200: 100%|████████████████| 73/73 [00:03<00:00, 19.43it/s, Loss=0.838]\nEpoch 121/200: 100%|████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=0.706]\nEpoch 122/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.758]\nEpoch 123/200: 100%|████████████████| 73/73 [00:03<00:00, 19.23it/s, Loss=0.818]\nEpoch 124/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.678]\nEpoch 125/200: 100%|████████████████| 73/73 [00:03<00:00, 19.47it/s, Loss=0.676]\nEpoch 126/200: 100%|████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=1.061]\nEpoch 127/200: 100%|████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.690]\nEpoch 128/200: 100%|████████████████| 73/73 [00:03<00:00, 19.35it/s, Loss=0.730]\nEpoch 129/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.766]\nEpoch 130/200: 100%|████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.516]\nEpoch 131/200: 100%|████████████████| 73/73 [00:03<00:00, 19.46it/s, Loss=0.582]\nEpoch 132/200: 100%|████████████████| 73/73 [00:03<00:00, 19.61it/s, Loss=0.825]\nEpoch 133/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.805]\nEpoch 134/200: 100%|████████████████| 73/73 [00:03<00:00, 19.61it/s, Loss=0.666]\nEpoch 135/200: 100%|████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.759]\nEpoch 136/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.848]\nEpoch 137/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.840]\nEpoch 138/200: 100%|████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.601]\nEpoch 139/200: 100%|████████████████| 73/73 [00:03<00:00, 19.61it/s, Loss=0.582]\nEpoch 140/200: 100%|████████████████| 73/73 [00:03<00:00, 19.46it/s, Loss=0.741]\nEpoch 141/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.605]\nEpoch 142/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.642]\nEpoch 143/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.718]\nEpoch 144/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.655]\nEpoch 145/200: 100%|████████████████| 73/73 [00:03<00:00, 19.59it/s, Loss=0.556]\nEpoch 146/200: 100%|████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.747]\nEpoch 147/200: 100%|████████████████| 73/73 [00:03<00:00, 19.64it/s, Loss=0.911]\nEpoch 148/200: 100%|████████████████| 73/73 [00:03<00:00, 19.42it/s, Loss=0.670]\nEpoch 149/200: 100%|████████████████| 73/73 [00:03<00:00, 19.44it/s, Loss=0.889]\nEpoch 150/200: 100%|████████████████| 73/73 [00:03<00:00, 19.64it/s, Loss=0.526]\nEpoch 151/200: 100%|████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.730]\nEpoch 152/200: 100%|████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.640]\nEpoch 153/200: 100%|████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.834]\nEpoch 154/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.705]\nEpoch 155/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.960]\nEpoch 156/200: 100%|████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=0.923]\nEpoch 157/200: 100%|████████████████| 73/73 [00:03<00:00, 19.44it/s, Loss=0.691]\nEpoch 158/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.559]\nEpoch 159/200: 100%|████████████████| 73/73 [00:03<00:00, 19.68it/s, Loss=1.211]\nEpoch 160/200: 100%|████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.750]\nEpoch 161/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.219]\nEpoch 162/200: 100%|████████████████| 73/73 [00:03<00:00, 19.71it/s, Loss=0.336]\nEpoch 163/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.095]\nEpoch 164/200: 100%|████████████████| 73/73 [00:03<00:00, 19.67it/s, Loss=0.243]\nEpoch 165/200: 100%|████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.122]\nEpoch 166/200: 100%|████████████████| 73/73 [00:03<00:00, 19.43it/s, Loss=0.189]\nEpoch 167/200: 100%|████████████████| 73/73 [00:03<00:00, 19.61it/s, Loss=0.194]\nEpoch 168/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.125]\nEpoch 169/200: 100%|████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.153]\nEpoch 170/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.079]\nEpoch 171/200: 100%|████████████████| 73/73 [00:03<00:00, 19.61it/s, Loss=0.171]\nEpoch 172/200: 100%|████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.140]\nEpoch 173/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.111]\nEpoch 174/200: 100%|████████████████| 73/73 [00:03<00:00, 19.56it/s, Loss=0.214]\nEpoch 175/200: 100%|████████████████| 73/73 [00:03<00:00, 19.56it/s, Loss=0.137]\nEpoch 176/200: 100%|████████████████| 73/73 [00:03<00:00, 19.49it/s, Loss=0.085]\nEpoch 177/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.086]\nEpoch 178/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.075]\nEpoch 179/200: 100%|████████████████| 73/73 [00:03<00:00, 19.59it/s, Loss=0.062]\nEpoch 180/200: 100%|████████████████| 73/73 [00:03<00:00, 19.65it/s, Loss=0.080]\nEpoch 181/200: 100%|████████████████| 73/73 [00:03<00:00, 19.64it/s, Loss=0.100]\nEpoch 182/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.056]\nEpoch 183/200: 100%|████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.052]\nEpoch 184/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.119]\nEpoch 185/200: 100%|████████████████| 73/73 [00:03<00:00, 19.69it/s, Loss=0.072]\nEpoch 186/200: 100%|████████████████| 73/73 [00:03<00:00, 19.56it/s, Loss=0.092]\nEpoch 187/200: 100%|████████████████| 73/73 [00:03<00:00, 19.47it/s, Loss=0.085]\nEpoch 188/200: 100%|████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.070]\nEpoch 189/200: 100%|████████████████| 73/73 [00:03<00:00, 19.63it/s, Loss=0.138]\nEpoch 190/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.052]\nEpoch 191/200: 100%|████████████████| 73/73 [00:03<00:00, 19.55it/s, Loss=0.059]\nEpoch 192/200: 100%|████████████████| 73/73 [00:03<00:00, 19.54it/s, Loss=0.142]\nEpoch 193/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.096]\nEpoch 194/200: 100%|████████████████| 73/73 [00:03<00:00, 19.66it/s, Loss=0.048]\nEpoch 195/200: 100%|████████████████| 73/73 [00:03<00:00, 19.53it/s, Loss=0.060]\nEpoch 196/200: 100%|████████████████| 73/73 [00:03<00:00, 19.62it/s, Loss=0.076]\nEpoch 197/200: 100%|████████████████| 73/73 [00:03<00:00, 19.57it/s, Loss=0.052]\nEpoch 198/200: 100%|████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.116]\nEpoch 199/200: 100%|████████████████| 73/73 [00:03<00:00, 19.60it/s, Loss=0.080]\nEpoch 200/200: 100%|████████████████| 73/73 [00:03<00:00, 19.44it/s, Loss=0.054]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar100_imb0.005_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\nFile 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar100\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=100\nnum_meta=10\nimb_factor=0.005\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar100_IF200_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar100_imb0.005_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/582]\\tLoss 5.9945 (5.9945)\\tPrec@1 56.250 (56.250)\nEpoch: [160][100/582]\\tLoss 4.9199 (5.3620)\\tPrec@1 56.250 (72.710)\nEpoch: [160][200/582]\\tLoss 4.3850 (5.9057)\\tPrec@1 75.000 (73.539)\nEpoch: [160][300/582]\\tLoss 2.0160 (5.6548)\\tPrec@1 75.000 (73.879)\nEpoch: [160][400/582]\\tLoss 6.0517 (6.0269)\\tPrec@1 62.500 (73.800)\nEpoch: [160][500/582]\\tLoss 4.7504 (5.9630)\\tPrec@1 93.750 (74.102)\nEpoch: [160][581/582]\\tLoss 4.1792 (5.9871)\\tPrec@1 50.000 (74.226)\n * Prec@1 38.880\nEpoch: [161][0/582]\\tLoss 5.9165 (5.9165)\\tPrec@1 43.750 (43.750)\nEpoch: [161][100/582]\\tLoss 1.5958 (5.1014)\\tPrec@1 68.750 (75.681)\nEpoch: [161][200/582]\\tLoss 0.6976 (5.9472)\\tPrec@1 87.500 (74.751)\nEpoch: [161][300/582]\\tLoss 3.4491 (5.7629)\\tPrec@1 81.250 (74.522)\nEpoch: [161][400/582]\\tLoss 11.0529 (5.6319)\\tPrec@1 68.750 (74.610)\nEpoch: [161][500/582]\\tLoss 0.8920 (5.6090)\\tPrec@1 81.250 (74.513)\nEpoch: [161][581/582]\\tLoss 4.6185 (5.7185)\\tPrec@1 62.500 (74.570)\n * Prec@1 39.110\nEpoch: [162][0/582]\\tLoss 2.7618 (2.7618)\\tPrec@1 68.750 (68.750)\nEpoch: [162][100/582]\\tLoss 8.9149 (6.7838)\\tPrec@1 81.250 (75.248)\nEpoch: [162][200/582]\\tLoss 2.2509 (6.6803)\\tPrec@1 62.500 (74.534)\nEpoch: [162][300/582]\\tLoss 20.2521 (6.4146)\\tPrec@1 81.250 (73.713)\nEpoch: [162][400/582]\\tLoss 0.9147 (5.8986)\\tPrec@1 75.000 (74.049)\nEpoch: [162][500/582]\\tLoss 1.7970 (5.7067)\\tPrec@1 75.000 (73.790)\nEpoch: [162][581/582]\\tLoss 0.1144 (5.5896)\\tPrec@1 100.000 (73.753)\n * Prec@1 39.850\nEpoch: [163][0/582]\\tLoss 5.1731 (5.1731)\\tPrec@1 62.500 (62.500)\nEpoch: [163][100/582]\\tLoss 0.9024 (5.8576)\\tPrec@1 81.250 (76.114)\nEpoch: [163][200/582]\\tLoss 1.3263 (5.6812)\\tPrec@1 87.500 (74.938)\nEpoch: [163][300/582]\\tLoss 3.9395 (5.3038)\\tPrec@1 81.250 (74.564)\nEpoch: [163][400/582]\\tLoss 1.6736 (5.4398)\\tPrec@1 68.750 (74.034)\nEpoch: [163][500/582]\\tLoss 7.9906 (5.2170)\\tPrec@1 75.000 (74.613)\nEpoch: [163][581/582]\\tLoss 0.9698 (5.2471)\\tPrec@1 62.500 (74.387)\n * Prec@1 40.100\nEpoch: [164][0/582]\\tLoss 0.7430 (0.7430)\\tPrec@1 75.000 (75.000)\nEpoch: [164][100/582]\\tLoss 10.1661 (4.1637)\\tPrec@1 75.000 (74.505)\nEpoch: [164][200/582]\\tLoss 23.7249 (5.3067)\\tPrec@1 56.250 (73.476)\nEpoch: [164][300/582]\\tLoss 0.9529 (5.3866)\\tPrec@1 87.500 (73.505)\nEpoch: [164][400/582]\\tLoss 0.6300 (5.3948)\\tPrec@1 81.250 (73.457)\nEpoch: [164][500/582]\\tLoss 3.6502 (5.3045)\\tPrec@1 81.250 (73.503)\nEpoch: [164][581/582]\\tLoss 0.2783 (5.2063)\\tPrec@1 75.000 (73.302)\n * Prec@1 40.610\nEpoch: [165][0/582]\\tLoss 3.8206 (3.8206)\\tPrec@1 62.500 (62.500)\nEpoch: [165][100/582]\\tLoss 4.4956 (5.4724)\\tPrec@1 75.000 (71.535)\nEpoch: [165][200/582]\\tLoss 2.9622 (4.4232)\\tPrec@1 81.250 (72.792)\nEpoch: [165][300/582]\\tLoss 3.2143 (4.6665)\\tPrec@1 75.000 (72.633)\nEpoch: [165][400/582]\\tLoss 12.5795 (4.8296)\\tPrec@1 81.250 (72.304)\nEpoch: [165][500/582]\\tLoss 3.1280 (5.0193)\\tPrec@1 37.500 (72.567)\nEpoch: [165][581/582]\\tLoss 2.2551 (4.9588)\\tPrec@1 37.500 (72.743)\n * Prec@1 40.990\nEpoch: [166][0/582]\\tLoss 5.2881 (5.2881)\\tPrec@1 68.750 (68.750)\nEpoch: [166][100/582]\\tLoss 0.8099 (4.7481)\\tPrec@1 81.250 (73.144)\nEpoch: [166][200/582]\\tLoss 0.4104 (4.3345)\\tPrec@1 81.250 (72.233)\nEpoch: [166][300/582]\\tLoss 11.2032 (4.7875)\\tPrec@1 68.750 (72.384)\nEpoch: [166][400/582]\\tLoss 7.2525 (5.0374)\\tPrec@1 43.750 (72.382)\nEpoch: [166][500/582]\\tLoss 2.5279 (5.0628)\\tPrec@1 62.500 (72.343)\nEpoch: [166][581/582]\\tLoss 0.8771 (5.0306)\\tPrec@1 87.500 (72.152)\n * Prec@1 41.190\nEpoch: [167][0/582]\\tLoss 15.1527 (15.1527)\\tPrec@1 56.250 (56.250)\nEpoch: [167][100/582]\\tLoss 7.8254 (5.3122)\\tPrec@1 75.000 (72.153)\nEpoch: [167][200/582]\\tLoss 1.3762 (4.9444)\\tPrec@1 81.250 (71.580)\nEpoch: [167][300/582]\\tLoss 1.6069 (5.0195)\\tPrec@1 81.250 (71.096)\nEpoch: [167][400/582]\\tLoss 2.6678 (4.8873)\\tPrec@1 81.250 (71.368)\nEpoch: [167][500/582]\\tLoss 5.5030 (4.8116)\\tPrec@1 56.250 (71.557)\nEpoch: [167][581/582]\\tLoss 0.1537 (4.8075)\\tPrec@1 87.500 (71.507)\n * Prec@1 41.160\nEpoch: [168][0/582]\\tLoss 3.4976 (3.4976)\\tPrec@1 81.250 (81.250)\nEpoch: [168][100/582]\\tLoss 1.5643 (5.0867)\\tPrec@1 75.000 (72.153)\nEpoch: [168][200/582]\\tLoss 1.9459 (5.1383)\\tPrec@1 81.250 (71.611)\nEpoch: [168][300/582]\\tLoss 0.7628 (4.7371)\\tPrec@1 62.500 (72.384)\nEpoch: [168][400/582]\\tLoss 2.4004 (4.5901)\\tPrec@1 68.750 (72.724)\nEpoch: [168][500/582]\\tLoss 1.2155 (4.6247)\\tPrec@1 68.750 (72.343)\nEpoch: [168][581/582]\\tLoss 0.1850 (4.7581)\\tPrec@1 87.500 (72.034)\n * Prec@1 41.440\nEpoch: [169][0/582]\\tLoss 3.5310 (3.5310)\\tPrec@1 62.500 (62.500)\nEpoch: [169][100/582]\\tLoss 5.1786 (5.2142)\\tPrec@1 75.000 (70.792)\nEpoch: [169][200/582]\\tLoss 2.9295 (5.2227)\\tPrec@1 81.250 (69.652)\nEpoch: [169][300/582]\\tLoss 2.5507 (4.9634)\\tPrec@1 62.500 (70.245)\nEpoch: [169][400/582]\\tLoss 17.0976 (4.8798)\\tPrec@1 75.000 (70.651)\nEpoch: [169][500/582]\\tLoss 1.2563 (4.7944)\\tPrec@1 62.500 (70.696)\nEpoch: [169][581/582]\\tLoss 0.9722 (4.7534)\\tPrec@1 62.500 (70.647)\n * Prec@1 41.640\nEpoch: [170][0/582]\\tLoss 6.9336 (6.9336)\\tPrec@1 62.500 (62.500)\nEpoch: [170][100/582]\\tLoss 10.9089 (4.8290)\\tPrec@1 43.750 (71.597)\nEpoch: [170][200/582]\\tLoss 1.6034 (4.6469)\\tPrec@1 81.250 (71.331)\nEpoch: [170][300/582]\\tLoss 3.5159 (4.5646)\\tPrec@1 56.250 (71.200)\nEpoch: [170][400/582]\\tLoss 2.6765 (4.4499)\\tPrec@1 50.000 (70.761)\nEpoch: [170][500/582]\\tLoss 2.6080 (4.5736)\\tPrec@1 62.500 (70.908)\nEpoch: [170][581/582]\\tLoss 8.0718 (4.5684)\\tPrec@1 50.000 (70.647)\n * Prec@1 42.070\nEpoch: [171][0/582]\\tLoss 2.0809 (2.0809)\\tPrec@1 68.750 (68.750)\nEpoch: [171][100/582]\\tLoss 5.6964 (4.2803)\\tPrec@1 75.000 (68.936)\nEpoch: [171][200/582]\\tLoss 3.5337 (4.4963)\\tPrec@1 68.750 (69.932)\nEpoch: [171][300/582]\\tLoss 12.4942 (4.6687)\\tPrec@1 62.500 (70.287)\nEpoch: [171][400/582]\\tLoss 0.6082 (4.4241)\\tPrec@1 81.250 (70.574)\nEpoch: [171][500/582]\\tLoss 2.0644 (4.4893)\\tPrec@1 68.750 (70.646)\nEpoch: [171][581/582]\\tLoss 1.3590 (4.6437)\\tPrec@1 62.500 (70.540)\n * Prec@1 42.240\nEpoch: [172][0/582]\\tLoss 3.3800 (3.3800)\\tPrec@1 75.000 (75.000)\nEpoch: [172][100/582]\\tLoss 34.6659 (4.9081)\\tPrec@1 68.750 (70.173)\nEpoch: [172][200/582]\\tLoss 1.6069 (4.6773)\\tPrec@1 62.500 (69.869)\nEpoch: [172][300/582]\\tLoss 1.7947 (4.5603)\\tPrec@1 75.000 (69.539)\nEpoch: [172][400/582]\\tLoss 1.1410 (4.4216)\\tPrec@1 93.750 (69.592)\nEpoch: [172][500/582]\\tLoss 8.1663 (4.5260)\\tPrec@1 56.250 (69.673)\nEpoch: [172][581/582]\\tLoss 3.4598 (4.5254)\\tPrec@1 25.000 (69.486)\n * Prec@1 42.070\nEpoch: [173][0/582]\\tLoss 4.1473 (4.1473)\\tPrec@1 62.500 (62.500)\nEpoch: [173][100/582]\\tLoss 2.5871 (3.8058)\\tPrec@1 50.000 (69.307)\nEpoch: [173][200/582]\\tLoss 17.3179 (4.5595)\\tPrec@1 56.250 (69.621)\nEpoch: [173][300/582]\\tLoss 1.6453 (4.8331)\\tPrec@1 62.500 (69.228)\nEpoch: [173][400/582]\\tLoss 3.1470 (4.9165)\\tPrec@1 62.500 (69.311)\nEpoch: [173][500/582]\\tLoss 3.2770 (4.7129)\\tPrec@1 68.750 (69.261)\nEpoch: [173][581/582]\\tLoss 0.3968 (4.5958)\\tPrec@1 87.500 (69.400)\n * Prec@1 42.420\nEpoch: [174][0/582]\\tLoss 3.9607 (3.9607)\\tPrec@1 62.500 (62.500)\nEpoch: [174][100/582]\\tLoss 2.4746 (5.3298)\\tPrec@1 68.750 (67.389)\nEpoch: [174][200/582]\\tLoss 4.0694 (4.5727)\\tPrec@1 75.000 (69.527)\nEpoch: [174][300/582]\\tLoss 0.7615 (4.6316)\\tPrec@1 81.250 (69.352)\nEpoch: [174][400/582]\\tLoss 2.2415 (4.5640)\\tPrec@1 75.000 (69.031)\nEpoch: [174][500/582]\\tLoss 5.4175 (4.4613)\\tPrec@1 68.750 (69.162)\nEpoch: [174][581/582]\\tLoss 19.4941 (4.4722)\\tPrec@1 25.000 (68.949)\n * Prec@1 42.400\nEpoch: [175][0/582]\\tLoss 1.6855 (1.6855)\\tPrec@1 81.250 (81.250)\nEpoch: [175][100/582]\\tLoss 5.4483 (4.3793)\\tPrec@1 75.000 (70.359)\nEpoch: [175][200/582]\\tLoss 6.1177 (4.5014)\\tPrec@1 75.000 (70.336)\nEpoch: [175][300/582]\\tLoss 2.8438 (4.4481)\\tPrec@1 50.000 (69.726)\nEpoch: [175][400/582]\\tLoss 29.5399 (4.6131)\\tPrec@1 62.500 (69.389)\nEpoch: [175][500/582]\\tLoss 1.0110 (4.4994)\\tPrec@1 81.250 (69.760)\nEpoch: [175][581/582]\\tLoss 1.3783 (4.4205)\\tPrec@1 62.500 (69.744)\n * Prec@1 42.390\nEpoch: [176][0/582]\\tLoss 3.8679 (3.8679)\\tPrec@1 62.500 (62.500)\nEpoch: [176][100/582]\\tLoss 3.2497 (4.1834)\\tPrec@1 81.250 (68.007)\nEpoch: [176][200/582]\\tLoss 8.1913 (4.2693)\\tPrec@1 62.500 (68.563)\nEpoch: [176][300/582]\\tLoss 1.1672 (4.6102)\\tPrec@1 75.000 (68.688)\nEpoch: [176][400/582]\\tLoss 4.3401 (4.4833)\\tPrec@1 68.750 (68.547)\nEpoch: [176][500/582]\\tLoss 6.9399 (4.4153)\\tPrec@1 56.250 (68.775)\nEpoch: [176][581/582]\\tLoss 1.0054 (4.4202)\\tPrec@1 62.500 (68.594)\n * Prec@1 42.220\nEpoch: [177][0/582]\\tLoss 0.5794 (0.5794)\\tPrec@1 81.250 (81.250)\nEpoch: [177][100/582]\\tLoss 2.0712 (4.3494)\\tPrec@1 68.750 (68.750)\nEpoch: [177][200/582]\\tLoss 1.3846 (4.3764)\\tPrec@1 81.250 (68.905)\nEpoch: [177][300/582]\\tLoss 5.7189 (4.4832)\\tPrec@1 75.000 (68.584)\nEpoch: [177][400/582]\\tLoss 2.4033 (4.4989)\\tPrec@1 68.750 (68.547)\nEpoch: [177][500/582]\\tLoss 6.9311 (4.4549)\\tPrec@1 56.250 (68.426)\nEpoch: [177][581/582]\\tLoss 1.9778 (4.3974)\\tPrec@1 50.000 (68.454)\n * Prec@1 43.010\nEpoch: [178][0/582]\\tLoss 4.6993 (4.6993)\\tPrec@1 62.500 (62.500)\nEpoch: [178][100/582]\\tLoss 6.6186 (4.0237)\\tPrec@1 62.500 (67.389)\nEpoch: [178][200/582]\\tLoss 1.5223 (4.2440)\\tPrec@1 68.750 (67.226)\nEpoch: [178][300/582]\\tLoss 1.6357 (4.1724)\\tPrec@1 81.250 (67.276)\nEpoch: [178][400/582]\\tLoss 1.2210 (4.2281)\\tPrec@1 62.500 (67.503)\nEpoch: [178][500/582]\\tLoss 1.2423 (4.2513)\\tPrec@1 62.500 (67.465)\nEpoch: [178][581/582]\\tLoss 1.0283 (4.3172)\\tPrec@1 50.000 (67.240)\n * Prec@1 42.640\nEpoch: [179][0/582]\\tLoss 1.2079 (1.2079)\\tPrec@1 62.500 (62.500)\nEpoch: [179][100/582]\\tLoss 0.5714 (4.2840)\\tPrec@1 62.500 (68.626)\nEpoch: [179][200/582]\\tLoss 0.7343 (4.4880)\\tPrec@1 68.750 (69.310)\nEpoch: [179][300/582]\\tLoss 7.1068 (4.4925)\\tPrec@1 68.750 (68.293)\nEpoch: [179][400/582]\\tLoss 1.9831 (4.4319)\\tPrec@1 56.250 (67.830)\nEpoch: [179][500/582]\\tLoss 1.8105 (4.3784)\\tPrec@1 81.250 (67.652)\nEpoch: [179][581/582]\\tLoss 1.3273 (4.4016)\\tPrec@1 50.000 (67.767)\n * Prec@1 42.670\nEpoch: [180][0/582]\\tLoss 2.0389 (2.0389)\\tPrec@1 62.500 (62.500)\nEpoch: [180][100/582]\\tLoss 2.8357 (5.6349)\\tPrec@1 62.500 (65.842)\nEpoch: [180][200/582]\\tLoss 1.8670 (4.8331)\\tPrec@1 68.750 (66.231)\nEpoch: [180][300/582]\\tLoss 3.1195 (4.7607)\\tPrec@1 75.000 (66.715)\nEpoch: [180][400/582]\\tLoss 13.7484 (4.6087)\\tPrec@1 43.750 (67.207)\nEpoch: [180][500/582]\\tLoss 1.8939 (4.3813)\\tPrec@1 68.750 (67.153)\nEpoch: [180][581/582]\\tLoss 0.2948 (4.3357)\\tPrec@1 75.000 (67.261)\n * Prec@1 42.640\nEpoch: [181][0/582]\\tLoss 5.8784 (5.8784)\\tPrec@1 62.500 (62.500)\nEpoch: [181][100/582]\\tLoss 14.9016 (4.8370)\\tPrec@1 56.250 (66.646)\nEpoch: [181][200/582]\\tLoss 2.1859 (4.5840)\\tPrec@1 62.500 (66.978)\nEpoch: [181][300/582]\\tLoss 6.0738 (4.5619)\\tPrec@1 56.250 (66.923)\nEpoch: [181][400/582]\\tLoss 11.0314 (4.5402)\\tPrec@1 56.250 (67.113)\nEpoch: [181][500/582]\\tLoss 3.5173 (4.4259)\\tPrec@1 75.000 (66.966)\nEpoch: [181][581/582]\\tLoss 1.1327 (4.4182)\\tPrec@1 62.500 (66.885)\n * Prec@1 42.970\nEpoch: [182][0/582]\\tLoss 15.0460 (15.0460)\\tPrec@1 62.500 (62.500)\nEpoch: [182][100/582]\\tLoss 3.9883 (4.1362)\\tPrec@1 56.250 (65.718)\nEpoch: [182][200/582]\\tLoss 1.2745 (3.7671)\\tPrec@1 81.250 (67.662)\nEpoch: [182][300/582]\\tLoss 3.8074 (4.2138)\\tPrec@1 68.750 (67.504)\nEpoch: [182][400/582]\\tLoss 1.2470 (4.2115)\\tPrec@1 68.750 (67.191)\nEpoch: [182][500/582]\\tLoss 1.3561 (4.2118)\\tPrec@1 62.500 (67.627)\nEpoch: [182][581/582]\\tLoss 3.1281 (4.2705)\\tPrec@1 50.000 (67.390)\n * Prec@1 42.760\nEpoch: [183][0/582]\\tLoss 7.6763 (7.6763)\\tPrec@1 62.500 (62.500)\nEpoch: [183][100/582]\\tLoss 3.1569 (4.5113)\\tPrec@1 62.500 (67.079)\nEpoch: [183][200/582]\\tLoss 0.1687 (4.4080)\\tPrec@1 93.750 (66.853)\nEpoch: [183][300/582]\\tLoss 4.2563 (4.4705)\\tPrec@1 43.750 (67.027)\nEpoch: [183][400/582]\\tLoss 4.8762 (4.3146)\\tPrec@1 43.750 (66.864)\nEpoch: [183][500/582]\\tLoss 3.2524 (4.3969)\\tPrec@1 81.250 (67.203)\nEpoch: [183][581/582]\\tLoss 0.5339 (4.3152)\\tPrec@1 87.500 (66.831)\n * Prec@1 43.080\nEpoch: [184][0/582]\\tLoss 14.8379 (14.8379)\\tPrec@1 43.750 (43.750)\nEpoch: [184][100/582]\\tLoss 2.0359 (4.4228)\\tPrec@1 87.500 (66.770)\nEpoch: [184][200/582]\\tLoss 0.7599 (4.0271)\\tPrec@1 87.500 (66.978)\nEpoch: [184][300/582]\\tLoss 1.9155 (4.3450)\\tPrec@1 81.250 (66.674)\nEpoch: [184][400/582]\\tLoss 3.9657 (4.3030)\\tPrec@1 56.250 (66.443)\nEpoch: [184][500/582]\\tLoss 9.9457 (4.2345)\\tPrec@1 62.500 (66.816)\nEpoch: [184][581/582]\\tLoss 0.4919 (4.2494)\\tPrec@1 87.500 (66.864)\n * Prec@1 43.100\nEpoch: [185][0/582]\\tLoss 2.5270 (2.5270)\\tPrec@1 56.250 (56.250)\nEpoch: [185][100/582]\\tLoss 2.8138 (4.0125)\\tPrec@1 75.000 (66.584)\nEpoch: [185][200/582]\\tLoss 6.8315 (3.8115)\\tPrec@1 68.750 (67.040)\nEpoch: [185][300/582]\\tLoss 24.3964 (4.3661)\\tPrec@1 75.000 (66.902)\nEpoch: [185][400/582]\\tLoss 1.5888 (4.1835)\\tPrec@1 56.250 (66.958)\nEpoch: [185][500/582]\\tLoss 4.1001 (4.1505)\\tPrec@1 50.000 (66.617)\nEpoch: [185][581/582]\\tLoss 2.0346 (4.3042)\\tPrec@1 50.000 (66.713)\n * Prec@1 43.250\nEpoch: [186][0/582]\\tLoss 1.8488 (1.8488)\\tPrec@1 93.750 (93.750)\nEpoch: [186][100/582]\\tLoss 1.1233 (3.9601)\\tPrec@1 81.250 (66.089)\nEpoch: [186][200/582]\\tLoss 3.1283 (4.4765)\\tPrec@1 50.000 (65.765)\nEpoch: [186][300/582]\\tLoss 1.4783 (4.3858)\\tPrec@1 68.750 (66.549)\nEpoch: [186][400/582]\\tLoss 2.1975 (4.2603)\\tPrec@1 50.000 (66.272)\nEpoch: [186][500/582]\\tLoss 1.9948 (4.3012)\\tPrec@1 68.750 (66.118)\nEpoch: [186][581/582]\\tLoss 1.5193 (4.2211)\\tPrec@1 25.000 (66.369)\n * Prec@1 42.700\nEpoch: [187][0/582]\\tLoss 2.4029 (2.4029)\\tPrec@1 75.000 (75.000)\nEpoch: [187][100/582]\\tLoss 9.1716 (5.0141)\\tPrec@1 50.000 (64.604)\nEpoch: [187][200/582]\\tLoss 3.4478 (4.4133)\\tPrec@1 68.750 (65.454)\nEpoch: [187][300/582]\\tLoss 0.8178 (4.3278)\\tPrec@1 75.000 (66.694)\nEpoch: [187][400/582]\\tLoss 1.0206 (4.1800)\\tPrec@1 75.000 (66.911)\nEpoch: [187][500/582]\\tLoss 8.7277 (4.0644)\\tPrec@1 62.500 (66.954)\nEpoch: [187][581/582]\\tLoss 1.6612 (4.1717)\\tPrec@1 62.500 (66.466)\n * Prec@1 42.890\nEpoch: [188][0/582]\\tLoss 0.7773 (0.7773)\\tPrec@1 68.750 (68.750)\nEpoch: [188][100/582]\\tLoss 2.9941 (4.3041)\\tPrec@1 50.000 (64.913)\nEpoch: [188][200/582]\\tLoss 0.9040 (4.2222)\\tPrec@1 75.000 (65.827)\nEpoch: [188][300/582]\\tLoss 7.0395 (4.1161)\\tPrec@1 68.750 (66.424)\nEpoch: [188][400/582]\\tLoss 13.0258 (4.3659)\\tPrec@1 43.750 (66.038)\nEpoch: [188][500/582]\\tLoss 2.8014 (4.2395)\\tPrec@1 62.500 (66.068)\nEpoch: [188][581/582]\\tLoss 1.1131 (4.2175)\\tPrec@1 62.500 (66.111)\n * Prec@1 43.330\nEpoch: [189][0/582]\\tLoss 1.7618 (1.7618)\\tPrec@1 62.500 (62.500)\nEpoch: [189][100/582]\\tLoss 0.7981 (4.6027)\\tPrec@1 75.000 (64.295)\nEpoch: [189][200/582]\\tLoss 1.5449 (4.4557)\\tPrec@1 50.000 (64.521)\nEpoch: [189][300/582]\\tLoss 2.1524 (4.3326)\\tPrec@1 75.000 (65.428)\nEpoch: [189][400/582]\\tLoss 1.0143 (4.3759)\\tPrec@1 68.750 (65.602)\nEpoch: [189][500/582]\\tLoss 12.5944 (4.2466)\\tPrec@1 62.500 (65.918)\nEpoch: [189][581/582]\\tLoss 2.0374 (4.1801)\\tPrec@1 37.500 (65.746)\n * Prec@1 43.260\nEpoch: [190][0/582]\\tLoss 3.6190 (3.6190)\\tPrec@1 87.500 (87.500)\nEpoch: [190][100/582]\\tLoss 3.0241 (4.3238)\\tPrec@1 37.500 (64.233)\nEpoch: [190][200/582]\\tLoss 4.8929 (4.4093)\\tPrec@1 50.000 (65.609)\nEpoch: [190][300/582]\\tLoss 1.3370 (4.1845)\\tPrec@1 68.750 (66.217)\nEpoch: [190][400/582]\\tLoss 1.8632 (4.2424)\\tPrec@1 68.750 (65.820)\nEpoch: [190][500/582]\\tLoss 10.8109 (4.1853)\\tPrec@1 56.250 (66.480)\nEpoch: [190][581/582]\\tLoss 0.9424 (4.1726)\\tPrec@1 50.000 (66.627)\n * Prec@1 43.320\nEpoch: [191][0/582]\\tLoss 3.4239 (3.4239)\\tPrec@1 75.000 (75.000)\nEpoch: [191][100/582]\\tLoss 1.3155 (3.8595)\\tPrec@1 81.250 (65.656)\nEpoch: [191][200/582]\\tLoss 1.4217 (4.0150)\\tPrec@1 100.000 (65.920)\nEpoch: [191][300/582]\\tLoss 1.4473 (4.2378)\\tPrec@1 81.250 (65.698)\nEpoch: [191][400/582]\\tLoss 2.2376 (4.1443)\\tPrec@1 62.500 (65.742)\nEpoch: [191][500/582]\\tLoss 0.9182 (4.2353)\\tPrec@1 81.250 (65.457)\nEpoch: [191][581/582]\\tLoss 9.6453 (4.1776)\\tPrec@1 37.500 (65.585)\n * Prec@1 43.400\nEpoch: [192][0/582]\\tLoss 2.9084 (2.9084)\\tPrec@1 68.750 (68.750)\nEpoch: [192][100/582]\\tLoss 5.1704 (4.2349)\\tPrec@1 62.500 (66.522)\nEpoch: [192][200/582]\\tLoss 7.1133 (3.9211)\\tPrec@1 43.750 (66.853)\nEpoch: [192][300/582]\\tLoss 6.0658 (4.0779)\\tPrec@1 62.500 (66.154)\nEpoch: [192][400/582]\\tLoss 3.1834 (4.0446)\\tPrec@1 50.000 (65.882)\nEpoch: [192][500/582]\\tLoss 10.0367 (4.1052)\\tPrec@1 75.000 (65.893)\nEpoch: [192][581/582]\\tLoss 0.5218 (4.1197)\\tPrec@1 62.500 (65.628)\n * Prec@1 42.970\nEpoch: [193][0/582]\\tLoss 4.6005 (4.6005)\\tPrec@1 56.250 (56.250)\nEpoch: [193][100/582]\\tLoss 1.5266 (4.1309)\\tPrec@1 68.750 (64.480)\nEpoch: [193][200/582]\\tLoss 4.1934 (4.1289)\\tPrec@1 50.000 (66.231)\nEpoch: [193][300/582]\\tLoss 2.5444 (4.2403)\\tPrec@1 68.750 (66.217)\nEpoch: [193][400/582]\\tLoss 3.1101 (4.3224)\\tPrec@1 68.750 (66.350)\nEpoch: [193][500/582]\\tLoss 5.3146 (4.2492)\\tPrec@1 68.750 (66.068)\nEpoch: [193][581/582]\\tLoss 0.8870 (4.2210)\\tPrec@1 50.000 (65.724)\n * Prec@1 43.150\nEpoch: [194][0/582]\\tLoss 9.6469 (9.6469)\\tPrec@1 62.500 (62.500)\nEpoch: [194][100/582]\\tLoss 1.2933 (4.2278)\\tPrec@1 62.500 (64.728)\nEpoch: [194][200/582]\\tLoss 1.4299 (4.1705)\\tPrec@1 75.000 (65.609)\nEpoch: [194][300/582]\\tLoss 2.8314 (4.1375)\\tPrec@1 56.250 (65.656)\nEpoch: [194][400/582]\\tLoss 8.3009 (4.1909)\\tPrec@1 37.500 (65.695)\nEpoch: [194][500/582]\\tLoss 6.4162 (4.2127)\\tPrec@1 75.000 (65.619)\nEpoch: [194][581/582]\\tLoss 1.1142 (4.1589)\\tPrec@1 62.500 (65.853)\n * Prec@1 43.190\nEpoch: [195][0/582]\\tLoss 1.1241 (1.1241)\\tPrec@1 75.000 (75.000)\nEpoch: [195][100/582]\\tLoss 2.8004 (4.3976)\\tPrec@1 68.750 (65.903)\nEpoch: [195][200/582]\\tLoss 12.0370 (4.5066)\\tPrec@1 81.250 (65.796)\nEpoch: [195][300/582]\\tLoss 1.9537 (4.3998)\\tPrec@1 75.000 (65.843)\nEpoch: [195][400/582]\\tLoss 4.1448 (4.3855)\\tPrec@1 56.250 (65.228)\nEpoch: [195][500/582]\\tLoss 6.5296 (4.3709)\\tPrec@1 50.000 (65.319)\nEpoch: [195][581/582]\\tLoss 1.2831 (4.2313)\\tPrec@1 25.000 (65.187)\n * Prec@1 43.220\nEpoch: [196][0/582]\\tLoss 17.0154 (17.0154)\\tPrec@1 37.500 (37.500)\nEpoch: [196][100/582]\\tLoss 4.6577 (3.9990)\\tPrec@1 68.750 (66.584)\nEpoch: [196][200/582]\\tLoss 4.6891 (4.1469)\\tPrec@1 56.250 (65.547)\nEpoch: [196][300/582]\\tLoss 7.4403 (3.9021)\\tPrec@1 75.000 (65.760)\nEpoch: [196][400/582]\\tLoss 1.2723 (4.1331)\\tPrec@1 75.000 (65.633)\nEpoch: [196][500/582]\\tLoss 3.0298 (4.1096)\\tPrec@1 62.500 (65.556)\nEpoch: [196][581/582]\\tLoss 2.4829 (4.0762)\\tPrec@1 25.000 (65.456)\n * Prec@1 43.040\nEpoch: [197][0/582]\\tLoss 1.9414 (1.9414)\\tPrec@1 68.750 (68.750)\nEpoch: [197][100/582]\\tLoss 0.4309 (4.6608)\\tPrec@1 81.250 (66.089)\nEpoch: [197][200/582]\\tLoss 5.9587 (4.5757)\\tPrec@1 68.750 (65.609)\nEpoch: [197][300/582]\\tLoss 2.3290 (4.3832)\\tPrec@1 62.500 (65.739)\nEpoch: [197][400/582]\\tLoss 1.5380 (4.1245)\\tPrec@1 56.250 (65.945)\nEpoch: [197][500/582]\\tLoss 2.6454 (4.0631)\\tPrec@1 75.000 (65.856)\nEpoch: [197][581/582]\\tLoss 1.3578 (4.1053)\\tPrec@1 37.500 (65.628)\n * Prec@1 43.580\nEpoch: [198][0/582]\\tLoss 4.1296 (4.1296)\\tPrec@1 43.750 (43.750)\nEpoch: [198][100/582]\\tLoss 23.3263 (3.9030)\\tPrec@1 56.250 (65.532)\nEpoch: [198][200/582]\\tLoss 3.0027 (4.1985)\\tPrec@1 62.500 (65.236)\nEpoch: [198][300/582]\\tLoss 1.9548 (4.1060)\\tPrec@1 81.250 (65.096)\nEpoch: [198][400/582]\\tLoss 2.3920 (4.0195)\\tPrec@1 87.500 (65.212)\nEpoch: [198][500/582]\\tLoss 5.3559 (4.0412)\\tPrec@1 50.000 (64.833)\nEpoch: [198][581/582]\\tLoss 0.2856 (4.1127)\\tPrec@1 75.000 (65.015)\n * Prec@1 42.840\nEpoch: [199][0/582]\\tLoss 11.1607 (11.1607)\\tPrec@1 56.250 (56.250)\nEpoch: [199][100/582]\\tLoss 6.0720 (4.3837)\\tPrec@1 50.000 (65.903)\nEpoch: [199][200/582]\\tLoss 1.7935 (4.0026)\\tPrec@1 68.750 (64.863)\nEpoch: [199][300/582]\\tLoss 3.7052 (4.1536)\\tPrec@1 56.250 (64.784)\nEpoch: [199][400/582]\\tLoss 1.3888 (3.9966)\\tPrec@1 87.500 (65.773)\nEpoch: [199][500/582]\\tLoss 2.3199 (4.0056)\\tPrec@1 75.000 (65.432)\nEpoch: [199][581/582]\\tLoss 0.9439 (4.0160)\\tPrec@1 75.000 (65.477)\n * Prec@1 43.400\nBest accuracy:  43.58\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Stage 1 (IF=100)\n!python pretrain_stage1.py --dataset cifar100 --imb_factor 0.01 --epochs 200\n\n# Stage 2 (IF=100)\n!python OT_train.py \\\n--dataset cifar100 \\\n--num_classes 100 \\\n--imb_factor 0.01 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar100_IF100_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar100_imb0.01_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:37:31.185984Z","iopub.execute_input":"2025-06-15T08:37:31.186227Z","iopub.status.idle":"2025-06-15T09:05:07.366352Z","shell.execute_reply.started":"2025-06-15T08:37:31.186202Z","shell.execute_reply":"2025-06-15T09:05:07.365622Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|██████████████████| 84/84 [00:05<00:00, 15.13it/s, Loss=2.319]\nEpoch 2/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=5.040]\nEpoch 3/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.84it/s, Loss=3.575]\nEpoch 4/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.77it/s, Loss=5.474]\nEpoch 5/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.73it/s, Loss=5.256]\nEpoch 6/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.82it/s, Loss=5.069]\nEpoch 7/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.76it/s, Loss=5.206]\nEpoch 8/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.63it/s, Loss=2.081]\nEpoch 9/200: 100%|██████████████████| 84/84 [00:04<00:00, 19.67it/s, Loss=3.670]\nEpoch 10/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.68it/s, Loss=2.752]\nEpoch 11/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.77it/s, Loss=4.221]\nEpoch 12/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.80it/s, Loss=2.864]\nEpoch 13/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=5.092]\nEpoch 14/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=1.934]\nEpoch 15/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=3.646]\nEpoch 16/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.12it/s, Loss=5.318]\nEpoch 17/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.14it/s, Loss=2.880]\nEpoch 18/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.13it/s, Loss=4.786]\nEpoch 19/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.17it/s, Loss=4.042]\nEpoch 20/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=4.574]\nEpoch 21/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=2.857]\nEpoch 22/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=3.237]\nEpoch 23/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=3.106]\nEpoch 24/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.86it/s, Loss=2.663]\nEpoch 25/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.71it/s, Loss=2.878]\nEpoch 26/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.66it/s, Loss=4.754]\nEpoch 27/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.78it/s, Loss=2.986]\nEpoch 28/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.60it/s, Loss=3.229]\nEpoch 29/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=1.911]\nEpoch 30/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.88it/s, Loss=3.323]\nEpoch 31/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=4.321]\nEpoch 32/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=4.338]\nEpoch 33/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=3.452]\nEpoch 34/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=2.958]\nEpoch 35/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.62it/s, Loss=2.869]\nEpoch 36/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=2.407]\nEpoch 37/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=3.069]\nEpoch 38/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=2.924]\nEpoch 39/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.78it/s, Loss=3.006]\nEpoch 40/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=1.872]\nEpoch 41/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=2.112]\nEpoch 42/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.08it/s, Loss=2.283]\nEpoch 43/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=4.929]\nEpoch 44/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=4.324]\nEpoch 45/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=3.785]\nEpoch 46/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=2.225]\nEpoch 47/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.86it/s, Loss=2.109]\nEpoch 48/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=2.682]\nEpoch 49/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=2.716]\nEpoch 50/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.58it/s, Loss=1.544]\nEpoch 51/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.59it/s, Loss=6.152]\nEpoch 52/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.81it/s, Loss=3.184]\nEpoch 53/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.81it/s, Loss=3.277]\nEpoch 54/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.84it/s, Loss=2.387]\nEpoch 55/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=4.292]\nEpoch 56/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=4.047]\nEpoch 57/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=3.489]\nEpoch 58/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.79it/s, Loss=4.431]\nEpoch 59/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=3.129]\nEpoch 60/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=3.311]\nEpoch 61/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=4.684]\nEpoch 62/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=5.724]\nEpoch 63/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=5.576]\nEpoch 64/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=3.612]\nEpoch 65/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.85it/s, Loss=2.341]\nEpoch 66/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.61it/s, Loss=2.351]\nEpoch 67/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=2.336]\nEpoch 68/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=1.685]\nEpoch 69/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=3.789]\nEpoch 70/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=3.130]\nEpoch 71/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=3.399]\nEpoch 72/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=3.669]\nEpoch 73/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.74it/s, Loss=3.893]\nEpoch 74/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=2.038]\nEpoch 75/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=4.237]\nEpoch 76/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=3.094]\nEpoch 77/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=3.095]\nEpoch 78/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.02it/s, Loss=4.939]\nEpoch 79/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=2.687]\nEpoch 80/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=3.526]\nEpoch 81/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=5.049]\nEpoch 82/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=2.233]\nEpoch 83/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.82it/s, Loss=2.088]\nEpoch 84/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.02it/s, Loss=2.332]\nEpoch 85/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=1.307]\nEpoch 86/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=2.786]\nEpoch 87/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.88it/s, Loss=4.170]\nEpoch 88/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.74it/s, Loss=3.869]\nEpoch 89/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.64it/s, Loss=3.565]\nEpoch 90/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=3.099]\nEpoch 91/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=2.540]\nEpoch 92/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=3.156]\nEpoch 93/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=4.086]\nEpoch 94/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=3.870]\nEpoch 95/200: 100%|█████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=3.627]\nEpoch 96/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=0.757]\nEpoch 97/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=2.279]\nEpoch 98/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=1.388]\nEpoch 99/200: 100%|█████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=2.494]\nEpoch 100/200: 100%|████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=2.269]\nEpoch 101/200: 100%|████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=3.018]\nEpoch 102/200: 100%|████████████████| 84/84 [00:04<00:00, 19.84it/s, Loss=1.164]\nEpoch 103/200: 100%|████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=3.804]\nEpoch 104/200: 100%|████████████████| 84/84 [00:04<00:00, 19.54it/s, Loss=3.209]\nEpoch 105/200: 100%|████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=3.353]\nEpoch 106/200: 100%|████████████████| 84/84 [00:04<00:00, 19.83it/s, Loss=4.298]\nEpoch 107/200: 100%|████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=1.796]\nEpoch 108/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=5.143]\nEpoch 109/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=1.185]\nEpoch 110/200: 100%|████████████████| 84/84 [00:04<00:00, 20.03it/s, Loss=3.175]\nEpoch 111/200: 100%|████████████████| 84/84 [00:04<00:00, 19.81it/s, Loss=3.080]\nEpoch 112/200: 100%|████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=2.768]\nEpoch 113/200: 100%|████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=4.852]\nEpoch 114/200: 100%|████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=6.169]\nEpoch 115/200: 100%|████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=5.290]\nEpoch 116/200: 100%|████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=2.637]\nEpoch 117/200: 100%|████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=3.834]\nEpoch 118/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=3.573]\nEpoch 119/200: 100%|████████████████| 84/84 [00:04<00:00, 19.80it/s, Loss=2.003]\nEpoch 120/200: 100%|████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=3.665]\nEpoch 121/200: 100%|████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=2.544]\nEpoch 122/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=3.266]\nEpoch 123/200: 100%|████████████████| 84/84 [00:04<00:00, 19.95it/s, Loss=3.152]\nEpoch 124/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=2.425]\nEpoch 125/200: 100%|████████████████| 84/84 [00:04<00:00, 19.85it/s, Loss=2.992]\nEpoch 126/200: 100%|████████████████| 84/84 [00:04<00:00, 19.79it/s, Loss=3.282]\nEpoch 127/200: 100%|████████████████| 84/84 [00:04<00:00, 19.76it/s, Loss=1.440]\nEpoch 128/200: 100%|████████████████| 84/84 [00:04<00:00, 19.81it/s, Loss=2.043]\nEpoch 129/200: 100%|████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=2.352]\nEpoch 130/200: 100%|████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=4.736]\nEpoch 131/200: 100%|████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=2.727]\nEpoch 132/200: 100%|████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=1.990]\nEpoch 133/200: 100%|████████████████| 84/84 [00:04<00:00, 20.09it/s, Loss=3.143]\nEpoch 134/200: 100%|████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=4.032]\nEpoch 135/200: 100%|████████████████| 84/84 [00:04<00:00, 20.02it/s, Loss=0.711]\nEpoch 136/200: 100%|████████████████| 84/84 [00:04<00:00, 20.08it/s, Loss=3.743]\nEpoch 137/200: 100%|████████████████| 84/84 [00:04<00:00, 20.06it/s, Loss=2.512]\nEpoch 138/200: 100%|████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=6.360]\nEpoch 139/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=3.716]\nEpoch 140/200: 100%|████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=3.828]\nEpoch 141/200: 100%|████████████████| 84/84 [00:04<00:00, 20.02it/s, Loss=1.056]\nEpoch 142/200: 100%|████████████████| 84/84 [00:04<00:00, 19.66it/s, Loss=2.733]\nEpoch 143/200: 100%|████████████████| 84/84 [00:04<00:00, 20.11it/s, Loss=5.185]\nEpoch 144/200: 100%|████████████████| 84/84 [00:04<00:00, 20.10it/s, Loss=3.911]\nEpoch 145/200: 100%|████████████████| 84/84 [00:04<00:00, 20.08it/s, Loss=1.852]\nEpoch 146/200: 100%|████████████████| 84/84 [00:04<00:00, 20.09it/s, Loss=3.531]\nEpoch 147/200: 100%|████████████████| 84/84 [00:04<00:00, 20.04it/s, Loss=2.801]\nEpoch 148/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=4.510]\nEpoch 149/200: 100%|████████████████| 84/84 [00:04<00:00, 19.87it/s, Loss=3.437]\nEpoch 150/200: 100%|████████████████| 84/84 [00:04<00:00, 19.89it/s, Loss=4.202]\nEpoch 151/200: 100%|████████████████| 84/84 [00:04<00:00, 19.94it/s, Loss=2.892]\nEpoch 152/200: 100%|████████████████| 84/84 [00:04<00:00, 20.11it/s, Loss=4.616]\nEpoch 153/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=4.201]\nEpoch 154/200: 100%|████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=2.394]\nEpoch 155/200: 100%|████████████████| 84/84 [00:04<00:00, 20.08it/s, Loss=2.533]\nEpoch 156/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=3.293]\nEpoch 157/200: 100%|████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=3.997]\nEpoch 158/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=5.415]\nEpoch 159/200: 100%|████████████████| 84/84 [00:04<00:00, 19.93it/s, Loss=4.844]\nEpoch 160/200: 100%|████████████████| 84/84 [00:04<00:00, 20.13it/s, Loss=3.787]\nEpoch 161/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=2.873]\nEpoch 162/200: 100%|████████████████| 84/84 [00:04<00:00, 20.16it/s, Loss=3.136]\nEpoch 163/200: 100%|████████████████| 84/84 [00:04<00:00, 20.08it/s, Loss=2.840]\nEpoch 164/200: 100%|████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=1.286]\nEpoch 165/200: 100%|████████████████| 84/84 [00:04<00:00, 19.75it/s, Loss=3.794]\nEpoch 166/200: 100%|████████████████| 84/84 [00:04<00:00, 20.02it/s, Loss=3.338]\nEpoch 167/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=1.595]\nEpoch 168/200: 100%|████████████████| 84/84 [00:04<00:00, 20.10it/s, Loss=1.417]\nEpoch 169/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=2.223]\nEpoch 170/200: 100%|████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=2.362]\nEpoch 171/200: 100%|████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=1.157]\nEpoch 172/200: 100%|████████████████| 84/84 [00:04<00:00, 19.90it/s, Loss=0.940]\nEpoch 173/200: 100%|████████████████| 84/84 [00:04<00:00, 20.03it/s, Loss=1.006]\nEpoch 174/200: 100%|████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=2.240]\nEpoch 175/200: 100%|████████████████| 84/84 [00:04<00:00, 19.96it/s, Loss=2.432]\nEpoch 176/200: 100%|████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=3.514]\nEpoch 177/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=3.241]\nEpoch 178/200: 100%|████████████████| 84/84 [00:04<00:00, 20.03it/s, Loss=2.885]\nEpoch 179/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=1.975]\nEpoch 180/200: 100%|████████████████| 84/84 [00:04<00:00, 19.91it/s, Loss=1.148]\nEpoch 181/200: 100%|████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=0.610]\nEpoch 182/200: 100%|████████████████| 84/84 [00:04<00:00, 20.03it/s, Loss=2.908]\nEpoch 183/200: 100%|████████████████| 84/84 [00:04<00:00, 20.07it/s, Loss=2.321]\nEpoch 184/200: 100%|████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=2.192]\nEpoch 185/200: 100%|████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=1.737]\nEpoch 186/200: 100%|████████████████| 84/84 [00:04<00:00, 19.99it/s, Loss=4.223]\nEpoch 187/200: 100%|████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=0.883]\nEpoch 188/200: 100%|████████████████| 84/84 [00:04<00:00, 19.81it/s, Loss=2.844]\nEpoch 189/200: 100%|████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=3.044]\nEpoch 190/200: 100%|████████████████| 84/84 [00:04<00:00, 20.00it/s, Loss=1.511]\nEpoch 191/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=2.889]\nEpoch 192/200: 100%|████████████████| 84/84 [00:04<00:00, 19.92it/s, Loss=3.695]\nEpoch 193/200: 100%|████████████████| 84/84 [00:04<00:00, 20.04it/s, Loss=2.350]\nEpoch 194/200: 100%|████████████████| 84/84 [00:04<00:00, 20.01it/s, Loss=4.680]\nEpoch 195/200: 100%|████████████████| 84/84 [00:04<00:00, 19.84it/s, Loss=0.615]\nEpoch 196/200: 100%|████████████████| 84/84 [00:04<00:00, 20.03it/s, Loss=4.806]\nEpoch 197/200: 100%|████████████████| 84/84 [00:04<00:00, 19.98it/s, Loss=4.146]\nEpoch 198/200: 100%|████████████████| 84/84 [00:04<00:00, 20.05it/s, Loss=2.413]\nEpoch 199/200: 100%|████████████████| 84/84 [00:04<00:00, 19.97it/s, Loss=3.434]\nEpoch 200/200: 100%|████████████████| 84/84 [00:04<00:00, 20.04it/s, Loss=2.377]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar100_imb0.01_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\nFile 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar100\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=100\nnum_meta=10\nimb_factor=0.01\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar100_IF100_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar100_imb0.01_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/665]\\tLoss 15.5923 (15.5923)\\tPrec@1 75.000 (75.000)\nEpoch: [160][100/665]\\tLoss 25.2960 (7.6804)\\tPrec@1 56.250 (68.131)\nEpoch: [160][200/665]\\tLoss 7.7974 (9.1759)\\tPrec@1 62.500 (67.320)\nEpoch: [160][300/665]\\tLoss 7.8133 (9.2827)\\tPrec@1 75.000 (67.213)\nEpoch: [160][400/665]\\tLoss 14.3645 (8.9393)\\tPrec@1 68.750 (67.160)\nEpoch: [160][500/665]\\tLoss 37.0132 (8.8630)\\tPrec@1 50.000 (67.228)\nEpoch: [160][600/665]\\tLoss 9.2565 (8.9111)\\tPrec@1 75.000 (67.263)\nEpoch: [160][664/665]\\tLoss 1.7823 (8.9361)\\tPrec@1 50.000 (67.062)\n * Prec@1 39.630\nEpoch: [161][0/665]\\tLoss 17.5793 (17.5793)\\tPrec@1 68.750 (68.750)\nEpoch: [161][100/665]\\tLoss 10.0352 (7.9266)\\tPrec@1 62.500 (69.493)\nEpoch: [161][200/665]\\tLoss 1.8438 (7.6000)\\tPrec@1 81.250 (68.439)\nEpoch: [161][300/665]\\tLoss 13.3972 (8.3632)\\tPrec@1 56.250 (67.878)\nEpoch: [161][400/665]\\tLoss 6.0984 (8.4742)\\tPrec@1 62.500 (67.347)\nEpoch: [161][500/665]\\tLoss 2.0246 (8.6821)\\tPrec@1 75.000 (67.066)\nEpoch: [161][600/665]\\tLoss 2.7220 (8.5182)\\tPrec@1 62.500 (67.190)\nEpoch: [161][664/665]\\tLoss 0.4977 (8.6296)\\tPrec@1 50.000 (67.297)\n * Prec@1 40.940\nEpoch: [162][0/665]\\tLoss 2.5665 (2.5665)\\tPrec@1 75.000 (75.000)\nEpoch: [162][100/665]\\tLoss 6.0296 (8.1600)\\tPrec@1 56.250 (69.183)\nEpoch: [162][200/665]\\tLoss 1.7938 (8.2837)\\tPrec@1 81.250 (68.843)\nEpoch: [162][300/665]\\tLoss 8.5214 (8.0441)\\tPrec@1 68.750 (68.314)\nEpoch: [162][400/665]\\tLoss 4.8826 (8.1808)\\tPrec@1 56.250 (67.924)\nEpoch: [162][500/665]\\tLoss 17.3227 (8.2856)\\tPrec@1 56.250 (67.540)\nEpoch: [162][600/665]\\tLoss 6.2207 (8.3966)\\tPrec@1 81.250 (67.523)\nEpoch: [162][664/665]\\tLoss 3.3709 (8.2090)\\tPrec@1 50.000 (67.448)\n * Prec@1 40.830\nEpoch: [163][0/665]\\tLoss 25.5902 (25.5902)\\tPrec@1 56.250 (56.250)\nEpoch: [163][100/665]\\tLoss 1.0202 (7.2414)\\tPrec@1 87.500 (69.183)\nEpoch: [163][200/665]\\tLoss 1.3384 (8.0214)\\tPrec@1 68.750 (68.066)\nEpoch: [163][300/665]\\tLoss 0.9297 (7.7435)\\tPrec@1 87.500 (67.857)\nEpoch: [163][400/665]\\tLoss 2.8141 (7.8851)\\tPrec@1 62.500 (66.880)\nEpoch: [163][500/665]\\tLoss 8.6675 (7.9186)\\tPrec@1 56.250 (66.954)\nEpoch: [163][600/665]\\tLoss 2.4824 (8.0349)\\tPrec@1 50.000 (66.535)\nEpoch: [163][664/665]\\tLoss 1.2943 (7.8803)\\tPrec@1 50.000 (66.620)\n * Prec@1 41.470\nEpoch: [164][0/665]\\tLoss 10.0041 (10.0041)\\tPrec@1 81.250 (81.250)\nEpoch: [164][100/665]\\tLoss 2.7870 (8.0860)\\tPrec@1 81.250 (66.770)\nEpoch: [164][200/665]\\tLoss 6.0809 (7.7856)\\tPrec@1 68.750 (66.822)\nEpoch: [164][300/665]\\tLoss 1.7045 (7.6641)\\tPrec@1 68.750 (66.217)\nEpoch: [164][400/665]\\tLoss 2.3021 (7.6891)\\tPrec@1 62.500 (66.397)\nEpoch: [164][500/665]\\tLoss 20.6383 (7.6542)\\tPrec@1 62.500 (66.405)\nEpoch: [164][600/665]\\tLoss 7.2355 (7.7846)\\tPrec@1 43.750 (66.192)\nEpoch: [164][664/665]\\tLoss 0.3997 (7.7894)\\tPrec@1 0.000 (66.111)\n * Prec@1 42.180\nEpoch: [165][0/665]\\tLoss 2.9022 (2.9022)\\tPrec@1 81.250 (81.250)\nEpoch: [165][100/665]\\tLoss 3.9780 (7.7898)\\tPrec@1 75.000 (67.203)\nEpoch: [165][200/665]\\tLoss 5.7248 (7.7084)\\tPrec@1 56.250 (66.511)\nEpoch: [165][300/665]\\tLoss 3.0726 (7.3930)\\tPrec@1 87.500 (66.860)\nEpoch: [165][400/665]\\tLoss 1.3687 (7.3533)\\tPrec@1 87.500 (66.397)\nEpoch: [165][500/665]\\tLoss 4.6725 (7.4879)\\tPrec@1 50.000 (65.931)\nEpoch: [165][600/665]\\tLoss 11.2142 (7.5001)\\tPrec@1 81.250 (65.921)\nEpoch: [165][664/665]\\tLoss 3.6557 (7.5176)\\tPrec@1 0.000 (65.801)\n * Prec@1 42.610\nEpoch: [166][0/665]\\tLoss 2.4142 (2.4142)\\tPrec@1 75.000 (75.000)\nEpoch: [166][100/665]\\tLoss 9.4082 (7.5251)\\tPrec@1 43.750 (63.800)\nEpoch: [166][200/665]\\tLoss 5.7292 (7.8596)\\tPrec@1 62.500 (64.583)\nEpoch: [166][300/665]\\tLoss 1.4677 (7.9686)\\tPrec@1 75.000 (64.348)\nEpoch: [166][400/665]\\tLoss 3.3509 (7.7500)\\tPrec@1 50.000 (64.822)\nEpoch: [166][500/665]\\tLoss 11.2540 (7.8301)\\tPrec@1 43.750 (64.596)\nEpoch: [166][600/665]\\tLoss 2.6149 (7.6326)\\tPrec@1 75.000 (64.757)\nEpoch: [166][664/665]\\tLoss 0.3490 (7.6003)\\tPrec@1 50.000 (64.916)\n * Prec@1 43.020\nEpoch: [167][0/665]\\tLoss 2.5599 (2.5599)\\tPrec@1 75.000 (75.000)\nEpoch: [167][100/665]\\tLoss 2.5045 (7.9371)\\tPrec@1 62.500 (65.903)\nEpoch: [167][200/665]\\tLoss 5.0135 (7.7658)\\tPrec@1 68.750 (65.267)\nEpoch: [167][300/665]\\tLoss 5.3579 (7.4809)\\tPrec@1 56.250 (64.639)\nEpoch: [167][400/665]\\tLoss 5.8088 (7.3208)\\tPrec@1 50.000 (64.573)\nEpoch: [167][500/665]\\tLoss 5.5401 (7.3584)\\tPrec@1 50.000 (64.558)\nEpoch: [167][600/665]\\tLoss 6.1832 (7.3858)\\tPrec@1 75.000 (64.382)\nEpoch: [167][664/665]\\tLoss 0.2791 (7.4074)\\tPrec@1 50.000 (64.342)\n * Prec@1 43.090\nEpoch: [168][0/665]\\tLoss 4.5573 (4.5573)\\tPrec@1 62.500 (62.500)\nEpoch: [168][100/665]\\tLoss 27.4790 (7.7513)\\tPrec@1 62.500 (63.304)\nEpoch: [168][200/665]\\tLoss 18.5655 (6.9006)\\tPrec@1 50.000 (63.837)\nEpoch: [168][300/665]\\tLoss 1.9985 (7.0279)\\tPrec@1 62.500 (63.767)\nEpoch: [168][400/665]\\tLoss 30.0574 (7.1971)\\tPrec@1 56.250 (63.778)\nEpoch: [168][500/665]\\tLoss 2.5523 (7.4329)\\tPrec@1 68.750 (63.922)\nEpoch: [168][600/665]\\tLoss 3.7340 (7.3839)\\tPrec@1 75.000 (63.800)\nEpoch: [168][664/665]\\tLoss 0.3051 (7.3296)\\tPrec@1 0.000 (63.872)\n * Prec@1 43.600\nEpoch: [169][0/665]\\tLoss 8.2101 (8.2101)\\tPrec@1 50.000 (50.000)\nEpoch: [169][100/665]\\tLoss 5.3289 (7.5520)\\tPrec@1 68.750 (62.500)\nEpoch: [169][200/665]\\tLoss 4.3768 (7.5956)\\tPrec@1 62.500 (63.837)\nEpoch: [169][300/665]\\tLoss 1.9357 (7.7282)\\tPrec@1 75.000 (63.663)\nEpoch: [169][400/665]\\tLoss 5.0654 (7.7145)\\tPrec@1 50.000 (63.872)\nEpoch: [169][500/665]\\tLoss 3.8192 (7.5294)\\tPrec@1 56.250 (64.022)\nEpoch: [169][600/665]\\tLoss 6.1906 (7.3771)\\tPrec@1 50.000 (63.665)\nEpoch: [169][664/665]\\tLoss 0.6007 (7.2455)\\tPrec@1 0.000 (63.909)\n * Prec@1 43.530\nEpoch: [170][0/665]\\tLoss 4.1113 (4.1113)\\tPrec@1 56.250 (56.250)\nEpoch: [170][100/665]\\tLoss 3.0846 (7.4047)\\tPrec@1 62.500 (61.881)\nEpoch: [170][200/665]\\tLoss 5.1653 (7.7956)\\tPrec@1 62.500 (61.287)\nEpoch: [170][300/665]\\tLoss 4.4527 (7.3517)\\tPrec@1 68.750 (61.877)\nEpoch: [170][400/665]\\tLoss 3.8367 (7.1735)\\tPrec@1 62.500 (62.219)\nEpoch: [170][500/665]\\tLoss 2.6545 (7.1490)\\tPrec@1 81.250 (62.525)\nEpoch: [170][600/665]\\tLoss 4.7547 (7.1222)\\tPrec@1 75.000 (63.093)\nEpoch: [170][664/665]\\tLoss 0.4259 (7.1650)\\tPrec@1 50.000 (63.175)\n * Prec@1 43.870\nEpoch: [171][0/665]\\tLoss 2.3677 (2.3677)\\tPrec@1 62.500 (62.500)\nEpoch: [171][100/665]\\tLoss 2.2082 (7.0461)\\tPrec@1 62.500 (61.634)\nEpoch: [171][200/665]\\tLoss 5.5395 (7.2307)\\tPrec@1 50.000 (62.220)\nEpoch: [171][300/665]\\tLoss 3.5580 (7.3285)\\tPrec@1 75.000 (62.209)\nEpoch: [171][400/665]\\tLoss 8.0914 (7.2356)\\tPrec@1 50.000 (61.970)\nEpoch: [171][500/665]\\tLoss 2.4528 (7.1509)\\tPrec@1 68.750 (62.425)\nEpoch: [171][600/665]\\tLoss 4.3418 (7.2620)\\tPrec@1 50.000 (62.438)\nEpoch: [171][664/665]\\tLoss 0.6057 (7.1497)\\tPrec@1 50.000 (62.535)\n * Prec@1 43.800\nEpoch: [172][0/665]\\tLoss 4.8264 (4.8264)\\tPrec@1 62.500 (62.500)\nEpoch: [172][100/665]\\tLoss 4.4761 (6.8983)\\tPrec@1 50.000 (63.985)\nEpoch: [172][200/665]\\tLoss 6.5371 (6.5342)\\tPrec@1 50.000 (64.179)\nEpoch: [172][300/665]\\tLoss 3.2391 (6.8976)\\tPrec@1 62.500 (63.829)\nEpoch: [172][400/665]\\tLoss 5.2921 (6.9838)\\tPrec@1 62.500 (63.685)\nEpoch: [172][500/665]\\tLoss 8.9057 (7.0773)\\tPrec@1 56.250 (63.398)\nEpoch: [172][600/665]\\tLoss 1.1860 (7.1045)\\tPrec@1 93.750 (63.280)\nEpoch: [172][664/665]\\tLoss 2.2844 (7.0276)\\tPrec@1 0.000 (63.072)\n * Prec@1 43.470\nEpoch: [173][0/665]\\tLoss 9.2048 (9.2048)\\tPrec@1 56.250 (56.250)\nEpoch: [173][100/665]\\tLoss 1.8700 (7.2032)\\tPrec@1 68.750 (60.520)\nEpoch: [173][200/665]\\tLoss 6.8105 (6.8789)\\tPrec@1 56.250 (61.754)\nEpoch: [173][300/665]\\tLoss 1.4909 (7.0298)\\tPrec@1 87.500 (61.752)\nEpoch: [173][400/665]\\tLoss 5.0656 (7.0014)\\tPrec@1 56.250 (61.736)\nEpoch: [173][500/665]\\tLoss 4.1946 (6.8462)\\tPrec@1 43.750 (61.664)\nEpoch: [173][600/665]\\tLoss 12.4306 (6.9775)\\tPrec@1 56.250 (61.626)\nEpoch: [173][664/665]\\tLoss 1.8023 (6.9665)\\tPrec@1 0.000 (61.566)\n * Prec@1 43.570\nEpoch: [174][0/665]\\tLoss 5.3422 (5.3422)\\tPrec@1 75.000 (75.000)\nEpoch: [174][100/665]\\tLoss 9.4860 (6.5794)\\tPrec@1 50.000 (61.943)\nEpoch: [174][200/665]\\tLoss 1.0685 (6.9800)\\tPrec@1 87.500 (62.158)\nEpoch: [174][300/665]\\tLoss 3.0108 (6.9728)\\tPrec@1 43.750 (61.877)\nEpoch: [174][400/665]\\tLoss 4.9243 (7.0881)\\tPrec@1 75.000 (61.612)\nEpoch: [174][500/665]\\tLoss 4.0604 (7.0068)\\tPrec@1 62.500 (61.839)\nEpoch: [174][600/665]\\tLoss 4.4772 (6.9595)\\tPrec@1 56.250 (61.502)\nEpoch: [174][664/665]\\tLoss 2.0353 (6.9256)\\tPrec@1 0.000 (61.425)\n * Prec@1 44.120\nEpoch: [175][0/665]\\tLoss 22.7969 (22.7969)\\tPrec@1 56.250 (56.250)\nEpoch: [175][100/665]\\tLoss 3.1121 (7.0278)\\tPrec@1 56.250 (60.149)\nEpoch: [175][200/665]\\tLoss 2.6659 (7.1686)\\tPrec@1 68.750 (59.546)\nEpoch: [175][300/665]\\tLoss 23.4661 (6.9324)\\tPrec@1 62.500 (61.109)\nEpoch: [175][400/665]\\tLoss 6.2019 (6.8660)\\tPrec@1 68.750 (61.238)\nEpoch: [175][500/665]\\tLoss 2.6824 (6.8221)\\tPrec@1 75.000 (60.828)\nEpoch: [175][600/665]\\tLoss 4.9295 (6.9965)\\tPrec@1 50.000 (60.857)\nEpoch: [175][664/665]\\tLoss 2.2435 (6.9430)\\tPrec@1 0.000 (60.757)\n * Prec@1 43.770\nEpoch: [176][0/665]\\tLoss 8.2997 (8.2997)\\tPrec@1 56.250 (56.250)\nEpoch: [176][100/665]\\tLoss 1.0538 (7.6172)\\tPrec@1 81.250 (60.334)\nEpoch: [176][200/665]\\tLoss 5.3245 (6.9453)\\tPrec@1 50.000 (60.728)\nEpoch: [176][300/665]\\tLoss 10.2207 (6.8846)\\tPrec@1 68.750 (60.673)\nEpoch: [176][400/665]\\tLoss 3.2629 (6.9261)\\tPrec@1 62.500 (61.004)\nEpoch: [176][500/665]\\tLoss 18.0630 (6.7694)\\tPrec@1 50.000 (61.527)\nEpoch: [176][600/665]\\tLoss 16.1916 (6.7692)\\tPrec@1 62.500 (61.398)\nEpoch: [176][664/665]\\tLoss 2.3856 (6.8826)\\tPrec@1 0.000 (61.557)\n * Prec@1 43.460\nEpoch: [177][0/665]\\tLoss 4.9798 (4.9798)\\tPrec@1 56.250 (56.250)\nEpoch: [177][100/665]\\tLoss 2.9174 (7.0319)\\tPrec@1 56.250 (60.520)\nEpoch: [177][200/665]\\tLoss 10.0881 (7.0822)\\tPrec@1 68.750 (61.194)\nEpoch: [177][300/665]\\tLoss 1.9156 (7.0654)\\tPrec@1 56.250 (60.465)\nEpoch: [177][400/665]\\tLoss 7.9235 (7.0063)\\tPrec@1 56.250 (60.489)\nEpoch: [177][500/665]\\tLoss 6.7732 (6.9675)\\tPrec@1 56.250 (60.616)\nEpoch: [177][600/665]\\tLoss 1.8492 (6.8668)\\tPrec@1 56.250 (60.878)\nEpoch: [177][664/665]\\tLoss 4.2428 (6.8819)\\tPrec@1 0.000 (60.992)\n * Prec@1 44.120\nEpoch: [178][0/665]\\tLoss 5.8742 (5.8742)\\tPrec@1 56.250 (56.250)\nEpoch: [178][100/665]\\tLoss 4.5376 (7.5393)\\tPrec@1 56.250 (61.634)\nEpoch: [178][200/665]\\tLoss 2.7052 (7.4371)\\tPrec@1 81.250 (61.194)\nEpoch: [178][300/665]\\tLoss 3.1669 (7.2202)\\tPrec@1 62.500 (61.462)\nEpoch: [178][400/665]\\tLoss 4.5234 (7.1721)\\tPrec@1 62.500 (61.082)\nEpoch: [178][500/665]\\tLoss 1.5879 (7.1739)\\tPrec@1 68.750 (61.178)\nEpoch: [178][600/665]\\tLoss 1.5687 (7.0280)\\tPrec@1 81.250 (61.158)\nEpoch: [178][664/665]\\tLoss 0.4522 (6.8804)\\tPrec@1 0.000 (61.077)\n * Prec@1 43.990\nEpoch: [179][0/665]\\tLoss 6.2952 (6.2952)\\tPrec@1 56.250 (56.250)\nEpoch: [179][100/665]\\tLoss 5.0571 (8.5759)\\tPrec@1 68.750 (57.983)\nEpoch: [179][200/665]\\tLoss 1.4093 (7.7764)\\tPrec@1 75.000 (59.670)\nEpoch: [179][300/665]\\tLoss 2.7719 (7.1525)\\tPrec@1 68.750 (60.257)\nEpoch: [179][400/665]\\tLoss 8.4499 (7.0233)\\tPrec@1 75.000 (59.975)\nEpoch: [179][500/665]\\tLoss 3.3622 (6.7971)\\tPrec@1 62.500 (60.167)\nEpoch: [179][600/665]\\tLoss 7.0853 (6.8672)\\tPrec@1 56.250 (60.316)\nEpoch: [179][664/665]\\tLoss 1.2594 (6.8501)\\tPrec@1 0.000 (60.296)\n * Prec@1 44.280\nEpoch: [180][0/665]\\tLoss 8.8186 (8.8186)\\tPrec@1 43.750 (43.750)\nEpoch: [180][100/665]\\tLoss 2.2921 (6.3794)\\tPrec@1 75.000 (61.386)\nEpoch: [180][200/665]\\tLoss 0.5948 (6.3958)\\tPrec@1 87.500 (61.318)\nEpoch: [180][300/665]\\tLoss 9.7904 (6.7423)\\tPrec@1 68.750 (61.296)\nEpoch: [180][400/665]\\tLoss 14.3408 (6.6259)\\tPrec@1 50.000 (61.050)\nEpoch: [180][500/665]\\tLoss 8.6876 (6.4416)\\tPrec@1 62.500 (61.065)\nEpoch: [180][600/665]\\tLoss 1.5763 (6.6188)\\tPrec@1 81.250 (60.722)\nEpoch: [180][664/665]\\tLoss 1.1531 (6.7215)\\tPrec@1 50.000 (60.568)\n * Prec@1 44.430\nEpoch: [181][0/665]\\tLoss 25.0479 (25.0479)\\tPrec@1 50.000 (50.000)\nEpoch: [181][100/665]\\tLoss 10.7040 (7.3927)\\tPrec@1 43.750 (60.644)\nEpoch: [181][200/665]\\tLoss 12.0260 (6.9073)\\tPrec@1 50.000 (61.629)\nEpoch: [181][300/665]\\tLoss 5.9468 (6.8406)\\tPrec@1 62.500 (60.673)\nEpoch: [181][400/665]\\tLoss 6.6220 (6.7400)\\tPrec@1 56.250 (60.863)\nEpoch: [181][500/665]\\tLoss 3.5427 (6.6932)\\tPrec@1 68.750 (60.654)\nEpoch: [181][600/665]\\tLoss 8.9560 (6.8071)\\tPrec@1 50.000 (60.472)\nEpoch: [181][664/665]\\tLoss 0.3958 (6.7995)\\tPrec@1 50.000 (60.343)\n * Prec@1 44.350\nEpoch: [182][0/665]\\tLoss 1.3726 (1.3726)\\tPrec@1 81.250 (81.250)\nEpoch: [182][100/665]\\tLoss 7.5061 (7.7025)\\tPrec@1 68.750 (62.252)\nEpoch: [182][200/665]\\tLoss 9.2249 (6.6827)\\tPrec@1 37.500 (61.256)\nEpoch: [182][300/665]\\tLoss 3.3063 (6.6690)\\tPrec@1 62.500 (60.922)\nEpoch: [182][400/665]\\tLoss 15.0736 (6.8237)\\tPrec@1 62.500 (61.238)\nEpoch: [182][500/665]\\tLoss 8.8600 (6.7087)\\tPrec@1 62.500 (60.891)\nEpoch: [182][600/665]\\tLoss 4.4572 (6.7392)\\tPrec@1 37.500 (60.472)\nEpoch: [182][664/665]\\tLoss 0.5591 (6.8001)\\tPrec@1 0.000 (60.371)\n * Prec@1 44.030\nEpoch: [183][0/665]\\tLoss 3.6057 (3.6057)\\tPrec@1 75.000 (75.000)\nEpoch: [183][100/665]\\tLoss 13.9874 (7.0998)\\tPrec@1 37.500 (59.282)\nEpoch: [183][200/665]\\tLoss 4.8552 (7.1597)\\tPrec@1 68.750 (59.639)\nEpoch: [183][300/665]\\tLoss 17.5645 (6.8765)\\tPrec@1 87.500 (60.008)\nEpoch: [183][400/665]\\tLoss 2.9184 (6.8788)\\tPrec@1 68.750 (60.037)\nEpoch: [183][500/665]\\tLoss 8.6547 (6.8049)\\tPrec@1 62.500 (60.205)\nEpoch: [183][600/665]\\tLoss 1.4863 (6.8029)\\tPrec@1 62.500 (60.254)\nEpoch: [183][664/665]\\tLoss 0.5128 (6.8061)\\tPrec@1 0.000 (60.267)\n * Prec@1 44.460\nEpoch: [184][0/665]\\tLoss 7.7868 (7.7868)\\tPrec@1 43.750 (43.750)\nEpoch: [184][100/665]\\tLoss 6.3848 (6.1743)\\tPrec@1 31.250 (61.077)\nEpoch: [184][200/665]\\tLoss 25.5135 (6.7305)\\tPrec@1 56.250 (60.137)\nEpoch: [184][300/665]\\tLoss 3.0850 (6.9303)\\tPrec@1 50.000 (60.444)\nEpoch: [184][400/665]\\tLoss 15.7949 (7.0012)\\tPrec@1 50.000 (60.692)\nEpoch: [184][500/665]\\tLoss 4.4036 (6.9148)\\tPrec@1 75.000 (60.903)\nEpoch: [184][600/665]\\tLoss 5.3880 (6.8420)\\tPrec@1 50.000 (60.857)\nEpoch: [184][664/665]\\tLoss 0.4295 (6.7963)\\tPrec@1 0.000 (60.870)\n * Prec@1 43.850\nEpoch: [185][0/665]\\tLoss 2.7952 (2.7952)\\tPrec@1 50.000 (50.000)\nEpoch: [185][100/665]\\tLoss 4.2322 (6.7785)\\tPrec@1 56.250 (61.139)\nEpoch: [185][200/665]\\tLoss 2.7699 (6.4543)\\tPrec@1 68.750 (61.007)\nEpoch: [185][300/665]\\tLoss 3.4249 (6.5099)\\tPrec@1 43.750 (60.486)\nEpoch: [185][400/665]\\tLoss 6.5439 (6.5705)\\tPrec@1 62.500 (60.380)\nEpoch: [185][500/665]\\tLoss 6.5288 (6.6497)\\tPrec@1 37.500 (60.354)\nEpoch: [185][600/665]\\tLoss 6.1107 (6.8583)\\tPrec@1 50.000 (59.713)\nEpoch: [185][664/665]\\tLoss 4.2189 (6.8313)\\tPrec@1 0.000 (59.910)\n * Prec@1 44.100\nEpoch: [186][0/665]\\tLoss 7.7593 (7.7593)\\tPrec@1 75.000 (75.000)\nEpoch: [186][100/665]\\tLoss 1.6459 (6.2777)\\tPrec@1 81.250 (61.386)\nEpoch: [186][200/665]\\tLoss 3.7387 (6.6891)\\tPrec@1 68.750 (60.728)\nEpoch: [186][300/665]\\tLoss 5.6098 (6.3875)\\tPrec@1 50.000 (60.527)\nEpoch: [186][400/665]\\tLoss 16.8923 (6.4094)\\tPrec@1 56.250 (60.521)\nEpoch: [186][500/665]\\tLoss 8.0381 (6.5881)\\tPrec@1 62.500 (59.918)\nEpoch: [186][600/665]\\tLoss 2.1264 (6.5507)\\tPrec@1 75.000 (60.306)\nEpoch: [186][664/665]\\tLoss 2.2966 (6.6196)\\tPrec@1 50.000 (60.352)\n * Prec@1 42.640\nEpoch: [187][0/665]\\tLoss 2.1863 (2.1863)\\tPrec@1 68.750 (68.750)\nEpoch: [187][100/665]\\tLoss 2.1800 (7.1965)\\tPrec@1 75.000 (60.705)\nEpoch: [187][200/665]\\tLoss 23.4896 (6.8882)\\tPrec@1 75.000 (60.106)\nEpoch: [187][300/665]\\tLoss 4.8034 (6.5985)\\tPrec@1 68.750 (60.922)\nEpoch: [187][400/665]\\tLoss 1.5771 (6.6909)\\tPrec@1 68.750 (60.661)\nEpoch: [187][500/665]\\tLoss 6.6905 (6.6650)\\tPrec@1 56.250 (60.467)\nEpoch: [187][600/665]\\tLoss 3.8443 (6.7054)\\tPrec@1 62.500 (60.233)\nEpoch: [187][664/665]\\tLoss 4.7067 (6.7362)\\tPrec@1 0.000 (60.211)\n * Prec@1 44.040\nEpoch: [188][0/665]\\tLoss 16.8780 (16.8780)\\tPrec@1 50.000 (50.000)\nEpoch: [188][100/665]\\tLoss 8.5081 (7.4888)\\tPrec@1 68.750 (58.230)\nEpoch: [188][200/665]\\tLoss 3.5843 (7.0291)\\tPrec@1 43.750 (58.582)\nEpoch: [188][300/665]\\tLoss 7.6758 (6.8531)\\tPrec@1 68.750 (58.846)\nEpoch: [188][400/665]\\tLoss 2.4583 (6.5963)\\tPrec@1 50.000 (59.445)\nEpoch: [188][500/665]\\tLoss 3.6967 (6.5858)\\tPrec@1 75.000 (59.543)\nEpoch: [188][600/665]\\tLoss 17.6307 (6.6163)\\tPrec@1 62.500 (59.713)\nEpoch: [188][664/665]\\tLoss 0.8935 (6.7341)\\tPrec@1 50.000 (59.571)\n * Prec@1 43.680\nEpoch: [189][0/665]\\tLoss 11.3477 (11.3477)\\tPrec@1 43.750 (43.750)\nEpoch: [189][100/665]\\tLoss 10.7921 (6.2306)\\tPrec@1 56.250 (59.344)\nEpoch: [189][200/665]\\tLoss 7.2744 (6.5564)\\tPrec@1 62.500 (59.204)\nEpoch: [189][300/665]\\tLoss 8.6793 (6.5723)\\tPrec@1 50.000 (59.032)\nEpoch: [189][400/665]\\tLoss 8.4552 (6.8175)\\tPrec@1 75.000 (59.024)\nEpoch: [189][500/665]\\tLoss 2.4226 (6.7440)\\tPrec@1 68.750 (59.119)\nEpoch: [189][600/665]\\tLoss 2.4350 (6.7991)\\tPrec@1 62.500 (59.224)\nEpoch: [189][664/665]\\tLoss 0.5002 (6.7196)\\tPrec@1 0.000 (59.401)\n * Prec@1 44.400\nEpoch: [190][0/665]\\tLoss 30.1526 (30.1526)\\tPrec@1 43.750 (43.750)\nEpoch: [190][100/665]\\tLoss 8.3625 (6.3229)\\tPrec@1 75.000 (58.168)\nEpoch: [190][200/665]\\tLoss 11.5429 (6.6527)\\tPrec@1 68.750 (59.453)\nEpoch: [190][300/665]\\tLoss 25.8040 (6.4307)\\tPrec@1 50.000 (59.489)\nEpoch: [190][400/665]\\tLoss 7.3874 (6.5040)\\tPrec@1 50.000 (59.570)\nEpoch: [190][500/665]\\tLoss 6.2345 (6.6493)\\tPrec@1 62.500 (59.793)\nEpoch: [190][600/665]\\tLoss 4.4576 (6.8914)\\tPrec@1 75.000 (59.463)\nEpoch: [190][664/665]\\tLoss 0.4649 (6.8346)\\tPrec@1 0.000 (59.289)\n * Prec@1 44.310\nEpoch: [191][0/665]\\tLoss 8.3680 (8.3680)\\tPrec@1 62.500 (62.500)\nEpoch: [191][100/665]\\tLoss 3.2988 (6.5963)\\tPrec@1 68.750 (60.210)\nEpoch: [191][200/665]\\tLoss 3.3928 (6.7580)\\tPrec@1 62.500 (58.675)\nEpoch: [191][300/665]\\tLoss 6.1705 (6.6037)\\tPrec@1 75.000 (58.617)\nEpoch: [191][400/665]\\tLoss 9.4848 (6.6231)\\tPrec@1 62.500 (58.713)\nEpoch: [191][500/665]\\tLoss 6.1620 (6.5914)\\tPrec@1 68.750 (59.144)\nEpoch: [191][600/665]\\tLoss 8.7855 (6.7897)\\tPrec@1 62.500 (59.349)\nEpoch: [191][664/665]\\tLoss 0.6926 (6.6846)\\tPrec@1 0.000 (59.580)\n * Prec@1 43.530\nEpoch: [192][0/665]\\tLoss 2.6725 (2.6725)\\tPrec@1 62.500 (62.500)\nEpoch: [192][100/665]\\tLoss 5.9748 (6.0312)\\tPrec@1 62.500 (60.767)\nEpoch: [192][200/665]\\tLoss 5.0578 (6.3821)\\tPrec@1 81.250 (60.603)\nEpoch: [192][300/665]\\tLoss 20.4186 (6.4523)\\tPrec@1 56.250 (60.195)\nEpoch: [192][400/665]\\tLoss 2.6957 (6.5654)\\tPrec@1 56.250 (60.022)\nEpoch: [192][500/665]\\tLoss 24.2083 (6.5811)\\tPrec@1 50.000 (59.918)\nEpoch: [192][600/665]\\tLoss 8.0557 (6.5358)\\tPrec@1 62.500 (59.921)\nEpoch: [192][664/665]\\tLoss 0.9351 (6.5789)\\tPrec@1 0.000 (59.834)\n * Prec@1 44.540\nEpoch: [193][0/665]\\tLoss 4.2804 (4.2804)\\tPrec@1 62.500 (62.500)\nEpoch: [193][100/665]\\tLoss 3.4728 (6.7041)\\tPrec@1 81.250 (59.406)\nEpoch: [193][200/665]\\tLoss 5.2887 (6.8902)\\tPrec@1 68.750 (59.080)\nEpoch: [193][300/665]\\tLoss 4.6966 (6.4327)\\tPrec@1 62.500 (60.071)\nEpoch: [193][400/665]\\tLoss 11.0391 (6.4769)\\tPrec@1 56.250 (60.536)\nEpoch: [193][500/665]\\tLoss 5.7238 (6.5161)\\tPrec@1 62.500 (60.541)\nEpoch: [193][600/665]\\tLoss 3.8097 (6.5417)\\tPrec@1 50.000 (60.368)\nEpoch: [193][664/665]\\tLoss 0.3975 (6.5551)\\tPrec@1 50.000 (60.258)\n * Prec@1 43.850\nEpoch: [194][0/665]\\tLoss 2.2298 (2.2298)\\tPrec@1 68.750 (68.750)\nEpoch: [194][100/665]\\tLoss 6.8070 (6.1553)\\tPrec@1 68.750 (61.139)\nEpoch: [194][200/665]\\tLoss 17.9752 (6.4446)\\tPrec@1 43.750 (60.261)\nEpoch: [194][300/665]\\tLoss 5.8374 (6.5857)\\tPrec@1 56.250 (60.008)\nEpoch: [194][400/665]\\tLoss 4.0800 (6.4476)\\tPrec@1 62.500 (60.193)\nEpoch: [194][500/665]\\tLoss 33.8713 (6.5891)\\tPrec@1 62.500 (59.968)\nEpoch: [194][600/665]\\tLoss 11.8918 (6.5239)\\tPrec@1 68.750 (60.129)\nEpoch: [194][664/665]\\tLoss 1.1264 (6.5651)\\tPrec@1 0.000 (60.079)\n * Prec@1 44.430\nEpoch: [195][0/665]\\tLoss 3.8822 (3.8822)\\tPrec@1 50.000 (50.000)\nEpoch: [195][100/665]\\tLoss 5.6205 (6.8924)\\tPrec@1 50.000 (58.416)\nEpoch: [195][200/665]\\tLoss 7.3261 (6.7059)\\tPrec@1 43.750 (59.359)\nEpoch: [195][300/665]\\tLoss 1.3159 (6.3592)\\tPrec@1 87.500 (59.468)\nEpoch: [195][400/665]\\tLoss 5.6510 (6.5874)\\tPrec@1 31.250 (59.305)\nEpoch: [195][500/665]\\tLoss 9.8189 (6.5637)\\tPrec@1 56.250 (59.232)\nEpoch: [195][600/665]\\tLoss 3.7080 (6.7130)\\tPrec@1 56.250 (58.995)\nEpoch: [195][664/665]\\tLoss 1.0008 (6.7182)\\tPrec@1 50.000 (59.119)\n * Prec@1 44.160\nEpoch: [196][0/665]\\tLoss 3.2613 (3.2613)\\tPrec@1 56.250 (56.250)\nEpoch: [196][100/665]\\tLoss 3.0051 (6.5644)\\tPrec@1 75.000 (58.106)\nEpoch: [196][200/665]\\tLoss 7.4452 (6.3222)\\tPrec@1 43.750 (60.137)\nEpoch: [196][300/665]\\tLoss 3.0587 (6.4044)\\tPrec@1 68.750 (60.154)\nEpoch: [196][400/665]\\tLoss 4.8903 (6.4854)\\tPrec@1 56.250 (60.053)\nEpoch: [196][500/665]\\tLoss 14.4657 (6.4913)\\tPrec@1 50.000 (60.454)\nEpoch: [196][600/665]\\tLoss 5.3493 (6.4172)\\tPrec@1 81.250 (60.171)\nEpoch: [196][664/665]\\tLoss 0.5675 (6.5859)\\tPrec@1 0.000 (59.919)\n * Prec@1 43.980\nEpoch: [197][0/665]\\tLoss 7.8033 (7.8033)\\tPrec@1 31.250 (31.250)\nEpoch: [197][100/665]\\tLoss 2.6947 (6.3022)\\tPrec@1 75.000 (59.406)\nEpoch: [197][200/665]\\tLoss 5.8985 (6.1842)\\tPrec@1 81.250 (59.950)\nEpoch: [197][300/665]\\tLoss 4.6339 (6.4826)\\tPrec@1 56.250 (60.237)\nEpoch: [197][400/665]\\tLoss 9.1616 (6.5300)\\tPrec@1 68.750 (60.240)\nEpoch: [197][500/665]\\tLoss 5.1882 (6.5839)\\tPrec@1 43.750 (59.993)\nEpoch: [197][600/665]\\tLoss 8.6197 (6.5679)\\tPrec@1 62.500 (60.046)\nEpoch: [197][664/665]\\tLoss 0.6103 (6.5818)\\tPrec@1 0.000 (59.957)\n * Prec@1 44.280\nEpoch: [198][0/665]\\tLoss 2.8288 (2.8288)\\tPrec@1 62.500 (62.500)\nEpoch: [198][100/665]\\tLoss 3.3135 (8.5076)\\tPrec@1 56.250 (57.550)\nEpoch: [198][200/665]\\tLoss 10.0302 (7.2600)\\tPrec@1 68.750 (59.297)\nEpoch: [198][300/665]\\tLoss 32.7654 (6.9899)\\tPrec@1 31.250 (59.448)\nEpoch: [198][400/665]\\tLoss 1.7371 (6.5907)\\tPrec@1 68.750 (59.757)\nEpoch: [198][500/665]\\tLoss 1.8621 (6.6250)\\tPrec@1 56.250 (59.419)\nEpoch: [198][600/665]\\tLoss 7.6274 (6.5389)\\tPrec@1 68.750 (59.682)\nEpoch: [198][664/665]\\tLoss 5.2131 (6.5061)\\tPrec@1 0.000 (59.712)\n * Prec@1 44.200\nEpoch: [199][0/665]\\tLoss 4.3681 (4.3681)\\tPrec@1 43.750 (43.750)\nEpoch: [199][100/665]\\tLoss 5.9446 (5.8562)\\tPrec@1 62.500 (61.881)\nEpoch: [199][200/665]\\tLoss 10.0986 (5.9540)\\tPrec@1 62.500 (60.417)\nEpoch: [199][300/665]\\tLoss 11.6378 (6.0466)\\tPrec@1 56.250 (60.403)\nEpoch: [199][400/665]\\tLoss 5.5295 (6.2869)\\tPrec@1 43.750 (59.928)\nEpoch: [199][500/665]\\tLoss 2.4404 (6.5472)\\tPrec@1 62.500 (59.718)\nEpoch: [199][600/665]\\tLoss 21.3605 (6.6363)\\tPrec@1 37.500 (60.035)\nEpoch: [199][664/665]\\tLoss 0.2837 (6.5654)\\tPrec@1 50.000 (59.976)\n * Prec@1 44.250\nBest accuracy:  44.54\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Stage 1 (IF=50)\n!python pretrain_stage1.py --dataset cifar100 --imb_factor 0.02 --epochs 200\n\n# Stage 2 (IF=50)\n!python OT_train.py \\\n--dataset cifar100 \\\n--num_classes 100 \\\n--imb_factor 0.02 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar100_IF50_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar100_imb0.02_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:05:07.367392Z","iopub.execute_input":"2025-06-15T09:05:07.367657Z","iopub.status.idle":"2025-06-15T09:36:50.295518Z","shell.execute_reply.started":"2025-06-15T09:05:07.367630Z","shell.execute_reply":"2025-06-15T09:36:50.294612Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|██████████████████| 97/97 [00:06<00:00, 15.76it/s, Loss=3.673]\nEpoch 2/200: 100%|██████████████████| 97/97 [00:04<00:00, 20.02it/s, Loss=3.547]\nEpoch 3/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.97it/s, Loss=3.003]\nEpoch 4/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=3.318]\nEpoch 5/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.44it/s, Loss=2.929]\nEpoch 6/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.55it/s, Loss=3.008]\nEpoch 7/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.61it/s, Loss=3.460]\nEpoch 8/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.61it/s, Loss=2.421]\nEpoch 9/200: 100%|██████████████████| 97/97 [00:04<00:00, 19.68it/s, Loss=3.085]\nEpoch 10/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=2.731]\nEpoch 11/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.75it/s, Loss=2.464]\nEpoch 12/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.98it/s, Loss=2.274]\nEpoch 13/200: 100%|█████████████████| 97/97 [00:04<00:00, 20.00it/s, Loss=2.015]\nEpoch 14/200: 100%|█████████████████| 97/97 [00:04<00:00, 20.07it/s, Loss=2.013]\nEpoch 15/200: 100%|█████████████████| 97/97 [00:04<00:00, 20.03it/s, Loss=2.038]\nEpoch 16/200: 100%|█████████████████| 97/97 [00:04<00:00, 20.03it/s, Loss=2.025]\nEpoch 17/200: 100%|█████████████████| 97/97 [00:04<00:00, 20.01it/s, Loss=1.668]\nEpoch 18/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.585]\nEpoch 19/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.93it/s, Loss=1.984]\nEpoch 20/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.93it/s, Loss=1.949]\nEpoch 21/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=2.201]\nEpoch 22/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.685]\nEpoch 23/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=1.899]\nEpoch 24/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.66it/s, Loss=1.693]\nEpoch 25/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=2.114]\nEpoch 26/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.778]\nEpoch 27/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.714]\nEpoch 28/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.89it/s, Loss=1.650]\nEpoch 29/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.87it/s, Loss=1.518]\nEpoch 30/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.90it/s, Loss=1.868]\nEpoch 31/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.474]\nEpoch 32/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=1.512]\nEpoch 33/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.460]\nEpoch 34/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.292]\nEpoch 35/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.165]\nEpoch 36/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=1.691]\nEpoch 37/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.57it/s, Loss=1.442]\nEpoch 38/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.54it/s, Loss=1.580]\nEpoch 39/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.75it/s, Loss=1.634]\nEpoch 40/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=1.400]\nEpoch 41/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=1.248]\nEpoch 42/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.021]\nEpoch 43/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.67it/s, Loss=1.338]\nEpoch 44/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.58it/s, Loss=1.567]\nEpoch 45/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.75it/s, Loss=1.286]\nEpoch 46/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.64it/s, Loss=1.312]\nEpoch 47/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=1.070]\nEpoch 48/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.64it/s, Loss=1.344]\nEpoch 49/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.71it/s, Loss=1.131]\nEpoch 50/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.61it/s, Loss=1.264]\nEpoch 51/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.67it/s, Loss=1.410]\nEpoch 52/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=1.433]\nEpoch 53/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.264]\nEpoch 54/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.541]\nEpoch 55/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.020]\nEpoch 56/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.100]\nEpoch 57/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=1.281]\nEpoch 58/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.215]\nEpoch 59/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.88it/s, Loss=1.145]\nEpoch 60/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.219]\nEpoch 61/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.62it/s, Loss=1.086]\nEpoch 62/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=1.237]\nEpoch 63/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.45it/s, Loss=0.736]\nEpoch 64/200: 100%|█████████████████| 97/97 [00:05<00:00, 19.24it/s, Loss=1.390]\nEpoch 65/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.88it/s, Loss=1.047]\nEpoch 66/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=0.760]\nEpoch 67/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.158]\nEpoch 68/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=0.930]\nEpoch 69/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.72it/s, Loss=1.177]\nEpoch 70/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.63it/s, Loss=1.196]\nEpoch 71/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=1.051]\nEpoch 72/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.73it/s, Loss=0.865]\nEpoch 73/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=0.897]\nEpoch 74/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.74it/s, Loss=0.890]\nEpoch 75/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.71it/s, Loss=1.181]\nEpoch 76/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=1.011]\nEpoch 77/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.52it/s, Loss=0.864]\nEpoch 78/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.907]\nEpoch 79/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.938]\nEpoch 80/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=1.043]\nEpoch 81/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=1.162]\nEpoch 82/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=1.224]\nEpoch 83/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=1.066]\nEpoch 84/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.90it/s, Loss=0.775]\nEpoch 85/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.952]\nEpoch 86/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.930]\nEpoch 87/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=0.830]\nEpoch 88/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=1.063]\nEpoch 89/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.88it/s, Loss=1.081]\nEpoch 90/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.68it/s, Loss=1.023]\nEpoch 91/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=1.261]\nEpoch 92/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.057]\nEpoch 93/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.855]\nEpoch 94/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=1.337]\nEpoch 95/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.091]\nEpoch 96/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.65it/s, Loss=0.804]\nEpoch 97/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.87it/s, Loss=1.400]\nEpoch 98/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=0.880]\nEpoch 99/200: 100%|█████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=0.951]\nEpoch 100/200: 100%|████████████████| 97/97 [00:04<00:00, 19.74it/s, Loss=1.115]\nEpoch 101/200: 100%|████████████████| 97/97 [00:04<00:00, 19.64it/s, Loss=0.946]\nEpoch 102/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.940]\nEpoch 103/200: 100%|████████████████| 97/97 [00:04<00:00, 19.70it/s, Loss=0.968]\nEpoch 104/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.820]\nEpoch 105/200: 100%|████████████████| 97/97 [00:04<00:00, 19.72it/s, Loss=0.837]\nEpoch 106/200: 100%|████████████████| 97/97 [00:04<00:00, 19.72it/s, Loss=1.236]\nEpoch 107/200: 100%|████████████████| 97/97 [00:04<00:00, 19.59it/s, Loss=1.087]\nEpoch 108/200: 100%|████████████████| 97/97 [00:04<00:00, 19.64it/s, Loss=0.910]\nEpoch 109/200: 100%|████████████████| 97/97 [00:04<00:00, 19.70it/s, Loss=0.730]\nEpoch 110/200: 100%|████████████████| 97/97 [00:04<00:00, 19.69it/s, Loss=0.722]\nEpoch 111/200: 100%|████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=0.888]\nEpoch 112/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.704]\nEpoch 113/200: 100%|████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=0.867]\nEpoch 114/200: 100%|████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=1.045]\nEpoch 115/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.822]\nEpoch 116/200: 100%|████████████████| 97/97 [00:04<00:00, 19.72it/s, Loss=0.820]\nEpoch 117/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=0.909]\nEpoch 118/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.839]\nEpoch 119/200: 100%|████████████████| 97/97 [00:04<00:00, 19.78it/s, Loss=1.059]\nEpoch 120/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=1.272]\nEpoch 121/200: 100%|████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=0.952]\nEpoch 122/200: 100%|████████████████| 97/97 [00:04<00:00, 19.73it/s, Loss=0.867]\nEpoch 123/200: 100%|████████████████| 97/97 [00:04<00:00, 19.62it/s, Loss=1.109]\nEpoch 124/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=1.050]\nEpoch 125/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.947]\nEpoch 126/200: 100%|████████████████| 97/97 [00:04<00:00, 19.88it/s, Loss=0.793]\nEpoch 127/200: 100%|████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=0.897]\nEpoch 128/200: 100%|████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=0.816]\nEpoch 129/200: 100%|████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=0.748]\nEpoch 130/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.725]\nEpoch 131/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.723]\nEpoch 132/200: 100%|████████████████| 97/97 [00:04<00:00, 19.87it/s, Loss=0.894]\nEpoch 133/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=1.205]\nEpoch 134/200: 100%|████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=1.084]\nEpoch 135/200: 100%|████████████████| 97/97 [00:04<00:00, 19.74it/s, Loss=0.958]\nEpoch 136/200: 100%|████████████████| 97/97 [00:04<00:00, 19.74it/s, Loss=1.249]\nEpoch 137/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=1.209]\nEpoch 138/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=1.216]\nEpoch 139/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.858]\nEpoch 140/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=1.221]\nEpoch 141/200: 100%|████████████████| 97/97 [00:04<00:00, 19.88it/s, Loss=1.154]\nEpoch 142/200: 100%|████████████████| 97/97 [00:04<00:00, 19.71it/s, Loss=1.361]\nEpoch 143/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.281]\nEpoch 144/200: 100%|████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.866]\nEpoch 145/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=1.128]\nEpoch 146/200: 100%|████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=1.087]\nEpoch 147/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.856]\nEpoch 148/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.964]\nEpoch 149/200: 100%|████████████████| 97/97 [00:04<00:00, 19.49it/s, Loss=0.979]\nEpoch 150/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=1.174]\nEpoch 151/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=0.748]\nEpoch 152/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.924]\nEpoch 153/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.762]\nEpoch 154/200: 100%|████████████████| 97/97 [00:04<00:00, 19.79it/s, Loss=0.902]\nEpoch 155/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=1.334]\nEpoch 156/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.578]\nEpoch 157/200: 100%|████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=1.081]\nEpoch 158/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.993]\nEpoch 159/200: 100%|████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=1.048]\nEpoch 160/200: 100%|████████████████| 97/97 [00:04<00:00, 19.77it/s, Loss=0.876]\nEpoch 161/200: 100%|████████████████| 97/97 [00:04<00:00, 19.72it/s, Loss=0.445]\nEpoch 162/200: 100%|████████████████| 97/97 [00:05<00:00, 19.36it/s, Loss=0.433]\nEpoch 163/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.361]\nEpoch 164/200: 100%|████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=0.372]\nEpoch 165/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.305]\nEpoch 166/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=0.346]\nEpoch 167/200: 100%|████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.411]\nEpoch 168/200: 100%|████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=0.218]\nEpoch 169/200: 100%|████████████████| 97/97 [00:04<00:00, 19.87it/s, Loss=0.151]\nEpoch 170/200: 100%|████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=0.247]\nEpoch 171/200: 100%|████████████████| 97/97 [00:04<00:00, 19.89it/s, Loss=0.197]\nEpoch 172/200: 100%|████████████████| 97/97 [00:04<00:00, 19.96it/s, Loss=0.220]\nEpoch 173/200: 100%|████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=0.270]\nEpoch 174/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.186]\nEpoch 175/200: 100%|████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.161]\nEpoch 176/200: 100%|████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=0.164]\nEpoch 177/200: 100%|████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=0.213]\nEpoch 178/200: 100%|████████████████| 97/97 [00:04<00:00, 19.91it/s, Loss=0.219]\nEpoch 179/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=0.124]\nEpoch 180/200: 100%|████████████████| 97/97 [00:04<00:00, 19.83it/s, Loss=0.119]\nEpoch 181/200: 100%|████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=0.160]\nEpoch 182/200: 100%|████████████████| 97/97 [00:04<00:00, 19.70it/s, Loss=0.082]\nEpoch 183/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=0.204]\nEpoch 184/200: 100%|████████████████| 97/97 [00:04<00:00, 19.89it/s, Loss=0.132]\nEpoch 185/200: 100%|████████████████| 97/97 [00:04<00:00, 19.89it/s, Loss=0.158]\nEpoch 186/200: 100%|████████████████| 97/97 [00:04<00:00, 19.81it/s, Loss=0.240]\nEpoch 187/200: 100%|████████████████| 97/97 [00:04<00:00, 19.92it/s, Loss=0.219]\nEpoch 188/200: 100%|████████████████| 97/97 [00:04<00:00, 19.75it/s, Loss=0.122]\nEpoch 189/200: 100%|████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=0.098]\nEpoch 190/200: 100%|████████████████| 97/97 [00:04<00:00, 19.87it/s, Loss=0.130]\nEpoch 191/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.160]\nEpoch 192/200: 100%|████████████████| 97/97 [00:04<00:00, 19.86it/s, Loss=0.118]\nEpoch 193/200: 100%|████████████████| 97/97 [00:04<00:00, 19.82it/s, Loss=0.115]\nEpoch 194/200: 100%|████████████████| 97/97 [00:04<00:00, 19.70it/s, Loss=0.086]\nEpoch 195/200: 100%|████████████████| 97/97 [00:04<00:00, 19.53it/s, Loss=0.145]\nEpoch 196/200: 100%|████████████████| 97/97 [00:04<00:00, 19.75it/s, Loss=0.127]\nEpoch 197/200: 100%|████████████████| 97/97 [00:04<00:00, 19.85it/s, Loss=0.203]\nEpoch 198/200: 100%|████████████████| 97/97 [00:04<00:00, 19.76it/s, Loss=0.175]\nEpoch 199/200: 100%|████████████████| 97/97 [00:04<00:00, 19.84it/s, Loss=0.157]\nEpoch 200/200: 100%|████████████████| 97/97 [00:04<00:00, 19.80it/s, Loss=0.090]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar100_imb0.02_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\nFile 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar100\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=100\nnum_meta=10\nimb_factor=0.02\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar100_IF50_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar100_imb0.02_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/773]\\tLoss 21.0584 (21.0584)\\tPrec@1 68.750 (68.750)\nEpoch: [160][100/773]\\tLoss 10.9734 (10.6254)\\tPrec@1 81.250 (74.196)\nEpoch: [160][200/773]\\tLoss 5.4586 (10.0401)\\tPrec@1 87.500 (73.912)\nEpoch: [160][300/773]\\tLoss 2.8776 (10.1619)\\tPrec@1 81.250 (73.754)\nEpoch: [160][400/773]\\tLoss 21.5860 (10.4229)\\tPrec@1 81.250 (73.893)\nEpoch: [160][500/773]\\tLoss 2.7176 (10.5902)\\tPrec@1 93.750 (73.927)\nEpoch: [160][600/773]\\tLoss 2.6602 (10.7511)\\tPrec@1 62.500 (73.638)\nEpoch: [160][700/773]\\tLoss 5.8407 (11.0429)\\tPrec@1 75.000 (73.413)\nEpoch: [160][772/773]\\tLoss 2.1324 (10.9432)\\tPrec@1 0.000 (73.353)\n * Prec@1 48.390\nEpoch: [161][0/773]\\tLoss 2.2259 (2.2259)\\tPrec@1 87.500 (87.500)\nEpoch: [161][100/773]\\tLoss 31.9231 (9.5501)\\tPrec@1 50.000 (72.525)\nEpoch: [161][200/773]\\tLoss 21.2336 (9.8651)\\tPrec@1 87.500 (72.948)\nEpoch: [161][300/773]\\tLoss 6.1049 (10.0877)\\tPrec@1 68.750 (73.339)\nEpoch: [161][400/773]\\tLoss 18.4492 (10.4510)\\tPrec@1 75.000 (72.740)\nEpoch: [161][500/773]\\tLoss 13.8057 (10.3853)\\tPrec@1 56.250 (72.730)\nEpoch: [161][600/773]\\tLoss 12.5134 (10.1726)\\tPrec@1 56.250 (72.587)\nEpoch: [161][700/773]\\tLoss 24.6417 (10.1755)\\tPrec@1 62.500 (72.307)\nEpoch: [161][772/773]\\tLoss 0.8297 (10.2041)\\tPrec@1 0.000 (72.381)\n * Prec@1 49.630\nEpoch: [162][0/773]\\tLoss 3.3457 (3.3457)\\tPrec@1 87.500 (87.500)\nEpoch: [162][100/773]\\tLoss 38.9095 (10.8246)\\tPrec@1 56.250 (71.411)\nEpoch: [162][200/773]\\tLoss 5.0602 (10.3949)\\tPrec@1 56.250 (71.175)\nEpoch: [162][300/773]\\tLoss 5.4956 (10.3756)\\tPrec@1 62.500 (71.200)\nEpoch: [162][400/773]\\tLoss 5.6376 (9.9249)\\tPrec@1 62.500 (71.883)\nEpoch: [162][500/773]\\tLoss 8.2759 (9.8178)\\tPrec@1 75.000 (71.632)\nEpoch: [162][600/773]\\tLoss 3.6405 (9.8610)\\tPrec@1 87.500 (71.454)\nEpoch: [162][700/773]\\tLoss 5.5811 (9.8303)\\tPrec@1 68.750 (71.692)\nEpoch: [162][772/773]\\tLoss 5.6899 (9.7638)\\tPrec@1 0.000 (71.855)\n * Prec@1 49.680\nEpoch: [163][0/773]\\tLoss 3.0992 (3.0992)\\tPrec@1 75.000 (75.000)\nEpoch: [163][100/773]\\tLoss 10.2281 (9.5825)\\tPrec@1 68.750 (72.215)\nEpoch: [163][200/773]\\tLoss 5.7357 (9.8779)\\tPrec@1 68.750 (71.797)\nEpoch: [163][300/773]\\tLoss 14.7768 (9.9861)\\tPrec@1 68.750 (71.449)\nEpoch: [163][400/773]\\tLoss 23.1128 (10.1852)\\tPrec@1 75.000 (71.041)\nEpoch: [163][500/773]\\tLoss 8.1637 (9.9902)\\tPrec@1 68.750 (71.108)\nEpoch: [163][600/773]\\tLoss 6.3805 (9.6200)\\tPrec@1 81.250 (71.090)\nEpoch: [163][700/773]\\tLoss 2.0522 (9.5938)\\tPrec@1 81.250 (70.872)\nEpoch: [163][772/773]\\tLoss 0.8614 (9.5331)\\tPrec@1 50.000 (70.690)\n * Prec@1 50.560\nEpoch: [164][0/773]\\tLoss 3.3512 (3.3512)\\tPrec@1 75.000 (75.000)\nEpoch: [164][100/773]\\tLoss 8.6877 (9.3862)\\tPrec@1 87.500 (71.411)\nEpoch: [164][200/773]\\tLoss 16.8043 (9.2449)\\tPrec@1 56.250 (71.362)\nEpoch: [164][300/773]\\tLoss 14.4305 (9.6588)\\tPrec@1 68.750 (70.473)\nEpoch: [164][400/773]\\tLoss 2.6032 (9.5099)\\tPrec@1 68.750 (69.623)\nEpoch: [164][500/773]\\tLoss 10.1169 (9.2736)\\tPrec@1 93.750 (69.798)\nEpoch: [164][600/773]\\tLoss 4.3182 (9.2542)\\tPrec@1 68.750 (70.008)\nEpoch: [164][700/773]\\tLoss 7.5756 (9.2274)\\tPrec@1 81.250 (69.829)\nEpoch: [164][772/773]\\tLoss 0.4714 (9.3362)\\tPrec@1 50.000 (69.565)\n * Prec@1 50.700\nEpoch: [165][0/773]\\tLoss 7.9251 (7.9251)\\tPrec@1 62.500 (62.500)\nEpoch: [165][100/773]\\tLoss 17.7869 (9.8785)\\tPrec@1 62.500 (69.121)\nEpoch: [165][200/773]\\tLoss 17.1948 (9.5736)\\tPrec@1 62.500 (69.216)\nEpoch: [165][300/773]\\tLoss 11.7093 (9.4394)\\tPrec@1 62.500 (69.020)\nEpoch: [165][400/773]\\tLoss 13.3880 (9.1156)\\tPrec@1 56.250 (69.327)\nEpoch: [165][500/773]\\tLoss 4.0548 (9.2212)\\tPrec@1 75.000 (69.212)\nEpoch: [165][600/773]\\tLoss 5.4446 (9.2358)\\tPrec@1 87.500 (69.228)\nEpoch: [165][700/773]\\tLoss 8.6898 (9.3174)\\tPrec@1 56.250 (69.062)\nEpoch: [165][772/773]\\tLoss 1.0842 (9.2905)\\tPrec@1 50.000 (68.974)\n * Prec@1 51.330\nEpoch: [166][0/773]\\tLoss 3.8545 (3.8545)\\tPrec@1 56.250 (56.250)\nEpoch: [166][100/773]\\tLoss 7.4632 (9.2338)\\tPrec@1 43.750 (67.636)\nEpoch: [166][200/773]\\tLoss 9.3263 (9.7600)\\tPrec@1 56.250 (67.910)\nEpoch: [166][300/773]\\tLoss 17.9537 (9.6244)\\tPrec@1 75.000 (68.169)\nEpoch: [166][400/773]\\tLoss 11.2272 (9.5874)\\tPrec@1 68.750 (68.360)\nEpoch: [166][500/773]\\tLoss 24.2860 (9.7334)\\tPrec@1 56.250 (68.451)\nEpoch: [166][600/773]\\tLoss 2.7561 (9.4211)\\tPrec@1 75.000 (68.563)\nEpoch: [166][700/773]\\tLoss 1.5355 (9.3253)\\tPrec@1 93.750 (68.732)\nEpoch: [166][772/773]\\tLoss 0.8808 (9.2443)\\tPrec@1 0.000 (68.577)\n * Prec@1 51.470\nEpoch: [167][0/773]\\tLoss 23.2747 (23.2747)\\tPrec@1 62.500 (62.500)\nEpoch: [167][100/773]\\tLoss 8.1709 (8.7961)\\tPrec@1 81.250 (69.926)\nEpoch: [167][200/773]\\tLoss 5.6300 (8.4256)\\tPrec@1 68.750 (69.590)\nEpoch: [167][300/773]\\tLoss 10.4535 (8.7486)\\tPrec@1 68.750 (69.020)\nEpoch: [167][400/773]\\tLoss 9.1968 (8.9355)\\tPrec@1 81.250 (68.875)\nEpoch: [167][500/773]\\tLoss 3.1520 (8.8238)\\tPrec@1 81.250 (69.124)\nEpoch: [167][600/773]\\tLoss 10.3312 (9.0904)\\tPrec@1 56.250 (69.041)\nEpoch: [167][700/773]\\tLoss 7.1546 (9.1155)\\tPrec@1 75.000 (68.866)\nEpoch: [167][772/773]\\tLoss 1.1698 (9.1244)\\tPrec@1 0.000 (68.779)\n * Prec@1 50.820\nEpoch: [168][0/773]\\tLoss 9.0270 (9.0270)\\tPrec@1 50.000 (50.000)\nEpoch: [168][100/773]\\tLoss 6.7426 (9.7879)\\tPrec@1 68.750 (67.265)\nEpoch: [168][200/773]\\tLoss 16.0780 (9.0191)\\tPrec@1 62.500 (68.315)\nEpoch: [168][300/773]\\tLoss 8.6621 (9.0421)\\tPrec@1 75.000 (68.480)\nEpoch: [168][400/773]\\tLoss 18.3392 (9.0804)\\tPrec@1 62.500 (68.423)\nEpoch: [168][500/773]\\tLoss 5.0195 (8.9289)\\tPrec@1 62.500 (68.513)\nEpoch: [168][600/773]\\tLoss 3.5755 (8.8396)\\tPrec@1 81.250 (68.636)\nEpoch: [168][700/773]\\tLoss 29.6285 (9.0078)\\tPrec@1 62.500 (68.242)\nEpoch: [168][772/773]\\tLoss 0.6586 (9.0046)\\tPrec@1 0.000 (68.051)\n * Prec@1 51.090\nEpoch: [169][0/773]\\tLoss 1.9032 (1.9032)\\tPrec@1 75.000 (75.000)\nEpoch: [169][100/773]\\tLoss 7.7713 (8.1312)\\tPrec@1 87.500 (68.998)\nEpoch: [169][200/773]\\tLoss 6.4172 (8.7259)\\tPrec@1 56.250 (68.563)\nEpoch: [169][300/773]\\tLoss 3.0026 (8.8218)\\tPrec@1 75.000 (67.899)\nEpoch: [169][400/773]\\tLoss 5.1123 (8.9538)\\tPrec@1 75.000 (67.846)\nEpoch: [169][500/773]\\tLoss 7.7604 (9.0541)\\tPrec@1 68.750 (67.540)\nEpoch: [169][600/773]\\tLoss 3.4527 (9.2048)\\tPrec@1 75.000 (67.554)\nEpoch: [169][700/773]\\tLoss 8.1899 (9.1606)\\tPrec@1 68.750 (67.520)\nEpoch: [169][772/773]\\tLoss 6.4513 (9.1593)\\tPrec@1 0.000 (67.670)\n * Prec@1 50.840\nEpoch: [170][0/773]\\tLoss 8.5863 (8.5863)\\tPrec@1 62.500 (62.500)\nEpoch: [170][100/773]\\tLoss 11.7143 (8.4445)\\tPrec@1 43.750 (66.337)\nEpoch: [170][200/773]\\tLoss 10.8796 (8.7324)\\tPrec@1 68.750 (67.693)\nEpoch: [170][300/773]\\tLoss 5.5775 (9.0751)\\tPrec@1 81.250 (67.566)\nEpoch: [170][400/773]\\tLoss 5.5206 (9.1779)\\tPrec@1 68.750 (67.145)\nEpoch: [170][500/773]\\tLoss 14.0043 (9.1211)\\tPrec@1 68.750 (67.066)\nEpoch: [170][600/773]\\tLoss 5.8215 (9.0143)\\tPrec@1 62.500 (67.419)\nEpoch: [170][700/773]\\tLoss 21.4284 (9.0554)\\tPrec@1 50.000 (67.457)\nEpoch: [170][772/773]\\tLoss 1.2701 (9.0391)\\tPrec@1 0.000 (67.508)\n * Prec@1 51.340\nEpoch: [171][0/773]\\tLoss 4.6980 (4.6980)\\tPrec@1 62.500 (62.500)\nEpoch: [171][100/773]\\tLoss 8.6784 (10.1775)\\tPrec@1 62.500 (64.418)\nEpoch: [171][200/773]\\tLoss 5.7971 (9.3470)\\tPrec@1 62.500 (66.294)\nEpoch: [171][300/773]\\tLoss 3.7005 (9.2013)\\tPrec@1 81.250 (66.424)\nEpoch: [171][400/773]\\tLoss 3.9130 (9.0073)\\tPrec@1 62.500 (66.849)\nEpoch: [171][500/773]\\tLoss 5.9940 (9.1401)\\tPrec@1 75.000 (66.729)\nEpoch: [171][600/773]\\tLoss 9.9287 (9.0017)\\tPrec@1 68.750 (66.920)\nEpoch: [171][700/773]\\tLoss 4.8662 (9.0033)\\tPrec@1 68.750 (66.994)\nEpoch: [171][772/773]\\tLoss 1.1306 (8.9965)\\tPrec@1 0.000 (67.104)\n * Prec@1 51.560\nEpoch: [172][0/773]\\tLoss 20.4068 (20.4068)\\tPrec@1 68.750 (68.750)\nEpoch: [172][100/773]\\tLoss 4.6737 (9.5387)\\tPrec@1 75.000 (67.636)\nEpoch: [172][200/773]\\tLoss 2.8751 (9.2206)\\tPrec@1 75.000 (67.071)\nEpoch: [172][300/773]\\tLoss 36.2424 (9.1028)\\tPrec@1 56.250 (67.670)\nEpoch: [172][400/773]\\tLoss 4.7211 (8.9744)\\tPrec@1 75.000 (67.799)\nEpoch: [172][500/773]\\tLoss 7.5317 (8.9281)\\tPrec@1 56.250 (67.802)\nEpoch: [172][600/773]\\tLoss 9.7564 (8.8831)\\tPrec@1 81.250 (67.897)\nEpoch: [172][700/773]\\tLoss 2.7615 (8.9658)\\tPrec@1 87.500 (67.555)\nEpoch: [172][772/773]\\tLoss 1.8455 (9.0726)\\tPrec@1 50.000 (67.403)\n * Prec@1 51.430\nEpoch: [173][0/773]\\tLoss 6.2264 (6.2264)\\tPrec@1 93.750 (93.750)\nEpoch: [173][100/773]\\tLoss 7.0864 (8.2803)\\tPrec@1 68.750 (67.698)\nEpoch: [173][200/773]\\tLoss 11.0033 (8.8252)\\tPrec@1 68.750 (68.004)\nEpoch: [173][300/773]\\tLoss 14.9501 (8.7861)\\tPrec@1 56.250 (67.525)\nEpoch: [173][400/773]\\tLoss 6.6910 (8.7094)\\tPrec@1 56.250 (67.612)\nEpoch: [173][500/773]\\tLoss 47.4647 (8.8023)\\tPrec@1 62.500 (67.764)\nEpoch: [173][600/773]\\tLoss 7.3243 (8.9148)\\tPrec@1 75.000 (67.658)\nEpoch: [173][700/773]\\tLoss 14.4649 (9.1022)\\tPrec@1 75.000 (67.359)\nEpoch: [173][772/773]\\tLoss 1.3762 (9.1213)\\tPrec@1 0.000 (67.452)\n * Prec@1 51.560\nEpoch: [174][0/773]\\tLoss 14.8752 (14.8752)\\tPrec@1 31.250 (31.250)\nEpoch: [174][100/773]\\tLoss 6.1541 (8.7514)\\tPrec@1 81.250 (66.955)\nEpoch: [174][200/773]\\tLoss 11.4607 (9.1265)\\tPrec@1 75.000 (66.760)\nEpoch: [174][300/773]\\tLoss 4.4329 (8.7059)\\tPrec@1 81.250 (66.964)\nEpoch: [174][400/773]\\tLoss 1.8862 (9.0650)\\tPrec@1 87.500 (67.020)\nEpoch: [174][500/773]\\tLoss 8.4293 (9.0411)\\tPrec@1 56.250 (67.103)\nEpoch: [174][600/773]\\tLoss 4.2847 (8.8891)\\tPrec@1 62.500 (66.993)\nEpoch: [174][700/773]\\tLoss 32.2099 (8.9291)\\tPrec@1 62.500 (67.109)\nEpoch: [174][772/773]\\tLoss 0.9963 (8.9165)\\tPrec@1 0.000 (67.136)\n * Prec@1 51.220\nEpoch: [175][0/773]\\tLoss 3.0392 (3.0392)\\tPrec@1 81.250 (81.250)\nEpoch: [175][100/773]\\tLoss 6.2838 (8.4323)\\tPrec@1 81.250 (68.812)\nEpoch: [175][200/773]\\tLoss 11.0982 (8.3025)\\tPrec@1 68.750 (68.563)\nEpoch: [175][300/773]\\tLoss 13.2088 (8.7842)\\tPrec@1 56.250 (66.985)\nEpoch: [175][400/773]\\tLoss 5.3707 (8.9087)\\tPrec@1 62.500 (66.895)\nEpoch: [175][500/773]\\tLoss 4.1225 (8.9505)\\tPrec@1 75.000 (67.116)\nEpoch: [175][600/773]\\tLoss 13.9466 (9.0495)\\tPrec@1 62.500 (67.086)\nEpoch: [175][700/773]\\tLoss 13.0332 (9.0576)\\tPrec@1 87.500 (66.958)\nEpoch: [175][772/773]\\tLoss 1.5616 (8.9914)\\tPrec@1 0.000 (66.958)\n * Prec@1 51.550\nEpoch: [176][0/773]\\tLoss 4.1388 (4.1388)\\tPrec@1 87.500 (87.500)\nEpoch: [176][100/773]\\tLoss 15.5839 (8.1751)\\tPrec@1 68.750 (70.111)\nEpoch: [176][200/773]\\tLoss 3.6841 (8.4585)\\tPrec@1 68.750 (67.910)\nEpoch: [176][300/773]\\tLoss 2.4316 (8.5012)\\tPrec@1 87.500 (68.106)\nEpoch: [176][400/773]\\tLoss 2.7554 (8.6717)\\tPrec@1 75.000 (67.768)\nEpoch: [176][500/773]\\tLoss 6.9070 (8.7513)\\tPrec@1 75.000 (67.889)\nEpoch: [176][600/773]\\tLoss 9.0261 (8.7355)\\tPrec@1 75.000 (67.512)\nEpoch: [176][700/773]\\tLoss 6.8366 (8.6219)\\tPrec@1 68.750 (67.689)\nEpoch: [176][772/773]\\tLoss 0.9859 (8.6883)\\tPrec@1 0.000 (67.678)\n * Prec@1 51.250\nEpoch: [177][0/773]\\tLoss 40.0848 (40.0848)\\tPrec@1 50.000 (50.000)\nEpoch: [177][100/773]\\tLoss 11.2952 (8.7533)\\tPrec@1 75.000 (67.574)\nEpoch: [177][200/773]\\tLoss 15.7921 (9.3951)\\tPrec@1 62.500 (67.973)\nEpoch: [177][300/773]\\tLoss 21.2539 (9.0263)\\tPrec@1 50.000 (68.148)\nEpoch: [177][400/773]\\tLoss 17.3397 (8.7921)\\tPrec@1 56.250 (68.173)\nEpoch: [177][500/773]\\tLoss 4.8353 (8.8032)\\tPrec@1 43.750 (67.802)\nEpoch: [177][600/773]\\tLoss 1.9072 (8.9136)\\tPrec@1 87.500 (67.752)\nEpoch: [177][700/773]\\tLoss 7.1467 (9.0778)\\tPrec@1 75.000 (67.475)\nEpoch: [177][772/773]\\tLoss 3.5900 (9.0714)\\tPrec@1 0.000 (67.500)\n * Prec@1 51.370\nEpoch: [178][0/773]\\tLoss 3.3730 (3.3730)\\tPrec@1 81.250 (81.250)\nEpoch: [178][100/773]\\tLoss 3.8985 (9.3621)\\tPrec@1 62.500 (65.718)\nEpoch: [178][200/773]\\tLoss 26.3089 (9.3928)\\tPrec@1 68.750 (66.853)\nEpoch: [178][300/773]\\tLoss 18.6825 (8.9506)\\tPrec@1 81.250 (67.089)\nEpoch: [178][400/773]\\tLoss 9.9688 (8.9661)\\tPrec@1 81.250 (67.316)\nEpoch: [178][500/773]\\tLoss 4.7815 (8.9316)\\tPrec@1 68.750 (67.490)\nEpoch: [178][600/773]\\tLoss 2.1416 (9.0209)\\tPrec@1 81.250 (67.149)\nEpoch: [178][700/773]\\tLoss 3.5550 (8.8479)\\tPrec@1 62.500 (67.065)\nEpoch: [178][772/773]\\tLoss 2.1674 (8.8465)\\tPrec@1 0.000 (66.820)\n * Prec@1 51.270\nEpoch: [179][0/773]\\tLoss 11.1743 (11.1743)\\tPrec@1 68.750 (68.750)\nEpoch: [179][100/773]\\tLoss 2.6305 (8.9331)\\tPrec@1 68.750 (66.646)\nEpoch: [179][200/773]\\tLoss 6.3066 (8.6751)\\tPrec@1 56.250 (67.693)\nEpoch: [179][300/773]\\tLoss 2.8114 (8.8422)\\tPrec@1 81.250 (67.006)\nEpoch: [179][400/773]\\tLoss 6.9952 (8.7313)\\tPrec@1 62.500 (67.410)\nEpoch: [179][500/773]\\tLoss 13.0367 (8.6926)\\tPrec@1 50.000 (67.365)\nEpoch: [179][600/773]\\tLoss 5.8968 (8.8734)\\tPrec@1 68.750 (67.107)\nEpoch: [179][700/773]\\tLoss 6.3229 (8.8187)\\tPrec@1 56.250 (67.038)\nEpoch: [179][772/773]\\tLoss 0.7133 (8.9662)\\tPrec@1 0.000 (67.023)\n * Prec@1 51.540\nEpoch: [180][0/773]\\tLoss 2.7756 (2.7756)\\tPrec@1 75.000 (75.000)\nEpoch: [180][100/773]\\tLoss 9.7032 (8.2329)\\tPrec@1 68.750 (67.698)\nEpoch: [180][200/773]\\tLoss 5.3728 (8.5824)\\tPrec@1 68.750 (68.035)\nEpoch: [180][300/773]\\tLoss 6.0473 (8.2784)\\tPrec@1 68.750 (68.418)\nEpoch: [180][400/773]\\tLoss 1.2351 (8.8189)\\tPrec@1 93.750 (67.971)\nEpoch: [180][500/773]\\tLoss 5.7968 (8.5924)\\tPrec@1 75.000 (67.964)\nEpoch: [180][600/773]\\tLoss 3.2902 (8.8107)\\tPrec@1 87.500 (67.804)\nEpoch: [180][700/773]\\tLoss 6.5885 (8.7348)\\tPrec@1 68.750 (67.448)\nEpoch: [180][772/773]\\tLoss 4.6641 (8.8626)\\tPrec@1 50.000 (67.290)\n * Prec@1 50.970\nEpoch: [181][0/773]\\tLoss 3.8127 (3.8127)\\tPrec@1 81.250 (81.250)\nEpoch: [181][100/773]\\tLoss 2.7563 (8.3824)\\tPrec@1 87.500 (68.131)\nEpoch: [181][200/773]\\tLoss 4.6380 (8.8857)\\tPrec@1 62.500 (67.071)\nEpoch: [181][300/773]\\tLoss 8.3776 (8.6185)\\tPrec@1 75.000 (67.151)\nEpoch: [181][400/773]\\tLoss 18.7106 (8.6447)\\tPrec@1 56.250 (66.724)\nEpoch: [181][500/773]\\tLoss 18.0650 (8.5264)\\tPrec@1 81.250 (66.804)\nEpoch: [181][600/773]\\tLoss 22.6702 (8.7432)\\tPrec@1 50.000 (66.681)\nEpoch: [181][700/773]\\tLoss 4.4818 (8.8057)\\tPrec@1 68.750 (66.548)\nEpoch: [181][772/773]\\tLoss 0.6866 (8.8361)\\tPrec@1 0.000 (66.570)\n * Prec@1 51.260\nEpoch: [182][0/773]\\tLoss 5.9872 (5.9872)\\tPrec@1 75.000 (75.000)\nEpoch: [182][100/773]\\tLoss 6.9858 (8.9541)\\tPrec@1 62.500 (66.894)\nEpoch: [182][200/773]\\tLoss 5.7801 (8.8097)\\tPrec@1 68.750 (66.294)\nEpoch: [182][300/773]\\tLoss 1.7170 (8.8922)\\tPrec@1 81.250 (66.591)\nEpoch: [182][400/773]\\tLoss 2.9900 (8.6561)\\tPrec@1 68.750 (66.397)\nEpoch: [182][500/773]\\tLoss 4.2585 (8.5894)\\tPrec@1 68.750 (66.779)\nEpoch: [182][600/773]\\tLoss 6.8437 (8.4900)\\tPrec@1 56.250 (66.868)\nEpoch: [182][700/773]\\tLoss 13.6354 (8.5082)\\tPrec@1 75.000 (67.020)\nEpoch: [182][772/773]\\tLoss 1.7715 (8.5276)\\tPrec@1 0.000 (67.015)\n * Prec@1 51.280\nEpoch: [183][0/773]\\tLoss 6.0761 (6.0761)\\tPrec@1 75.000 (75.000)\nEpoch: [183][100/773]\\tLoss 3.5080 (8.6793)\\tPrec@1 68.750 (67.946)\nEpoch: [183][200/773]\\tLoss 4.0720 (8.2111)\\tPrec@1 81.250 (67.693)\nEpoch: [183][300/773]\\tLoss 7.9334 (7.9993)\\tPrec@1 50.000 (67.193)\nEpoch: [183][400/773]\\tLoss 6.7721 (8.1145)\\tPrec@1 81.250 (67.706)\nEpoch: [183][500/773]\\tLoss 7.0541 (8.3248)\\tPrec@1 62.500 (67.453)\nEpoch: [183][600/773]\\tLoss 7.8501 (8.3961)\\tPrec@1 62.500 (67.388)\nEpoch: [183][700/773]\\tLoss 4.4504 (8.5081)\\tPrec@1 75.000 (67.413)\nEpoch: [183][772/773]\\tLoss 1.1587 (8.6285)\\tPrec@1 0.000 (67.177)\n * Prec@1 51.570\nEpoch: [184][0/773]\\tLoss 7.6166 (7.6166)\\tPrec@1 68.750 (68.750)\nEpoch: [184][100/773]\\tLoss 3.3098 (8.3964)\\tPrec@1 81.250 (68.626)\nEpoch: [184][200/773]\\tLoss 14.7880 (7.9200)\\tPrec@1 62.500 (68.657)\nEpoch: [184][300/773]\\tLoss 6.4652 (7.7297)\\tPrec@1 62.500 (68.293)\nEpoch: [184][400/773]\\tLoss 5.6289 (8.0610)\\tPrec@1 50.000 (67.441)\nEpoch: [184][500/773]\\tLoss 5.5018 (7.9966)\\tPrec@1 56.250 (67.465)\nEpoch: [184][600/773]\\tLoss 6.7765 (8.2872)\\tPrec@1 56.250 (67.512)\nEpoch: [184][700/773]\\tLoss 10.2826 (8.4267)\\tPrec@1 62.500 (67.127)\nEpoch: [184][772/773]\\tLoss 1.6840 (8.5282)\\tPrec@1 0.000 (67.047)\n * Prec@1 51.670\nEpoch: [185][0/773]\\tLoss 5.9548 (5.9548)\\tPrec@1 75.000 (75.000)\nEpoch: [185][100/773]\\tLoss 7.2917 (8.3431)\\tPrec@1 62.500 (67.017)\nEpoch: [185][200/773]\\tLoss 19.2708 (8.0269)\\tPrec@1 81.250 (67.164)\nEpoch: [185][300/773]\\tLoss 5.9658 (8.2130)\\tPrec@1 68.750 (67.400)\nEpoch: [185][400/773]\\tLoss 6.9748 (8.2879)\\tPrec@1 75.000 (67.503)\nEpoch: [185][500/773]\\tLoss 6.9899 (8.3406)\\tPrec@1 68.750 (67.440)\nEpoch: [185][600/773]\\tLoss 2.8176 (8.5581)\\tPrec@1 75.000 (67.564)\nEpoch: [185][700/773]\\tLoss 20.6952 (8.6114)\\tPrec@1 75.000 (67.466)\nEpoch: [185][772/773]\\tLoss 2.8216 (8.6284)\\tPrec@1 0.000 (67.508)\n * Prec@1 52.080\nEpoch: [186][0/773]\\tLoss 10.3562 (10.3562)\\tPrec@1 56.250 (56.250)\nEpoch: [186][100/773]\\tLoss 10.9076 (9.1070)\\tPrec@1 75.000 (67.327)\nEpoch: [186][200/773]\\tLoss 3.1984 (8.6971)\\tPrec@1 75.000 (67.600)\nEpoch: [186][300/773]\\tLoss 7.6118 (8.3261)\\tPrec@1 56.250 (67.774)\nEpoch: [186][400/773]\\tLoss 4.4537 (8.4193)\\tPrec@1 62.500 (67.877)\nEpoch: [186][500/773]\\tLoss 21.0215 (8.4826)\\tPrec@1 75.000 (67.390)\nEpoch: [186][600/773]\\tLoss 2.0984 (8.5608)\\tPrec@1 81.250 (67.149)\nEpoch: [186][700/773]\\tLoss 3.6337 (8.7583)\\tPrec@1 68.750 (67.127)\nEpoch: [186][772/773]\\tLoss 1.1822 (8.7002)\\tPrec@1 0.000 (67.201)\n * Prec@1 51.840\nEpoch: [187][0/773]\\tLoss 9.0568 (9.0568)\\tPrec@1 62.500 (62.500)\nEpoch: [187][100/773]\\tLoss 9.6622 (9.0322)\\tPrec@1 50.000 (64.975)\nEpoch: [187][200/773]\\tLoss 3.6755 (8.8402)\\tPrec@1 68.750 (66.542)\nEpoch: [187][300/773]\\tLoss 2.9567 (8.5088)\\tPrec@1 68.750 (66.860)\nEpoch: [187][400/773]\\tLoss 9.8749 (8.3016)\\tPrec@1 50.000 (66.817)\nEpoch: [187][500/773]\\tLoss 4.2830 (8.4102)\\tPrec@1 81.250 (66.679)\nEpoch: [187][600/773]\\tLoss 14.1169 (8.4484)\\tPrec@1 56.250 (67.128)\nEpoch: [187][700/773]\\tLoss 10.8987 (8.4447)\\tPrec@1 50.000 (67.118)\nEpoch: [187][772/773]\\tLoss 1.2193 (8.5098)\\tPrec@1 0.000 (67.233)\n * Prec@1 51.510\nEpoch: [188][0/773]\\tLoss 8.7241 (8.7241)\\tPrec@1 62.500 (62.500)\nEpoch: [188][100/773]\\tLoss 17.0855 (9.0270)\\tPrec@1 75.000 (66.399)\nEpoch: [188][200/773]\\tLoss 7.7296 (8.6774)\\tPrec@1 62.500 (67.475)\nEpoch: [188][300/773]\\tLoss 2.9206 (8.3974)\\tPrec@1 75.000 (67.753)\nEpoch: [188][400/773]\\tLoss 2.4501 (8.6720)\\tPrec@1 81.250 (67.862)\nEpoch: [188][500/773]\\tLoss 8.3884 (8.5576)\\tPrec@1 75.000 (67.640)\nEpoch: [188][600/773]\\tLoss 4.5624 (8.4579)\\tPrec@1 81.250 (67.845)\nEpoch: [188][700/773]\\tLoss 2.7671 (8.5557)\\tPrec@1 68.750 (67.627)\nEpoch: [188][772/773]\\tLoss 0.8551 (8.6346)\\tPrec@1 0.000 (67.573)\n * Prec@1 51.920\nEpoch: [189][0/773]\\tLoss 14.0707 (14.0707)\\tPrec@1 56.250 (56.250)\nEpoch: [189][100/773]\\tLoss 3.9200 (7.3996)\\tPrec@1 62.500 (66.832)\nEpoch: [189][200/773]\\tLoss 10.4041 (8.1564)\\tPrec@1 81.250 (66.387)\nEpoch: [189][300/773]\\tLoss 5.8262 (8.4041)\\tPrec@1 81.250 (67.380)\nEpoch: [189][400/773]\\tLoss 3.1248 (8.5050)\\tPrec@1 75.000 (66.989)\nEpoch: [189][500/773]\\tLoss 19.7965 (8.7046)\\tPrec@1 62.500 (67.091)\nEpoch: [189][600/773]\\tLoss 9.6391 (8.6598)\\tPrec@1 68.750 (67.232)\nEpoch: [189][700/773]\\tLoss 10.6545 (8.4597)\\tPrec@1 62.500 (67.484)\nEpoch: [189][772/773]\\tLoss 1.0862 (8.5241)\\tPrec@1 50.000 (67.403)\n * Prec@1 51.790\nEpoch: [190][0/773]\\tLoss 11.5976 (11.5976)\\tPrec@1 81.250 (81.250)\nEpoch: [190][100/773]\\tLoss 7.0999 (7.6575)\\tPrec@1 75.000 (69.926)\nEpoch: [190][200/773]\\tLoss 7.7459 (7.8235)\\tPrec@1 62.500 (68.657)\nEpoch: [190][300/773]\\tLoss 2.4211 (8.0509)\\tPrec@1 87.500 (68.916)\nEpoch: [190][400/773]\\tLoss 10.6839 (8.2711)\\tPrec@1 62.500 (68.392)\nEpoch: [190][500/773]\\tLoss 16.6658 (8.2911)\\tPrec@1 68.750 (68.588)\nEpoch: [190][600/773]\\tLoss 3.6526 (8.5414)\\tPrec@1 81.250 (68.157)\nEpoch: [190][700/773]\\tLoss 27.4763 (8.5218)\\tPrec@1 68.750 (68.117)\nEpoch: [190][772/773]\\tLoss 1.1206 (8.5519)\\tPrec@1 0.000 (68.075)\n * Prec@1 51.640\nEpoch: [191][0/773]\\tLoss 8.5352 (8.5352)\\tPrec@1 68.750 (68.750)\nEpoch: [191][100/773]\\tLoss 2.1079 (8.8423)\\tPrec@1 87.500 (67.760)\nEpoch: [191][200/773]\\tLoss 3.4881 (8.8196)\\tPrec@1 75.000 (66.636)\nEpoch: [191][300/773]\\tLoss 7.6095 (8.6093)\\tPrec@1 50.000 (66.196)\nEpoch: [191][400/773]\\tLoss 5.1068 (8.6214)\\tPrec@1 75.000 (66.646)\nEpoch: [191][500/773]\\tLoss 1.1052 (8.5958)\\tPrec@1 87.500 (66.779)\nEpoch: [191][600/773]\\tLoss 2.5470 (8.5190)\\tPrec@1 75.000 (67.034)\nEpoch: [191][700/773]\\tLoss 8.5689 (8.5174)\\tPrec@1 56.250 (66.985)\nEpoch: [191][772/773]\\tLoss 0.5250 (8.6315)\\tPrec@1 50.000 (67.104)\n * Prec@1 51.690\nEpoch: [192][0/773]\\tLoss 4.7210 (4.7210)\\tPrec@1 68.750 (68.750)\nEpoch: [192][100/773]\\tLoss 7.9137 (9.4335)\\tPrec@1 56.250 (66.027)\nEpoch: [192][200/773]\\tLoss 5.5466 (8.7460)\\tPrec@1 62.500 (66.822)\nEpoch: [192][300/773]\\tLoss 4.1546 (8.8537)\\tPrec@1 56.250 (67.380)\nEpoch: [192][400/773]\\tLoss 2.8019 (8.7096)\\tPrec@1 62.500 (67.659)\nEpoch: [192][500/773]\\tLoss 6.0099 (8.6325)\\tPrec@1 50.000 (67.914)\nEpoch: [192][600/773]\\tLoss 4.6694 (8.5722)\\tPrec@1 81.250 (67.783)\nEpoch: [192][700/773]\\tLoss 6.1323 (8.4779)\\tPrec@1 62.500 (67.680)\nEpoch: [192][772/773]\\tLoss 1.4210 (8.4603)\\tPrec@1 50.000 (67.573)\n * Prec@1 52.090\nEpoch: [193][0/773]\\tLoss 1.1399 (1.1399)\\tPrec@1 81.250 (81.250)\nEpoch: [193][100/773]\\tLoss 8.7517 (8.1510)\\tPrec@1 81.250 (70.050)\nEpoch: [193][200/773]\\tLoss 5.0042 (7.9777)\\tPrec@1 87.500 (68.781)\nEpoch: [193][300/773]\\tLoss 1.5029 (8.1885)\\tPrec@1 81.250 (68.293)\nEpoch: [193][400/773]\\tLoss 4.6669 (8.5131)\\tPrec@1 43.750 (67.877)\nEpoch: [193][500/773]\\tLoss 17.2102 (8.3068)\\tPrec@1 56.250 (67.527)\nEpoch: [193][600/773]\\tLoss 1.9809 (8.2607)\\tPrec@1 81.250 (67.471)\nEpoch: [193][700/773]\\tLoss 4.0923 (8.3626)\\tPrec@1 75.000 (67.243)\nEpoch: [193][772/773]\\tLoss 1.5660 (8.4080)\\tPrec@1 0.000 (67.322)\n * Prec@1 51.560\nEpoch: [194][0/773]\\tLoss 6.2924 (6.2924)\\tPrec@1 56.250 (56.250)\nEpoch: [194][100/773]\\tLoss 7.9377 (8.4353)\\tPrec@1 87.500 (68.131)\nEpoch: [194][200/773]\\tLoss 3.0842 (8.4795)\\tPrec@1 75.000 (67.755)\nEpoch: [194][300/773]\\tLoss 4.0866 (8.5869)\\tPrec@1 56.250 (67.421)\nEpoch: [194][400/773]\\tLoss 6.8220 (8.5579)\\tPrec@1 87.500 (67.768)\nEpoch: [194][500/773]\\tLoss 8.3168 (8.5137)\\tPrec@1 50.000 (67.939)\nEpoch: [194][600/773]\\tLoss 6.1980 (8.4873)\\tPrec@1 62.500 (68.105)\nEpoch: [194][700/773]\\tLoss 6.3550 (8.3292)\\tPrec@1 43.750 (68.153)\nEpoch: [194][772/773]\\tLoss 1.5161 (8.3783)\\tPrec@1 0.000 (68.083)\n * Prec@1 51.860\nEpoch: [195][0/773]\\tLoss 4.5467 (4.5467)\\tPrec@1 56.250 (56.250)\nEpoch: [195][100/773]\\tLoss 8.6927 (8.7727)\\tPrec@1 75.000 (66.708)\nEpoch: [195][200/773]\\tLoss 9.4890 (8.4716)\\tPrec@1 50.000 (67.133)\nEpoch: [195][300/773]\\tLoss 12.0978 (8.3334)\\tPrec@1 62.500 (67.463)\nEpoch: [195][400/773]\\tLoss 3.4732 (8.1892)\\tPrec@1 87.500 (68.204)\nEpoch: [195][500/773]\\tLoss 8.0490 (8.1492)\\tPrec@1 62.500 (67.977)\nEpoch: [195][600/773]\\tLoss 7.5768 (8.3042)\\tPrec@1 62.500 (67.720)\nEpoch: [195][700/773]\\tLoss 11.1058 (8.3002)\\tPrec@1 62.500 (67.716)\nEpoch: [195][772/773]\\tLoss 1.3241 (8.3406)\\tPrec@1 0.000 (67.759)\n * Prec@1 52.010\nEpoch: [196][0/773]\\tLoss 2.9642 (2.9642)\\tPrec@1 75.000 (75.000)\nEpoch: [196][100/773]\\tLoss 14.1011 (8.3482)\\tPrec@1 43.750 (67.698)\nEpoch: [196][200/773]\\tLoss 3.3999 (8.1583)\\tPrec@1 75.000 (67.662)\nEpoch: [196][300/773]\\tLoss 2.7321 (8.1686)\\tPrec@1 81.250 (67.795)\nEpoch: [196][400/773]\\tLoss 6.8778 (8.4028)\\tPrec@1 81.250 (67.768)\nEpoch: [196][500/773]\\tLoss 8.5992 (8.4382)\\tPrec@1 56.250 (68.101)\nEpoch: [196][600/773]\\tLoss 5.9102 (8.3602)\\tPrec@1 62.500 (68.022)\nEpoch: [196][700/773]\\tLoss 8.3750 (8.3504)\\tPrec@1 87.500 (67.983)\nEpoch: [196][772/773]\\tLoss 0.9634 (8.3538)\\tPrec@1 0.000 (67.824)\n * Prec@1 51.780\nEpoch: [197][0/773]\\tLoss 9.5994 (9.5994)\\tPrec@1 68.750 (68.750)\nEpoch: [197][100/773]\\tLoss 9.4906 (7.9596)\\tPrec@1 68.750 (67.079)\nEpoch: [197][200/773]\\tLoss 3.6456 (8.2660)\\tPrec@1 75.000 (67.755)\nEpoch: [197][300/773]\\tLoss 4.7270 (8.3077)\\tPrec@1 81.250 (67.712)\nEpoch: [197][400/773]\\tLoss 4.5231 (8.2512)\\tPrec@1 56.250 (67.721)\nEpoch: [197][500/773]\\tLoss 10.4336 (8.2389)\\tPrec@1 81.250 (67.478)\nEpoch: [197][600/773]\\tLoss 1.0037 (8.2773)\\tPrec@1 81.250 (67.554)\nEpoch: [197][700/773]\\tLoss 15.6791 (8.2695)\\tPrec@1 50.000 (68.162)\nEpoch: [197][772/773]\\tLoss 0.7265 (8.2569)\\tPrec@1 0.000 (68.059)\n * Prec@1 51.990\nEpoch: [198][0/773]\\tLoss 3.8823 (3.8823)\\tPrec@1 62.500 (62.500)\nEpoch: [198][100/773]\\tLoss 9.0879 (8.6404)\\tPrec@1 68.750 (69.121)\nEpoch: [198][200/773]\\tLoss 6.3348 (8.4425)\\tPrec@1 68.750 (68.004)\nEpoch: [198][300/773]\\tLoss 12.5253 (8.7417)\\tPrec@1 56.250 (66.860)\nEpoch: [198][400/773]\\tLoss 1.1345 (8.5799)\\tPrec@1 87.500 (67.347)\nEpoch: [198][500/773]\\tLoss 8.2639 (8.3666)\\tPrec@1 43.750 (67.789)\nEpoch: [198][600/773]\\tLoss 3.5109 (8.3341)\\tPrec@1 75.000 (67.575)\nEpoch: [198][700/773]\\tLoss 5.8003 (8.3590)\\tPrec@1 81.250 (67.644)\nEpoch: [198][772/773]\\tLoss 3.7562 (8.3740)\\tPrec@1 0.000 (67.565)\n * Prec@1 51.590\nEpoch: [199][0/773]\\tLoss 5.3601 (5.3601)\\tPrec@1 68.750 (68.750)\nEpoch: [199][100/773]\\tLoss 12.4014 (8.3593)\\tPrec@1 68.750 (67.327)\nEpoch: [199][200/773]\\tLoss 9.6411 (8.1200)\\tPrec@1 56.250 (68.408)\nEpoch: [199][300/773]\\tLoss 11.9103 (8.4590)\\tPrec@1 68.750 (67.629)\nEpoch: [199][400/773]\\tLoss 11.0948 (8.4299)\\tPrec@1 56.250 (67.394)\nEpoch: [199][500/773]\\tLoss 6.9192 (8.3019)\\tPrec@1 37.500 (67.265)\nEpoch: [199][600/773]\\tLoss 10.0420 (8.3888)\\tPrec@1 68.750 (67.117)\nEpoch: [199][700/773]\\tLoss 17.1544 (8.3062)\\tPrec@1 50.000 (67.430)\nEpoch: [199][772/773]\\tLoss 6.4436 (8.2627)\\tPrec@1 0.000 (67.387)\n * Prec@1 51.700\nBest accuracy:  52.09\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Stage 1 (IF=20)\n!python pretrain_stage1.py --dataset cifar100 --imb_factor 0.05 --epochs 200\n\n# Stage 2 (IF=20)\n!python OT_train.py \\\n--dataset cifar100 \\\n--num_classes 100 \\\n--imb_factor 0.05 \\\n--epochs 200 \\\n--cost combined \\\n--meta_set prototype \\\n--batch-size 16 \\\n--save_name OT_cifar100_IF20_full_run \\\n--ckpt_path checkpoint/ours/pretrain/cifar100_imb0.05_stage1.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:36:50.296651Z","iopub.execute_input":"2025-06-15T09:36:50.296937Z","iopub.status.idle":"2025-06-15T10:16:11.340087Z","shell.execute_reply.started":"2025-06-15T09:36:50.296904Z","shell.execute_reply":"2025-06-15T10:16:11.339353Z"}},"outputs":[{"name":"stdout","text":"File 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\n--> Creating Imbalanced Dataset...\nFiles already downloaded and verified\n--> Building and Training Model for Stage 1...\nEpoch 1/200: 100%|████████████████| 122/122 [00:07<00:00, 16.42it/s, Loss=3.731]\nEpoch 2/200: 100%|████████████████| 122/122 [00:06<00:00, 20.07it/s, Loss=3.445]\nEpoch 3/200: 100%|████████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=3.547]\nEpoch 4/200: 100%|████████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=3.269]\nEpoch 5/200: 100%|████████████████| 122/122 [00:06<00:00, 19.70it/s, Loss=3.224]\nEpoch 6/200: 100%|████████████████| 122/122 [00:06<00:00, 19.54it/s, Loss=2.696]\nEpoch 7/200: 100%|████████████████| 122/122 [00:06<00:00, 19.66it/s, Loss=2.964]\nEpoch 8/200: 100%|████████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=2.740]\nEpoch 9/200: 100%|████████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=2.608]\nEpoch 10/200: 100%|███████████████| 122/122 [00:06<00:00, 20.07it/s, Loss=2.358]\nEpoch 11/200: 100%|███████████████| 122/122 [00:06<00:00, 20.00it/s, Loss=2.411]\nEpoch 12/200: 100%|███████████████| 122/122 [00:06<00:00, 20.09it/s, Loss=2.375]\nEpoch 13/200: 100%|███████████████| 122/122 [00:06<00:00, 20.08it/s, Loss=2.224]\nEpoch 14/200: 100%|███████████████| 122/122 [00:06<00:00, 20.03it/s, Loss=2.193]\nEpoch 15/200: 100%|███████████████| 122/122 [00:06<00:00, 20.03it/s, Loss=2.184]\nEpoch 16/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.762]\nEpoch 17/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.934]\nEpoch 18/200: 100%|███████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=2.081]\nEpoch 19/200: 100%|███████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=1.973]\nEpoch 20/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.697]\nEpoch 21/200: 100%|███████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=2.033]\nEpoch 22/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.484]\nEpoch 23/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.693]\nEpoch 24/200: 100%|███████████████| 122/122 [00:06<00:00, 20.01it/s, Loss=1.673]\nEpoch 25/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.873]\nEpoch 26/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.816]\nEpoch 27/200: 100%|███████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=1.850]\nEpoch 28/200: 100%|███████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.594]\nEpoch 29/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.794]\nEpoch 30/200: 100%|███████████████| 122/122 [00:06<00:00, 20.00it/s, Loss=1.711]\nEpoch 31/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.369]\nEpoch 32/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.512]\nEpoch 33/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.405]\nEpoch 34/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.617]\nEpoch 35/200: 100%|███████████████| 122/122 [00:06<00:00, 20.00it/s, Loss=1.440]\nEpoch 36/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.474]\nEpoch 37/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.603]\nEpoch 38/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.425]\nEpoch 39/200: 100%|███████████████| 122/122 [00:06<00:00, 20.00it/s, Loss=1.418]\nEpoch 40/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.632]\nEpoch 41/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.682]\nEpoch 42/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.252]\nEpoch 43/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.457]\nEpoch 44/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.527]\nEpoch 45/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.306]\nEpoch 46/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.280]\nEpoch 47/200: 100%|███████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.464]\nEpoch 48/200: 100%|███████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=1.490]\nEpoch 49/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.270]\nEpoch 50/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.202]\nEpoch 51/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.288]\nEpoch 52/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.199]\nEpoch 53/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.283]\nEpoch 54/200: 100%|███████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.279]\nEpoch 55/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.590]\nEpoch 56/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.556]\nEpoch 57/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.251]\nEpoch 58/200: 100%|███████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=1.445]\nEpoch 59/200: 100%|███████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=1.183]\nEpoch 60/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.290]\nEpoch 61/200: 100%|███████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.224]\nEpoch 62/200: 100%|███████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.257]\nEpoch 63/200: 100%|███████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=0.978]\nEpoch 64/200: 100%|███████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=1.254]\nEpoch 65/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.087]\nEpoch 66/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.231]\nEpoch 67/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.238]\nEpoch 68/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.211]\nEpoch 69/200: 100%|███████████████| 122/122 [00:06<00:00, 19.76it/s, Loss=1.047]\nEpoch 70/200: 100%|███████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.296]\nEpoch 71/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.367]\nEpoch 72/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.216]\nEpoch 73/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.088]\nEpoch 74/200: 100%|███████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.230]\nEpoch 75/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.152]\nEpoch 76/200: 100%|███████████████| 122/122 [00:06<00:00, 19.99it/s, Loss=1.138]\nEpoch 77/200: 100%|███████████████| 122/122 [00:06<00:00, 19.98it/s, Loss=1.271]\nEpoch 78/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=0.993]\nEpoch 79/200: 100%|███████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=1.211]\nEpoch 80/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.221]\nEpoch 81/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=0.956]\nEpoch 82/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.333]\nEpoch 83/200: 100%|███████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.355]\nEpoch 84/200: 100%|███████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=1.138]\nEpoch 85/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.336]\nEpoch 86/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.311]\nEpoch 87/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.253]\nEpoch 88/200: 100%|███████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.102]\nEpoch 89/200: 100%|███████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=1.484]\nEpoch 90/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.050]\nEpoch 91/200: 100%|███████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.303]\nEpoch 92/200: 100%|███████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=1.106]\nEpoch 93/200: 100%|███████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=1.122]\nEpoch 94/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.168]\nEpoch 95/200: 100%|███████████████| 122/122 [00:06<00:00, 19.84it/s, Loss=1.221]\nEpoch 96/200: 100%|███████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.006]\nEpoch 97/200: 100%|███████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.160]\nEpoch 98/200: 100%|███████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=1.297]\nEpoch 99/200: 100%|███████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.233]\nEpoch 100/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.317]\nEpoch 101/200: 100%|██████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.063]\nEpoch 102/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.052]\nEpoch 103/200: 100%|██████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.305]\nEpoch 104/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.379]\nEpoch 105/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.974]\nEpoch 106/200: 100%|██████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=1.113]\nEpoch 107/200: 100%|██████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=0.966]\nEpoch 108/200: 100%|██████████████| 122/122 [00:06<00:00, 19.99it/s, Loss=1.294]\nEpoch 109/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.278]\nEpoch 110/200: 100%|██████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=1.282]\nEpoch 111/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.237]\nEpoch 112/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.075]\nEpoch 113/200: 100%|██████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=1.372]\nEpoch 114/200: 100%|██████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=1.111]\nEpoch 115/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.936]\nEpoch 116/200: 100%|██████████████| 122/122 [00:06<00:00, 19.65it/s, Loss=1.020]\nEpoch 117/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.082]\nEpoch 118/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=1.232]\nEpoch 119/200: 100%|██████████████| 122/122 [00:06<00:00, 19.99it/s, Loss=1.045]\nEpoch 120/200: 100%|██████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.124]\nEpoch 121/200: 100%|██████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=1.088]\nEpoch 122/200: 100%|██████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.308]\nEpoch 123/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=0.967]\nEpoch 124/200: 100%|██████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=1.011]\nEpoch 125/200: 100%|██████████████| 122/122 [00:06<00:00, 19.97it/s, Loss=1.199]\nEpoch 126/200: 100%|██████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=1.401]\nEpoch 127/200: 100%|██████████████| 122/122 [00:06<00:00, 19.82it/s, Loss=0.831]\nEpoch 128/200: 100%|██████████████| 122/122 [00:06<00:00, 19.74it/s, Loss=0.881]\nEpoch 129/200: 100%|██████████████| 122/122 [00:06<00:00, 19.62it/s, Loss=1.163]\nEpoch 130/200: 100%|██████████████| 122/122 [00:06<00:00, 19.65it/s, Loss=1.009]\nEpoch 131/200: 100%|██████████████| 122/122 [00:06<00:00, 19.77it/s, Loss=0.907]\nEpoch 132/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=1.037]\nEpoch 133/200: 100%|██████████████| 122/122 [00:06<00:00, 19.95it/s, Loss=0.914]\nEpoch 134/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.094]\nEpoch 135/200: 100%|██████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=1.066]\nEpoch 136/200: 100%|██████████████| 122/122 [00:06<00:00, 19.65it/s, Loss=1.107]\nEpoch 137/200: 100%|██████████████| 122/122 [00:06<00:00, 19.63it/s, Loss=1.102]\nEpoch 138/200: 100%|██████████████| 122/122 [00:06<00:00, 19.82it/s, Loss=1.084]\nEpoch 139/200: 100%|██████████████| 122/122 [00:06<00:00, 19.79it/s, Loss=1.235]\nEpoch 140/200: 100%|██████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=1.146]\nEpoch 141/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.066]\nEpoch 142/200: 100%|██████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=1.264]\nEpoch 143/200: 100%|██████████████| 122/122 [00:06<00:00, 19.94it/s, Loss=1.106]\nEpoch 144/200: 100%|██████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=0.920]\nEpoch 145/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=1.108]\nEpoch 146/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=0.961]\nEpoch 147/200: 100%|██████████████| 122/122 [00:06<00:00, 19.84it/s, Loss=0.988]\nEpoch 148/200: 100%|██████████████| 122/122 [00:06<00:00, 19.96it/s, Loss=1.269]\nEpoch 149/200: 100%|██████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=1.127]\nEpoch 150/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.073]\nEpoch 151/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=0.649]\nEpoch 152/200: 100%|██████████████| 122/122 [00:06<00:00, 19.76it/s, Loss=1.052]\nEpoch 153/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=0.973]\nEpoch 154/200: 100%|██████████████| 122/122 [00:06<00:00, 19.90it/s, Loss=0.858]\nEpoch 155/200: 100%|██████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=0.923]\nEpoch 156/200: 100%|██████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=1.178]\nEpoch 157/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.073]\nEpoch 158/200: 100%|██████████████| 122/122 [00:06<00:00, 19.67it/s, Loss=0.976]\nEpoch 159/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=0.954]\nEpoch 160/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=1.000]\nEpoch 161/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=0.659]\nEpoch 162/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.541]\nEpoch 163/200: 100%|██████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=0.422]\nEpoch 164/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.293]\nEpoch 165/200: 100%|██████████████| 122/122 [00:06<00:00, 19.79it/s, Loss=0.324]\nEpoch 166/200: 100%|██████████████| 122/122 [00:06<00:00, 19.82it/s, Loss=0.429]\nEpoch 167/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=0.344]\nEpoch 168/200: 100%|██████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=0.339]\nEpoch 169/200: 100%|██████████████| 122/122 [00:06<00:00, 19.92it/s, Loss=0.333]\nEpoch 170/200: 100%|██████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=0.251]\nEpoch 171/200: 100%|██████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=0.316]\nEpoch 172/200: 100%|██████████████| 122/122 [00:06<00:00, 19.89it/s, Loss=0.194]\nEpoch 173/200: 100%|██████████████| 122/122 [00:06<00:00, 19.79it/s, Loss=0.239]\nEpoch 174/200: 100%|██████████████| 122/122 [00:06<00:00, 19.93it/s, Loss=0.314]\nEpoch 175/200: 100%|██████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=0.269]\nEpoch 176/200: 100%|██████████████| 122/122 [00:06<00:00, 19.79it/s, Loss=0.232]\nEpoch 177/200: 100%|██████████████| 122/122 [00:06<00:00, 19.85it/s, Loss=0.237]\nEpoch 178/200: 100%|██████████████| 122/122 [00:06<00:00, 19.72it/s, Loss=0.253]\nEpoch 179/200: 100%|██████████████| 122/122 [00:06<00:00, 19.63it/s, Loss=0.142]\nEpoch 180/200: 100%|██████████████| 122/122 [00:06<00:00, 19.82it/s, Loss=0.285]\nEpoch 181/200: 100%|██████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=0.251]\nEpoch 182/200: 100%|██████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=0.142]\nEpoch 183/200: 100%|██████████████| 122/122 [00:06<00:00, 19.81it/s, Loss=0.196]\nEpoch 184/200: 100%|██████████████| 122/122 [00:06<00:00, 19.68it/s, Loss=0.224]\nEpoch 185/200: 100%|██████████████| 122/122 [00:06<00:00, 19.91it/s, Loss=0.204]\nEpoch 186/200: 100%|██████████████| 122/122 [00:06<00:00, 19.80it/s, Loss=0.306]\nEpoch 187/200: 100%|██████████████| 122/122 [00:06<00:00, 19.76it/s, Loss=0.149]\nEpoch 188/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.165]\nEpoch 189/200: 100%|██████████████| 122/122 [00:06<00:00, 19.75it/s, Loss=0.206]\nEpoch 190/200: 100%|██████████████| 122/122 [00:06<00:00, 19.87it/s, Loss=0.189]\nEpoch 191/200: 100%|██████████████| 122/122 [00:06<00:00, 19.88it/s, Loss=0.126]\nEpoch 192/200: 100%|██████████████| 122/122 [00:06<00:00, 19.86it/s, Loss=0.229]\nEpoch 193/200: 100%|██████████████| 122/122 [00:06<00:00, 19.84it/s, Loss=0.217]\nEpoch 194/200: 100%|██████████████| 122/122 [00:06<00:00, 19.68it/s, Loss=0.124]\nEpoch 195/200: 100%|██████████████| 122/122 [00:06<00:00, 19.82it/s, Loss=0.155]\nEpoch 196/200: 100%|██████████████| 122/122 [00:06<00:00, 19.83it/s, Loss=0.196]\nEpoch 197/200: 100%|██████████████| 122/122 [00:06<00:00, 19.80it/s, Loss=0.194]\nEpoch 198/200: 100%|██████████████| 122/122 [00:06<00:00, 19.78it/s, Loss=0.119]\nEpoch 199/200: 100%|██████████████| 122/122 [00:06<00:00, 19.67it/s, Loss=0.171]\nEpoch 200/200: 100%|██████████████| 122/122 [00:06<00:00, 19.80it/s, Loss=0.157]\n--> Stage 1 training complete. Saving model to checkpoint/ours/pretrain//cifar100_imb0.05_stage1.pth\nFile 'pretrain_stage1.py' has been saved.\nFile 'data_utils.py' has been saved.\nFile 'resnet.py' has been saved.\nFile 'Sinkhorn_distance.py' has been saved.\nFile 'Sinkhorn_distance_fl.py' has been saved.\ndataset=cifar100\ncost=combined\nmeta_set=prototype\nbatch_size=16\nnum_classes=100\nnum_meta=10\nimb_factor=0.05\nepochs=200\nlr=2e-05\nmomentum=0.9\nnesterov=True\nweight_decay=0.0005\nno_cuda=False\nseed=42\nprint_freq=100\ngpu=0\nsave_name=OT_cifar100_IF20_full_run\nidx=ours\nckpt_path=checkpoint/ours/pretrain/cifar100_imb0.05_stage1.pth\nFiles already downloaded and verified\nEpoch: [160][0/975]\\tLoss 3.3186 (3.3186)\\tPrec@1 87.500 (87.500)\nEpoch: [160][100/975]\\tLoss 12.2074 (15.4987)\\tPrec@1 81.250 (72.401)\nEpoch: [160][200/975]\\tLoss 15.9132 (15.0756)\\tPrec@1 62.500 (71.953)\nEpoch: [160][300/975]\\tLoss 10.6553 (14.1237)\\tPrec@1 62.500 (72.654)\nEpoch: [160][400/975]\\tLoss 14.6248 (13.8933)\\tPrec@1 62.500 (72.974)\nEpoch: [160][500/975]\\tLoss 1.0457 (14.0919)\\tPrec@1 93.750 (72.617)\nEpoch: [160][600/975]\\tLoss 24.8271 (13.9959)\\tPrec@1 68.750 (72.348)\nEpoch: [160][700/975]\\tLoss 16.4800 (13.8586)\\tPrec@1 75.000 (72.299)\nEpoch: [160][800/975]\\tLoss 9.1910 (13.7373)\\tPrec@1 56.250 (72.331)\nEpoch: [160][900/975]\\tLoss 11.5396 (13.6927)\\tPrec@1 81.250 (72.357)\nEpoch: [160][974/975]\\tLoss 3.5792 (13.6783)\\tPrec@1 70.000 (72.316)\n * Prec@1 55.060\nEpoch: [161][0/975]\\tLoss 17.1107 (17.1107)\\tPrec@1 81.250 (81.250)\nEpoch: [161][100/975]\\tLoss 18.9694 (13.0280)\\tPrec@1 62.500 (70.483)\nEpoch: [161][200/975]\\tLoss 16.5051 (13.3028)\\tPrec@1 68.750 (70.149)\nEpoch: [161][300/975]\\tLoss 7.0548 (13.3640)\\tPrec@1 68.750 (70.785)\nEpoch: [161][400/975]\\tLoss 7.2183 (13.0379)\\tPrec@1 62.500 (71.150)\nEpoch: [161][500/975]\\tLoss 21.2476 (13.1557)\\tPrec@1 75.000 (70.846)\nEpoch: [161][600/975]\\tLoss 6.3739 (13.0489)\\tPrec@1 68.750 (70.903)\nEpoch: [161][700/975]\\tLoss 12.4656 (13.0914)\\tPrec@1 50.000 (70.622)\nEpoch: [161][800/975]\\tLoss 3.9802 (13.1293)\\tPrec@1 81.250 (70.584)\nEpoch: [161][900/975]\\tLoss 7.7522 (13.2000)\\tPrec@1 68.750 (70.484)\nEpoch: [161][974/975]\\tLoss 3.5605 (13.0358)\\tPrec@1 80.000 (70.681)\n * Prec@1 55.870\nEpoch: [162][0/975]\\tLoss 17.6088 (17.6088)\\tPrec@1 50.000 (50.000)\nEpoch: [162][100/975]\\tLoss 31.6960 (13.0622)\\tPrec@1 62.500 (69.740)\nEpoch: [162][200/975]\\tLoss 13.4391 (12.8442)\\tPrec@1 68.750 (69.496)\nEpoch: [162][300/975]\\tLoss 16.7993 (12.7621)\\tPrec@1 62.500 (70.058)\nEpoch: [162][400/975]\\tLoss 14.7812 (12.8709)\\tPrec@1 62.500 (69.872)\nEpoch: [162][500/975]\\tLoss 24.1894 (12.6967)\\tPrec@1 62.500 (69.661)\nEpoch: [162][600/975]\\tLoss 20.4953 (12.6429)\\tPrec@1 75.000 (69.977)\nEpoch: [162][700/975]\\tLoss 28.7578 (12.5900)\\tPrec@1 50.000 (69.954)\nEpoch: [162][800/975]\\tLoss 8.5147 (12.6503)\\tPrec@1 81.250 (69.920)\nEpoch: [162][900/975]\\tLoss 8.3971 (12.6220)\\tPrec@1 75.000 (70.040)\nEpoch: [162][974/975]\\tLoss 10.3813 (12.6774)\\tPrec@1 70.000 (69.924)\n * Prec@1 55.880\nEpoch: [163][0/975]\\tLoss 12.5449 (12.5449)\\tPrec@1 62.500 (62.500)\nEpoch: [163][100/975]\\tLoss 17.9047 (12.3151)\\tPrec@1 75.000 (69.554)\nEpoch: [163][200/975]\\tLoss 13.3369 (12.1643)\\tPrec@1 62.500 (69.900)\nEpoch: [163][300/975]\\tLoss 6.1192 (12.2570)\\tPrec@1 68.750 (69.913)\nEpoch: [163][400/975]\\tLoss 14.7928 (12.4366)\\tPrec@1 81.250 (69.748)\nEpoch: [163][500/975]\\tLoss 10.3757 (12.4588)\\tPrec@1 81.250 (69.623)\nEpoch: [163][600/975]\\tLoss 3.8593 (12.4405)\\tPrec@1 75.000 (69.873)\nEpoch: [163][700/975]\\tLoss 16.7271 (12.5870)\\tPrec@1 68.750 (69.757)\nEpoch: [163][800/975]\\tLoss 14.3022 (12.5594)\\tPrec@1 75.000 (69.608)\nEpoch: [163][900/975]\\tLoss 4.1774 (12.4765)\\tPrec@1 81.250 (69.693)\nEpoch: [163][974/975]\\tLoss 5.3122 (12.5037)\\tPrec@1 80.000 (69.495)\n * Prec@1 55.840\nEpoch: [164][0/975]\\tLoss 34.2614 (34.2614)\\tPrec@1 62.500 (62.500)\nEpoch: [164][100/975]\\tLoss 10.3391 (12.8426)\\tPrec@1 62.500 (68.998)\nEpoch: [164][200/975]\\tLoss 12.7071 (12.2999)\\tPrec@1 75.000 (69.807)\nEpoch: [164][300/975]\\tLoss 4.4114 (12.4877)\\tPrec@1 68.750 (69.020)\nEpoch: [164][400/975]\\tLoss 14.1454 (12.4755)\\tPrec@1 56.250 (68.812)\nEpoch: [164][500/975]\\tLoss 10.0155 (12.4407)\\tPrec@1 62.500 (68.950)\nEpoch: [164][600/975]\\tLoss 9.6819 (12.2892)\\tPrec@1 56.250 (69.041)\nEpoch: [164][700/975]\\tLoss 32.6094 (12.2794)\\tPrec@1 50.000 (69.151)\nEpoch: [164][800/975]\\tLoss 4.4097 (12.3008)\\tPrec@1 81.250 (69.062)\nEpoch: [164][900/975]\\tLoss 4.3692 (12.4151)\\tPrec@1 87.500 (68.875)\nEpoch: [164][974/975]\\tLoss 3.6783 (12.4311)\\tPrec@1 50.000 (68.873)\n * Prec@1 56.560\nEpoch: [165][0/975]\\tLoss 8.3675 (8.3675)\\tPrec@1 81.250 (81.250)\nEpoch: [165][100/975]\\tLoss 6.9529 (14.1081)\\tPrec@1 75.000 (68.379)\nEpoch: [165][200/975]\\tLoss 13.8980 (13.4377)\\tPrec@1 68.750 (69.092)\nEpoch: [165][300/975]\\tLoss 14.1953 (13.0636)\\tPrec@1 68.750 (68.542)\nEpoch: [165][400/975]\\tLoss 14.6646 (12.7319)\\tPrec@1 56.250 (68.579)\nEpoch: [165][500/975]\\tLoss 10.3315 (12.4929)\\tPrec@1 68.750 (68.588)\nEpoch: [165][600/975]\\tLoss 8.6911 (12.5769)\\tPrec@1 75.000 (68.480)\nEpoch: [165][700/975]\\tLoss 19.8512 (12.6940)\\tPrec@1 62.500 (68.286)\nEpoch: [165][800/975]\\tLoss 9.5239 (12.5385)\\tPrec@1 68.750 (68.500)\nEpoch: [165][900/975]\\tLoss 41.3583 (12.5460)\\tPrec@1 75.000 (68.556)\nEpoch: [165][974/975]\\tLoss 3.2524 (12.5916)\\tPrec@1 70.000 (68.302)\n * Prec@1 55.800\nEpoch: [166][0/975]\\tLoss 4.8194 (4.8194)\\tPrec@1 87.500 (87.500)\nEpoch: [166][100/975]\\tLoss 18.1903 (11.5886)\\tPrec@1 81.250 (69.864)\nEpoch: [166][200/975]\\tLoss 12.5764 (12.2116)\\tPrec@1 68.750 (69.465)\nEpoch: [166][300/975]\\tLoss 16.5295 (12.1524)\\tPrec@1 62.500 (68.978)\nEpoch: [166][400/975]\\tLoss 15.4463 (12.1604)\\tPrec@1 68.750 (68.672)\nEpoch: [166][500/975]\\tLoss 12.7678 (12.0708)\\tPrec@1 68.750 (68.600)\nEpoch: [166][600/975]\\tLoss 5.8231 (12.1312)\\tPrec@1 56.250 (68.729)\nEpoch: [166][700/975]\\tLoss 27.4631 (12.2271)\\tPrec@1 56.250 (68.723)\nEpoch: [166][800/975]\\tLoss 6.7074 (12.0959)\\tPrec@1 68.750 (69.031)\nEpoch: [166][900/975]\\tLoss 9.2169 (12.2336)\\tPrec@1 75.000 (68.972)\nEpoch: [166][974/975]\\tLoss 6.7605 (12.2200)\\tPrec@1 50.000 (68.821)\n * Prec@1 56.340\nEpoch: [167][0/975]\\tLoss 9.3916 (9.3916)\\tPrec@1 68.750 (68.750)\nEpoch: [167][100/975]\\tLoss 10.7033 (12.3687)\\tPrec@1 62.500 (68.379)\nEpoch: [167][200/975]\\tLoss 13.0751 (12.5260)\\tPrec@1 50.000 (68.097)\nEpoch: [167][300/975]\\tLoss 8.4792 (12.6888)\\tPrec@1 68.750 (67.836)\nEpoch: [167][400/975]\\tLoss 3.9223 (12.6360)\\tPrec@1 75.000 (68.189)\nEpoch: [167][500/975]\\tLoss 14.3674 (12.5067)\\tPrec@1 75.000 (68.476)\nEpoch: [167][600/975]\\tLoss 29.8401 (12.5368)\\tPrec@1 43.750 (68.220)\nEpoch: [167][700/975]\\tLoss 16.3210 (12.3798)\\tPrec@1 56.250 (68.465)\nEpoch: [167][800/975]\\tLoss 10.3176 (12.4550)\\tPrec@1 75.000 (68.399)\nEpoch: [167][900/975]\\tLoss 8.8780 (12.3564)\\tPrec@1 75.000 (68.500)\nEpoch: [167][974/975]\\tLoss 3.6133 (12.2323)\\tPrec@1 70.000 (68.655)\n * Prec@1 56.750\nEpoch: [168][0/975]\\tLoss 16.2472 (16.2472)\\tPrec@1 62.500 (62.500)\nEpoch: [168][100/975]\\tLoss 19.5656 (12.2835)\\tPrec@1 37.500 (68.007)\nEpoch: [168][200/975]\\tLoss 26.8523 (12.0486)\\tPrec@1 50.000 (68.377)\nEpoch: [168][300/975]\\tLoss 14.1313 (12.0716)\\tPrec@1 75.000 (68.667)\nEpoch: [168][400/975]\\tLoss 6.7711 (12.1790)\\tPrec@1 68.750 (68.703)\nEpoch: [168][500/975]\\tLoss 20.5225 (12.1615)\\tPrec@1 62.500 (68.588)\nEpoch: [168][600/975]\\tLoss 13.5052 (12.1069)\\tPrec@1 75.000 (68.511)\nEpoch: [168][700/975]\\tLoss 7.3114 (12.1822)\\tPrec@1 62.500 (68.527)\nEpoch: [168][800/975]\\tLoss 19.9678 (12.1445)\\tPrec@1 62.500 (68.688)\nEpoch: [168][900/975]\\tLoss 25.4495 (12.2936)\\tPrec@1 62.500 (68.507)\nEpoch: [168][974/975]\\tLoss 9.0918 (12.2479)\\tPrec@1 60.000 (68.578)\n * Prec@1 56.560\nEpoch: [169][0/975]\\tLoss 1.9855 (1.9855)\\tPrec@1 93.750 (93.750)\nEpoch: [169][100/975]\\tLoss 8.3777 (12.6817)\\tPrec@1 75.000 (67.389)\nEpoch: [169][200/975]\\tLoss 7.0391 (12.4286)\\tPrec@1 62.500 (67.755)\nEpoch: [169][300/975]\\tLoss 20.6044 (12.2748)\\tPrec@1 56.250 (68.044)\nEpoch: [169][400/975]\\tLoss 11.6150 (12.2763)\\tPrec@1 75.000 (68.345)\nEpoch: [169][500/975]\\tLoss 6.2908 (12.2191)\\tPrec@1 62.500 (68.039)\nEpoch: [169][600/975]\\tLoss 24.2884 (12.0312)\\tPrec@1 56.250 (68.469)\nEpoch: [169][700/975]\\tLoss 11.9263 (12.0752)\\tPrec@1 68.750 (68.705)\nEpoch: [169][800/975]\\tLoss 25.5043 (12.1749)\\tPrec@1 68.750 (68.586)\nEpoch: [169][900/975]\\tLoss 13.2739 (12.0705)\\tPrec@1 43.750 (68.681)\nEpoch: [169][974/975]\\tLoss 7.6681 (12.0821)\\tPrec@1 70.000 (68.911)\n * Prec@1 56.260\nEpoch: [170][0/975]\\tLoss 15.0685 (15.0685)\\tPrec@1 62.500 (62.500)\nEpoch: [170][100/975]\\tLoss 19.6898 (13.0701)\\tPrec@1 68.750 (67.141)\nEpoch: [170][200/975]\\tLoss 11.7627 (12.3987)\\tPrec@1 75.000 (67.848)\nEpoch: [170][300/975]\\tLoss 6.9355 (11.9603)\\tPrec@1 81.250 (68.584)\nEpoch: [170][400/975]\\tLoss 22.8706 (12.1034)\\tPrec@1 50.000 (68.734)\nEpoch: [170][500/975]\\tLoss 11.0022 (12.1519)\\tPrec@1 62.500 (68.513)\nEpoch: [170][600/975]\\tLoss 10.2630 (12.1377)\\tPrec@1 62.500 (68.376)\nEpoch: [170][700/975]\\tLoss 30.3225 (12.2137)\\tPrec@1 62.500 (68.456)\nEpoch: [170][800/975]\\tLoss 7.4572 (12.2121)\\tPrec@1 62.500 (68.266)\nEpoch: [170][900/975]\\tLoss 27.6543 (12.1736)\\tPrec@1 75.000 (68.348)\nEpoch: [170][974/975]\\tLoss 4.4565 (12.0759)\\tPrec@1 50.000 (68.552)\n * Prec@1 56.290\nEpoch: [171][0/975]\\tLoss 3.0133 (3.0133)\\tPrec@1 81.250 (81.250)\nEpoch: [171][100/975]\\tLoss 8.1597 (11.6793)\\tPrec@1 62.500 (68.131)\nEpoch: [171][200/975]\\tLoss 17.1388 (12.3986)\\tPrec@1 56.250 (67.848)\nEpoch: [171][300/975]\\tLoss 11.2431 (12.2363)\\tPrec@1 62.500 (67.774)\nEpoch: [171][400/975]\\tLoss 7.0444 (12.0198)\\tPrec@1 56.250 (68.033)\nEpoch: [171][500/975]\\tLoss 16.7226 (11.9391)\\tPrec@1 75.000 (68.326)\nEpoch: [171][600/975]\\tLoss 10.6062 (11.9408)\\tPrec@1 81.250 (68.729)\nEpoch: [171][700/975]\\tLoss 12.7317 (12.0986)\\tPrec@1 81.250 (68.545)\nEpoch: [171][800/975]\\tLoss 2.8005 (12.1923)\\tPrec@1 87.500 (68.329)\nEpoch: [171][900/975]\\tLoss 13.6922 (12.1068)\\tPrec@1 56.250 (68.486)\nEpoch: [171][974/975]\\tLoss 11.3493 (12.0889)\\tPrec@1 50.000 (68.385)\n * Prec@1 56.530\nEpoch: [172][0/975]\\tLoss 15.2490 (15.2490)\\tPrec@1 68.750 (68.750)\nEpoch: [172][100/975]\\tLoss 15.7852 (12.8288)\\tPrec@1 62.500 (69.678)\nEpoch: [172][200/975]\\tLoss 8.6119 (12.4713)\\tPrec@1 68.750 (69.621)\nEpoch: [172][300/975]\\tLoss 8.4758 (12.2839)\\tPrec@1 81.250 (69.664)\nEpoch: [172][400/975]\\tLoss 24.2231 (12.4600)\\tPrec@1 75.000 (68.890)\nEpoch: [172][500/975]\\tLoss 10.6526 (12.3225)\\tPrec@1 56.250 (68.900)\nEpoch: [172][600/975]\\tLoss 6.3935 (12.0614)\\tPrec@1 62.500 (69.062)\nEpoch: [172][700/975]\\tLoss 20.5676 (12.0059)\\tPrec@1 56.250 (68.973)\nEpoch: [172][800/975]\\tLoss 16.2369 (12.0650)\\tPrec@1 56.250 (68.867)\nEpoch: [172][900/975]\\tLoss 8.1360 (12.0644)\\tPrec@1 75.000 (68.708)\nEpoch: [172][974/975]\\tLoss 3.8437 (11.9790)\\tPrec@1 80.000 (68.796)\n * Prec@1 56.740\nEpoch: [173][0/975]\\tLoss 13.5924 (13.5924)\\tPrec@1 62.500 (62.500)\nEpoch: [173][100/975]\\tLoss 7.9273 (12.2421)\\tPrec@1 68.750 (66.584)\nEpoch: [173][200/975]\\tLoss 5.9666 (12.1827)\\tPrec@1 81.250 (68.035)\nEpoch: [173][300/975]\\tLoss 7.8355 (12.1827)\\tPrec@1 68.750 (67.940)\nEpoch: [173][400/975]\\tLoss 7.0205 (12.3983)\\tPrec@1 81.250 (68.017)\nEpoch: [173][500/975]\\tLoss 5.7004 (12.3563)\\tPrec@1 87.500 (67.677)\nEpoch: [173][600/975]\\tLoss 21.7942 (12.3067)\\tPrec@1 62.500 (67.824)\nEpoch: [173][700/975]\\tLoss 17.9520 (12.2830)\\tPrec@1 68.750 (68.046)\nEpoch: [173][800/975]\\tLoss 11.4581 (12.1196)\\tPrec@1 87.500 (68.555)\nEpoch: [173][900/975]\\tLoss 9.8366 (12.0817)\\tPrec@1 62.500 (68.799)\nEpoch: [173][974/975]\\tLoss 13.5717 (12.0802)\\tPrec@1 40.000 (68.873)\n * Prec@1 56.280\nEpoch: [174][0/975]\\tLoss 7.7812 (7.7812)\\tPrec@1 62.500 (62.500)\nEpoch: [174][100/975]\\tLoss 6.7170 (10.9258)\\tPrec@1 87.500 (70.854)\nEpoch: [174][200/975]\\tLoss 2.7995 (11.3935)\\tPrec@1 93.750 (70.149)\nEpoch: [174][300/975]\\tLoss 10.9933 (11.6758)\\tPrec@1 81.250 (69.498)\nEpoch: [174][400/975]\\tLoss 10.8508 (11.5909)\\tPrec@1 56.250 (69.015)\nEpoch: [174][500/975]\\tLoss 8.8870 (11.9772)\\tPrec@1 56.250 (68.463)\nEpoch: [174][600/975]\\tLoss 11.9900 (12.0159)\\tPrec@1 68.750 (68.500)\nEpoch: [174][700/975]\\tLoss 20.3540 (12.1218)\\tPrec@1 56.250 (68.429)\nEpoch: [174][800/975]\\tLoss 10.1131 (12.1296)\\tPrec@1 75.000 (68.235)\nEpoch: [174][900/975]\\tLoss 42.0573 (12.0442)\\tPrec@1 62.500 (68.368)\nEpoch: [174][974/975]\\tLoss 1.5872 (12.0539)\\tPrec@1 90.000 (68.526)\n * Prec@1 56.860\nEpoch: [175][0/975]\\tLoss 11.9741 (11.9741)\\tPrec@1 56.250 (56.250)\nEpoch: [175][100/975]\\tLoss 16.9957 (11.7467)\\tPrec@1 62.500 (70.421)\nEpoch: [175][200/975]\\tLoss 10.0609 (11.9084)\\tPrec@1 81.250 (69.776)\nEpoch: [175][300/975]\\tLoss 7.8203 (11.9720)\\tPrec@1 62.500 (69.207)\nEpoch: [175][400/975]\\tLoss 8.4812 (11.9152)\\tPrec@1 81.250 (69.140)\nEpoch: [175][500/975]\\tLoss 3.7364 (11.8317)\\tPrec@1 68.750 (68.937)\nEpoch: [175][600/975]\\tLoss 18.6773 (12.0247)\\tPrec@1 50.000 (68.781)\nEpoch: [175][700/975]\\tLoss 5.4535 (11.9799)\\tPrec@1 81.250 (68.732)\nEpoch: [175][800/975]\\tLoss 10.8381 (11.9439)\\tPrec@1 62.500 (68.719)\nEpoch: [175][900/975]\\tLoss 13.4244 (11.9664)\\tPrec@1 75.000 (68.715)\nEpoch: [175][974/975]\\tLoss 10.3239 (11.8761)\\tPrec@1 50.000 (68.841)\n * Prec@1 56.470\nEpoch: [176][0/975]\\tLoss 15.7731 (15.7731)\\tPrec@1 56.250 (56.250)\nEpoch: [176][100/975]\\tLoss 7.9104 (11.1621)\\tPrec@1 62.500 (68.441)\nEpoch: [176][200/975]\\tLoss 18.7326 (11.3192)\\tPrec@1 68.750 (68.221)\nEpoch: [176][300/975]\\tLoss 6.3688 (11.5173)\\tPrec@1 75.000 (68.812)\nEpoch: [176][400/975]\\tLoss 14.4762 (11.8429)\\tPrec@1 56.250 (68.641)\nEpoch: [176][500/975]\\tLoss 13.5757 (12.0471)\\tPrec@1 56.250 (68.288)\nEpoch: [176][600/975]\\tLoss 11.4980 (11.9258)\\tPrec@1 62.500 (68.344)\nEpoch: [176][700/975]\\tLoss 14.8829 (12.1483)\\tPrec@1 56.250 (68.349)\nEpoch: [176][800/975]\\tLoss 6.1019 (12.2061)\\tPrec@1 68.750 (68.360)\nEpoch: [176][900/975]\\tLoss 8.7902 (12.1621)\\tPrec@1 75.000 (68.382)\nEpoch: [176][974/975]\\tLoss 4.1575 (12.0296)\\tPrec@1 90.000 (68.526)\n * Prec@1 57.050\nEpoch: [177][0/975]\\tLoss 17.9956 (17.9956)\\tPrec@1 68.750 (68.750)\nEpoch: [177][100/975]\\tLoss 7.3780 (12.9632)\\tPrec@1 75.000 (69.554)\nEpoch: [177][200/975]\\tLoss 19.2552 (12.0574)\\tPrec@1 50.000 (68.750)\nEpoch: [177][300/975]\\tLoss 17.2824 (11.8666)\\tPrec@1 56.250 (69.020)\nEpoch: [177][400/975]\\tLoss 5.9974 (11.4255)\\tPrec@1 62.500 (69.467)\nEpoch: [177][500/975]\\tLoss 21.1991 (11.7709)\\tPrec@1 50.000 (69.237)\nEpoch: [177][600/975]\\tLoss 10.9178 (11.5878)\\tPrec@1 87.500 (69.312)\nEpoch: [177][700/975]\\tLoss 9.2310 (11.7923)\\tPrec@1 56.250 (68.982)\nEpoch: [177][800/975]\\tLoss 16.7809 (11.7306)\\tPrec@1 68.750 (69.171)\nEpoch: [177][900/975]\\tLoss 14.1772 (11.7773)\\tPrec@1 50.000 (69.062)\nEpoch: [177][974/975]\\tLoss 3.2351 (11.7963)\\tPrec@1 90.000 (69.059)\n * Prec@1 56.700\nEpoch: [178][0/975]\\tLoss 16.0601 (16.0601)\\tPrec@1 62.500 (62.500)\nEpoch: [178][100/975]\\tLoss 23.2200 (11.6309)\\tPrec@1 62.500 (67.512)\nEpoch: [178][200/975]\\tLoss 7.9941 (11.4870)\\tPrec@1 50.000 (67.195)\nEpoch: [178][300/975]\\tLoss 23.2551 (12.0046)\\tPrec@1 56.250 (67.380)\nEpoch: [178][400/975]\\tLoss 13.9987 (11.8629)\\tPrec@1 68.750 (67.565)\nEpoch: [178][500/975]\\tLoss 1.5472 (11.7707)\\tPrec@1 100.000 (67.690)\nEpoch: [178][600/975]\\tLoss 2.9325 (11.8131)\\tPrec@1 87.500 (67.783)\nEpoch: [178][700/975]\\tLoss 18.8111 (11.6898)\\tPrec@1 81.250 (68.197)\nEpoch: [178][800/975]\\tLoss 14.0237 (11.8518)\\tPrec@1 68.750 (68.165)\nEpoch: [178][900/975]\\tLoss 12.9963 (11.8581)\\tPrec@1 56.250 (68.188)\nEpoch: [178][974/975]\\tLoss 4.1803 (11.8899)\\tPrec@1 70.000 (68.308)\n * Prec@1 57.160\nEpoch: [179][0/975]\\tLoss 16.3514 (16.3514)\\tPrec@1 81.250 (81.250)\nEpoch: [179][100/975]\\tLoss 20.8065 (12.6836)\\tPrec@1 75.000 (69.183)\nEpoch: [179][200/975]\\tLoss 4.2039 (12.0783)\\tPrec@1 62.500 (69.683)\nEpoch: [179][300/975]\\tLoss 7.6193 (12.3981)\\tPrec@1 68.750 (69.145)\nEpoch: [179][400/975]\\tLoss 18.4019 (11.9133)\\tPrec@1 62.500 (69.763)\nEpoch: [179][500/975]\\tLoss 5.8251 (11.6257)\\tPrec@1 75.000 (69.661)\nEpoch: [179][600/975]\\tLoss 6.2987 (11.5097)\\tPrec@1 62.500 (69.270)\nEpoch: [179][700/975]\\tLoss 6.4244 (11.3640)\\tPrec@1 68.750 (69.205)\nEpoch: [179][800/975]\\tLoss 10.0395 (11.4551)\\tPrec@1 75.000 (69.273)\nEpoch: [179][900/975]\\tLoss 8.4303 (11.6371)\\tPrec@1 75.000 (69.215)\nEpoch: [179][974/975]\\tLoss 2.3145 (11.7641)\\tPrec@1 80.000 (69.116)\n * Prec@1 56.540\nEpoch: [180][0/975]\\tLoss 10.0853 (10.0853)\\tPrec@1 68.750 (68.750)\nEpoch: [180][100/975]\\tLoss 13.4437 (11.7580)\\tPrec@1 56.250 (70.111)\nEpoch: [180][200/975]\\tLoss 8.4845 (12.3251)\\tPrec@1 62.500 (69.776)\nEpoch: [180][300/975]\\tLoss 11.0285 (12.2196)\\tPrec@1 50.000 (69.186)\nEpoch: [180][400/975]\\tLoss 8.4784 (11.8582)\\tPrec@1 62.500 (69.373)\nEpoch: [180][500/975]\\tLoss 7.6698 (11.9467)\\tPrec@1 68.750 (69.024)\nEpoch: [180][600/975]\\tLoss 8.3198 (11.8829)\\tPrec@1 75.000 (69.114)\nEpoch: [180][700/975]\\tLoss 9.6420 (11.8596)\\tPrec@1 81.250 (68.848)\nEpoch: [180][800/975]\\tLoss 30.3619 (11.7786)\\tPrec@1 75.000 (69.007)\nEpoch: [180][900/975]\\tLoss 3.3850 (11.7306)\\tPrec@1 87.500 (69.111)\nEpoch: [180][974/975]\\tLoss 2.8143 (11.7368)\\tPrec@1 90.000 (69.168)\n * Prec@1 56.960\nEpoch: [181][0/975]\\tLoss 2.2300 (2.2300)\\tPrec@1 81.250 (81.250)\nEpoch: [181][100/975]\\tLoss 4.3191 (11.7262)\\tPrec@1 81.250 (70.111)\nEpoch: [181][200/975]\\tLoss 16.9859 (11.3572)\\tPrec@1 75.000 (69.683)\nEpoch: [181][300/975]\\tLoss 3.0806 (11.7021)\\tPrec@1 81.250 (69.248)\nEpoch: [181][400/975]\\tLoss 9.6300 (11.6751)\\tPrec@1 81.250 (69.389)\nEpoch: [181][500/975]\\tLoss 4.5957 (11.7474)\\tPrec@1 75.000 (69.174)\nEpoch: [181][600/975]\\tLoss 20.5013 (11.8718)\\tPrec@1 62.500 (69.187)\nEpoch: [181][700/975]\\tLoss 22.0183 (11.8351)\\tPrec@1 68.750 (69.249)\nEpoch: [181][800/975]\\tLoss 8.4435 (11.7759)\\tPrec@1 56.250 (69.101)\nEpoch: [181][900/975]\\tLoss 19.0492 (11.7325)\\tPrec@1 68.750 (69.000)\nEpoch: [181][974/975]\\tLoss 5.6647 (11.6982)\\tPrec@1 80.000 (68.969)\n * Prec@1 56.960\nEpoch: [182][0/975]\\tLoss 6.3738 (6.3738)\\tPrec@1 81.250 (81.250)\nEpoch: [182][100/975]\\tLoss 17.2821 (11.9070)\\tPrec@1 56.250 (68.874)\nEpoch: [182][200/975]\\tLoss 8.7791 (12.0323)\\tPrec@1 62.500 (68.657)\nEpoch: [182][300/975]\\tLoss 7.1381 (12.0073)\\tPrec@1 62.500 (69.186)\nEpoch: [182][400/975]\\tLoss 21.7445 (12.0203)\\tPrec@1 50.000 (69.202)\nEpoch: [182][500/975]\\tLoss 5.4896 (11.8907)\\tPrec@1 68.750 (69.162)\nEpoch: [182][600/975]\\tLoss 25.3042 (11.6559)\\tPrec@1 56.250 (69.530)\nEpoch: [182][700/975]\\tLoss 13.6418 (11.7221)\\tPrec@1 68.750 (69.347)\nEpoch: [182][800/975]\\tLoss 10.7274 (11.7976)\\tPrec@1 75.000 (69.148)\nEpoch: [182][900/975]\\tLoss 3.8154 (11.8563)\\tPrec@1 81.250 (68.757)\nEpoch: [182][974/975]\\tLoss 2.9907 (11.8632)\\tPrec@1 80.000 (68.757)\n * Prec@1 56.930\nEpoch: [183][0/975]\\tLoss 5.9583 (5.9583)\\tPrec@1 81.250 (81.250)\nEpoch: [183][100/975]\\tLoss 6.5915 (10.9111)\\tPrec@1 68.750 (70.111)\nEpoch: [183][200/975]\\tLoss 16.2172 (11.2227)\\tPrec@1 62.500 (68.688)\nEpoch: [183][300/975]\\tLoss 5.6367 (11.2200)\\tPrec@1 62.500 (68.958)\nEpoch: [183][400/975]\\tLoss 11.4978 (11.6065)\\tPrec@1 62.500 (68.859)\nEpoch: [183][500/975]\\tLoss 3.7527 (11.3902)\\tPrec@1 87.500 (69.274)\nEpoch: [183][600/975]\\tLoss 17.8008 (11.6862)\\tPrec@1 50.000 (68.823)\nEpoch: [183][700/975]\\tLoss 31.8197 (11.6949)\\tPrec@1 62.500 (68.875)\nEpoch: [183][800/975]\\tLoss 11.1961 (11.6855)\\tPrec@1 81.250 (68.781)\nEpoch: [183][900/975]\\tLoss 11.8200 (11.6484)\\tPrec@1 68.750 (68.722)\nEpoch: [183][974/975]\\tLoss 9.4014 (11.7515)\\tPrec@1 50.000 (68.623)\n * Prec@1 57.090\nEpoch: [184][0/975]\\tLoss 8.3943 (8.3943)\\tPrec@1 62.500 (62.500)\nEpoch: [184][100/975]\\tLoss 19.7659 (12.0794)\\tPrec@1 50.000 (67.760)\nEpoch: [184][200/975]\\tLoss 7.7336 (11.8062)\\tPrec@1 75.000 (67.942)\nEpoch: [184][300/975]\\tLoss 11.7425 (11.9748)\\tPrec@1 75.000 (68.501)\nEpoch: [184][400/975]\\tLoss 15.8073 (11.6980)\\tPrec@1 56.250 (68.610)\nEpoch: [184][500/975]\\tLoss 15.7972 (11.6278)\\tPrec@1 75.000 (68.600)\nEpoch: [184][600/975]\\tLoss 15.9943 (11.6235)\\tPrec@1 56.250 (68.729)\nEpoch: [184][700/975]\\tLoss 3.6217 (11.5229)\\tPrec@1 81.250 (69.017)\nEpoch: [184][800/975]\\tLoss 9.4535 (11.8115)\\tPrec@1 62.500 (68.968)\nEpoch: [184][900/975]\\tLoss 4.3661 (11.7359)\\tPrec@1 81.250 (69.090)\nEpoch: [184][974/975]\\tLoss 2.5659 (11.7072)\\tPrec@1 80.000 (69.001)\n * Prec@1 56.960\nEpoch: [185][0/975]\\tLoss 26.2858 (26.2858)\\tPrec@1 56.250 (56.250)\nEpoch: [185][100/975]\\tLoss 10.2942 (12.5854)\\tPrec@1 87.500 (67.327)\nEpoch: [185][200/975]\\tLoss 26.5612 (11.8244)\\tPrec@1 56.250 (69.496)\nEpoch: [185][300/975]\\tLoss 4.7432 (11.2216)\\tPrec@1 68.750 (69.498)\nEpoch: [185][400/975]\\tLoss 3.1063 (11.2089)\\tPrec@1 81.250 (69.218)\nEpoch: [185][500/975]\\tLoss 27.5691 (11.5536)\\tPrec@1 56.250 (68.987)\nEpoch: [185][600/975]\\tLoss 4.5870 (11.4497)\\tPrec@1 75.000 (69.208)\nEpoch: [185][700/975]\\tLoss 6.1773 (11.5121)\\tPrec@1 68.750 (69.178)\nEpoch: [185][800/975]\\tLoss 12.0462 (11.6836)\\tPrec@1 50.000 (69.164)\nEpoch: [185][900/975]\\tLoss 13.3730 (11.5476)\\tPrec@1 68.750 (69.298)\nEpoch: [185][974/975]\\tLoss 6.9795 (11.5855)\\tPrec@1 60.000 (69.289)\n * Prec@1 57.160\nEpoch: [186][0/975]\\tLoss 14.9334 (14.9334)\\tPrec@1 68.750 (68.750)\nEpoch: [186][100/975]\\tLoss 9.4132 (11.1447)\\tPrec@1 75.000 (71.101)\nEpoch: [186][200/975]\\tLoss 5.7136 (12.0157)\\tPrec@1 56.250 (70.553)\nEpoch: [186][300/975]\\tLoss 7.7294 (11.6613)\\tPrec@1 87.500 (70.224)\nEpoch: [186][400/975]\\tLoss 11.3715 (11.6244)\\tPrec@1 68.750 (69.732)\nEpoch: [186][500/975]\\tLoss 17.0971 (11.5034)\\tPrec@1 62.500 (69.723)\nEpoch: [186][600/975]\\tLoss 8.0460 (11.5522)\\tPrec@1 62.500 (69.624)\nEpoch: [186][700/975]\\tLoss 19.9912 (11.6381)\\tPrec@1 68.750 (69.276)\nEpoch: [186][800/975]\\tLoss 3.7251 (11.6266)\\tPrec@1 68.750 (69.203)\nEpoch: [186][900/975]\\tLoss 14.7900 (11.5629)\\tPrec@1 68.750 (69.229)\nEpoch: [186][974/975]\\tLoss 3.7692 (11.5339)\\tPrec@1 60.000 (69.187)\n * Prec@1 57.110\nEpoch: [187][0/975]\\tLoss 15.9914 (15.9914)\\tPrec@1 56.250 (56.250)\nEpoch: [187][100/975]\\tLoss 14.3349 (11.8019)\\tPrec@1 50.000 (68.317)\nEpoch: [187][200/975]\\tLoss 4.6598 (11.4673)\\tPrec@1 68.750 (69.154)\nEpoch: [187][300/975]\\tLoss 14.7611 (11.2050)\\tPrec@1 43.750 (69.581)\nEpoch: [187][400/975]\\tLoss 6.0147 (11.1849)\\tPrec@1 81.250 (69.483)\nEpoch: [187][500/975]\\tLoss 6.1030 (11.3766)\\tPrec@1 75.000 (69.237)\nEpoch: [187][600/975]\\tLoss 10.4994 (11.3154)\\tPrec@1 68.750 (69.260)\nEpoch: [187][700/975]\\tLoss 13.0967 (11.5216)\\tPrec@1 50.000 (69.071)\nEpoch: [187][800/975]\\tLoss 14.4921 (11.5212)\\tPrec@1 62.500 (69.000)\nEpoch: [187][900/975]\\tLoss 6.0121 (11.6516)\\tPrec@1 75.000 (68.923)\nEpoch: [187][974/975]\\tLoss 4.5180 (11.5520)\\tPrec@1 80.000 (68.937)\n * Prec@1 57.240\nEpoch: [188][0/975]\\tLoss 11.4338 (11.4338)\\tPrec@1 62.500 (62.500)\nEpoch: [188][100/975]\\tLoss 13.8032 (11.4417)\\tPrec@1 56.250 (67.884)\nEpoch: [188][200/975]\\tLoss 8.7683 (11.6890)\\tPrec@1 68.750 (68.128)\nEpoch: [188][300/975]\\tLoss 5.5125 (11.8689)\\tPrec@1 87.500 (68.916)\nEpoch: [188][400/975]\\tLoss 3.6153 (11.8067)\\tPrec@1 81.250 (69.062)\nEpoch: [188][500/975]\\tLoss 28.0855 (11.5291)\\tPrec@1 37.500 (69.237)\nEpoch: [188][600/975]\\tLoss 5.3244 (11.4577)\\tPrec@1 75.000 (69.353)\nEpoch: [188][700/975]\\tLoss 5.8459 (11.4985)\\tPrec@1 62.500 (69.231)\nEpoch: [188][800/975]\\tLoss 6.8379 (11.5682)\\tPrec@1 75.000 (69.140)\nEpoch: [188][900/975]\\tLoss 25.1532 (11.5947)\\tPrec@1 75.000 (68.972)\nEpoch: [188][974/975]\\tLoss 0.7953 (11.6125)\\tPrec@1 100.000 (68.988)\n * Prec@1 56.930\nEpoch: [189][0/975]\\tLoss 21.3733 (21.3733)\\tPrec@1 68.750 (68.750)\nEpoch: [189][100/975]\\tLoss 11.2340 (12.0865)\\tPrec@1 68.750 (68.502)\nEpoch: [189][200/975]\\tLoss 5.9239 (11.9230)\\tPrec@1 81.250 (68.315)\nEpoch: [189][300/975]\\tLoss 10.6537 (11.6720)\\tPrec@1 75.000 (69.103)\nEpoch: [189][400/975]\\tLoss 13.4202 (11.9126)\\tPrec@1 68.750 (68.937)\nEpoch: [189][500/975]\\tLoss 13.8980 (11.4688)\\tPrec@1 62.500 (69.199)\nEpoch: [189][600/975]\\tLoss 9.5915 (11.3984)\\tPrec@1 68.750 (69.332)\nEpoch: [189][700/975]\\tLoss 7.8445 (11.2593)\\tPrec@1 75.000 (69.374)\nEpoch: [189][800/975]\\tLoss 5.5339 (11.4620)\\tPrec@1 81.250 (69.164)\nEpoch: [189][900/975]\\tLoss 15.0018 (11.6271)\\tPrec@1 56.250 (69.173)\nEpoch: [189][974/975]\\tLoss 9.2648 (11.5613)\\tPrec@1 50.000 (69.123)\n * Prec@1 56.990\nEpoch: [190][0/975]\\tLoss 6.8264 (6.8264)\\tPrec@1 81.250 (81.250)\nEpoch: [190][100/975]\\tLoss 6.7280 (11.9568)\\tPrec@1 68.750 (68.441)\nEpoch: [190][200/975]\\tLoss 5.3264 (11.2415)\\tPrec@1 75.000 (70.025)\nEpoch: [190][300/975]\\tLoss 3.5326 (10.9538)\\tPrec@1 75.000 (70.183)\nEpoch: [190][400/975]\\tLoss 7.6791 (10.9076)\\tPrec@1 56.250 (70.168)\nEpoch: [190][500/975]\\tLoss 12.5080 (11.2095)\\tPrec@1 56.250 (70.072)\nEpoch: [190][600/975]\\tLoss 13.7062 (11.2036)\\tPrec@1 68.750 (69.946)\nEpoch: [190][700/975]\\tLoss 6.8237 (11.3201)\\tPrec@1 81.250 (69.829)\nEpoch: [190][800/975]\\tLoss 12.9520 (11.2621)\\tPrec@1 68.750 (69.624)\nEpoch: [190][900/975]\\tLoss 32.4892 (11.3268)\\tPrec@1 56.250 (69.492)\nEpoch: [190][974/975]\\tLoss 8.7306 (11.3840)\\tPrec@1 50.000 (69.379)\n * Prec@1 56.980\nEpoch: [191][0/975]\\tLoss 10.5648 (10.5648)\\tPrec@1 75.000 (75.000)\nEpoch: [191][100/975]\\tLoss 14.2858 (11.0820)\\tPrec@1 50.000 (68.317)\nEpoch: [191][200/975]\\tLoss 4.5798 (11.2894)\\tPrec@1 87.500 (68.377)\nEpoch: [191][300/975]\\tLoss 4.9366 (11.6875)\\tPrec@1 75.000 (68.210)\nEpoch: [191][400/975]\\tLoss 5.2222 (11.6752)\\tPrec@1 68.750 (68.345)\nEpoch: [191][500/975]\\tLoss 28.9694 (11.4601)\\tPrec@1 68.750 (68.800)\nEpoch: [191][600/975]\\tLoss 3.6310 (11.3548)\\tPrec@1 81.250 (69.156)\nEpoch: [191][700/975]\\tLoss 7.8934 (11.2698)\\tPrec@1 81.250 (69.196)\nEpoch: [191][800/975]\\tLoss 5.7725 (11.3287)\\tPrec@1 62.500 (69.242)\nEpoch: [191][900/975]\\tLoss 10.7215 (11.2831)\\tPrec@1 56.250 (69.298)\nEpoch: [191][974/975]\\tLoss 2.6696 (11.2964)\\tPrec@1 80.000 (69.373)\n * Prec@1 56.900\nEpoch: [192][0/975]\\tLoss 7.5764 (7.5764)\\tPrec@1 87.500 (87.500)\nEpoch: [192][100/975]\\tLoss 8.1102 (10.5416)\\tPrec@1 75.000 (71.473)\nEpoch: [192][200/975]\\tLoss 11.1026 (10.9595)\\tPrec@1 62.500 (71.611)\nEpoch: [192][300/975]\\tLoss 6.8070 (10.9526)\\tPrec@1 75.000 (71.055)\nEpoch: [192][400/975]\\tLoss 10.0777 (11.4099)\\tPrec@1 68.750 (70.122)\nEpoch: [192][500/975]\\tLoss 17.4230 (11.3819)\\tPrec@1 56.250 (70.010)\nEpoch: [192][600/975]\\tLoss 7.3394 (11.4626)\\tPrec@1 68.750 (69.769)\nEpoch: [192][700/975]\\tLoss 6.7427 (11.4425)\\tPrec@1 68.750 (69.463)\nEpoch: [192][800/975]\\tLoss 8.9879 (11.4811)\\tPrec@1 56.250 (69.600)\nEpoch: [192][900/975]\\tLoss 9.8780 (11.5584)\\tPrec@1 68.750 (69.298)\nEpoch: [192][974/975]\\tLoss 9.4613 (11.4396)\\tPrec@1 60.000 (69.366)\n * Prec@1 57.370\nEpoch: [193][0/975]\\tLoss 16.4258 (16.4258)\\tPrec@1 68.750 (68.750)\nEpoch: [193][100/975]\\tLoss 12.4751 (10.9554)\\tPrec@1 50.000 (70.916)\nEpoch: [193][200/975]\\tLoss 22.5120 (10.5644)\\tPrec@1 31.250 (70.771)\nEpoch: [193][300/975]\\tLoss 5.1889 (10.6707)\\tPrec@1 87.500 (70.411)\nEpoch: [193][400/975]\\tLoss 17.0100 (10.8789)\\tPrec@1 62.500 (70.449)\nEpoch: [193][500/975]\\tLoss 8.5424 (11.1114)\\tPrec@1 50.000 (69.985)\nEpoch: [193][600/975]\\tLoss 2.8359 (11.2058)\\tPrec@1 81.250 (69.790)\nEpoch: [193][700/975]\\tLoss 10.4478 (11.1817)\\tPrec@1 68.750 (69.802)\nEpoch: [193][800/975]\\tLoss 7.9613 (11.2336)\\tPrec@1 75.000 (69.686)\nEpoch: [193][900/975]\\tLoss 15.9503 (11.1756)\\tPrec@1 43.750 (69.513)\nEpoch: [193][974/975]\\tLoss 14.0778 (11.3010)\\tPrec@1 30.000 (69.283)\n * Prec@1 57.100\nEpoch: [194][0/975]\\tLoss 9.7746 (9.7746)\\tPrec@1 75.000 (75.000)\nEpoch: [194][100/975]\\tLoss 7.1632 (11.2548)\\tPrec@1 81.250 (69.307)\nEpoch: [194][200/975]\\tLoss 17.0072 (11.0761)\\tPrec@1 62.500 (70.025)\nEpoch: [194][300/975]\\tLoss 15.7617 (11.6880)\\tPrec@1 56.250 (68.812)\nEpoch: [194][400/975]\\tLoss 22.6193 (11.6461)\\tPrec@1 75.000 (68.563)\nEpoch: [194][500/975]\\tLoss 15.3814 (11.4935)\\tPrec@1 37.500 (68.463)\nEpoch: [194][600/975]\\tLoss 4.0498 (11.4784)\\tPrec@1 81.250 (68.594)\nEpoch: [194][700/975]\\tLoss 14.5410 (11.4153)\\tPrec@1 68.750 (68.830)\nEpoch: [194][800/975]\\tLoss 3.9801 (11.4098)\\tPrec@1 81.250 (68.672)\nEpoch: [194][900/975]\\tLoss 17.4732 (11.3555)\\tPrec@1 56.250 (68.750)\nEpoch: [194][974/975]\\tLoss 7.0857 (11.3455)\\tPrec@1 60.000 (68.796)\n * Prec@1 56.980\nEpoch: [195][0/975]\\tLoss 36.6462 (36.6462)\\tPrec@1 43.750 (43.750)\nEpoch: [195][100/975]\\tLoss 4.4678 (11.2657)\\tPrec@1 75.000 (69.616)\nEpoch: [195][200/975]\\tLoss 16.3540 (11.8179)\\tPrec@1 68.750 (68.874)\nEpoch: [195][300/975]\\tLoss 12.2771 (11.6383)\\tPrec@1 68.750 (68.397)\nEpoch: [195][400/975]\\tLoss 2.6981 (11.4762)\\tPrec@1 75.000 (68.220)\nEpoch: [195][500/975]\\tLoss 9.9880 (11.4285)\\tPrec@1 62.500 (68.426)\nEpoch: [195][600/975]\\tLoss 6.9566 (11.5185)\\tPrec@1 68.750 (68.542)\nEpoch: [195][700/975]\\tLoss 9.9849 (11.4324)\\tPrec@1 68.750 (68.670)\nEpoch: [195][800/975]\\tLoss 10.5515 (11.4640)\\tPrec@1 62.500 (68.571)\nEpoch: [195][900/975]\\tLoss 7.1169 (11.4445)\\tPrec@1 75.000 (68.597)\nEpoch: [195][974/975]\\tLoss 10.9216 (11.4308)\\tPrec@1 50.000 (68.610)\n * Prec@1 56.780\nEpoch: [196][0/975]\\tLoss 7.9955 (7.9955)\\tPrec@1 68.750 (68.750)\nEpoch: [196][100/975]\\tLoss 10.2464 (11.7374)\\tPrec@1 75.000 (69.554)\nEpoch: [196][200/975]\\tLoss 6.3646 (11.3373)\\tPrec@1 68.750 (69.310)\nEpoch: [196][300/975]\\tLoss 9.2369 (11.2448)\\tPrec@1 68.750 (69.041)\nEpoch: [196][400/975]\\tLoss 16.3822 (11.4554)\\tPrec@1 62.500 (69.124)\nEpoch: [196][500/975]\\tLoss 5.9400 (11.5325)\\tPrec@1 68.750 (69.149)\nEpoch: [196][600/975]\\tLoss 11.4534 (11.6744)\\tPrec@1 62.500 (69.052)\nEpoch: [196][700/975]\\tLoss 8.4506 (11.4509)\\tPrec@1 62.500 (69.454)\nEpoch: [196][800/975]\\tLoss 10.5435 (11.5073)\\tPrec@1 81.250 (69.460)\nEpoch: [196][900/975]\\tLoss 5.5774 (11.3570)\\tPrec@1 68.750 (69.652)\nEpoch: [196][974/975]\\tLoss 6.5846 (11.2930)\\tPrec@1 70.000 (69.597)\n * Prec@1 57.200\nEpoch: [197][0/975]\\tLoss 11.2525 (11.2525)\\tPrec@1 87.500 (87.500)\nEpoch: [197][100/975]\\tLoss 5.8035 (10.9083)\\tPrec@1 68.750 (69.864)\nEpoch: [197][200/975]\\tLoss 12.8231 (10.5974)\\tPrec@1 75.000 (71.082)\nEpoch: [197][300/975]\\tLoss 13.2480 (10.5424)\\tPrec@1 75.000 (70.847)\nEpoch: [197][400/975]\\tLoss 17.6084 (10.9605)\\tPrec@1 56.250 (70.262)\nEpoch: [197][500/975]\\tLoss 16.9336 (10.9816)\\tPrec@1 50.000 (70.384)\nEpoch: [197][600/975]\\tLoss 2.3944 (11.0000)\\tPrec@1 87.500 (70.279)\nEpoch: [197][700/975]\\tLoss 5.7449 (11.0219)\\tPrec@1 81.250 (69.945)\nEpoch: [197][800/975]\\tLoss 13.5563 (11.0198)\\tPrec@1 81.250 (69.827)\nEpoch: [197][900/975]\\tLoss 11.9777 (11.0470)\\tPrec@1 81.250 (69.763)\nEpoch: [197][974/975]\\tLoss 7.3430 (11.1082)\\tPrec@1 70.000 (69.681)\n * Prec@1 57.020\nEpoch: [198][0/975]\\tLoss 6.4541 (6.4541)\\tPrec@1 68.750 (68.750)\nEpoch: [198][100/975]\\tLoss 4.5104 (10.8645)\\tPrec@1 81.250 (70.854)\nEpoch: [198][200/975]\\tLoss 11.1570 (10.3286)\\tPrec@1 62.500 (70.740)\nEpoch: [198][300/975]\\tLoss 5.8490 (10.7579)\\tPrec@1 75.000 (70.141)\nEpoch: [198][400/975]\\tLoss 8.1849 (10.5112)\\tPrec@1 75.000 (70.137)\nEpoch: [198][500/975]\\tLoss 4.2046 (10.6878)\\tPrec@1 75.000 (69.998)\nEpoch: [198][600/975]\\tLoss 7.6969 (10.8328)\\tPrec@1 62.500 (69.842)\nEpoch: [198][700/975]\\tLoss 22.2070 (10.9414)\\tPrec@1 68.750 (69.606)\nEpoch: [198][800/975]\\tLoss 11.7593 (11.0686)\\tPrec@1 62.500 (69.655)\nEpoch: [198][900/975]\\tLoss 14.4279 (11.0738)\\tPrec@1 62.500 (69.714)\nEpoch: [198][974/975]\\tLoss 5.8024 (11.0693)\\tPrec@1 80.000 (69.732)\n * Prec@1 56.260\nEpoch: [199][0/975]\\tLoss 3.8802 (3.8802)\\tPrec@1 81.250 (81.250)\nEpoch: [199][100/975]\\tLoss 10.8988 (10.8828)\\tPrec@1 56.250 (69.740)\nEpoch: [199][200/975]\\tLoss 6.4007 (11.0860)\\tPrec@1 75.000 (70.025)\nEpoch: [199][300/975]\\tLoss 5.2212 (11.0109)\\tPrec@1 81.250 (70.203)\nEpoch: [199][400/975]\\tLoss 7.9168 (10.8578)\\tPrec@1 87.500 (70.184)\nEpoch: [199][500/975]\\tLoss 12.4293 (11.1421)\\tPrec@1 56.250 (69.673)\nEpoch: [199][600/975]\\tLoss 5.3864 (11.0942)\\tPrec@1 81.250 (69.478)\nEpoch: [199][700/975]\\tLoss 12.9010 (11.1629)\\tPrec@1 56.250 (69.214)\nEpoch: [199][800/975]\\tLoss 13.0228 (11.0616)\\tPrec@1 50.000 (69.273)\nEpoch: [199][900/975]\\tLoss 4.6686 (11.2276)\\tPrec@1 62.500 (69.256)\nEpoch: [199][974/975]\\tLoss 3.9529 (11.3730)\\tPrec@1 60.000 (69.245)\n * Prec@1 56.800\nBest accuracy:  57.37\nFile 'OT_train.py' has been saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}